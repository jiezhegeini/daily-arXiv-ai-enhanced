<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 96]
- [cs.IR](#cs.IR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 144]
- [cs.AI](#cs.AI) [Total: 65]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 开发了基于大语言模型和搜索引擎的葡萄牙语新闻外部证据增强方法，用于提升半自动事实核查系统的性能。


<details>
  <summary>Details</summary>
Motivation: 由于虚假信息传播迅速，现有手动核查与只依赖文本特征的系统不足以应对

Method: 利用大型语言模型提取新闻主要声明，通过搜索引擎检索相关证据，并应用数据验证和去重框架增强语料库

Result: 成功丰富了葡萄牙语新闻语料库，为多证据支持的自动化核查打下基础

Conclusion: 该方法有效支持半自动事实核查，有助改善虚假信息检测的系统性能

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [2] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: 利用动态提示策略与检索增强生成，提高少样本生物医学实体识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在少样本生物医学NER任务中的性能挑战。

Method: 采用检索增强生成技术，动态选择与输入文本相似的示例，动态更新提示。

Result: 静态提示和动态提示显著提升模型在多个生物医学NER数据集上的性能，动态提示效果更佳。

Conclusion: 结合检索方法的上下文自适应提示策略，有助于提升少样本生物医学NER的效果。

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [3] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 提出	extit{CarbonScaling}框架，扩展神经缩放定律以考虑模型训练的碳排放，显示硬件进步与调优可减缓碳足迹增长。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型（LLMs）发展，衡量其碳排放成为亟待解决的问题，现有神经缩放定律未考虑碳排放带来的影响。

Method: 结合神经缩放规律、硬件演变、并行优化和碳排放估算，建立分析框架，连接模型准确性与碳足迹。

Result: 碳排放与准确性存在幂律关系，但实际效率降低了缩放效益。硬件技术能减缓中小模型碳排放，但对极大模型效果有限，通过训练优化可以改善效率。

Conclusion: 碳排放的定量分析有助于制定更绿色的训练策略，为大规模模型的可持续发展提供指导。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [4] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本研究系统性分析多语种环境中tokenization的影响并提出改进算法，显著提升模型的token效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管模型架构和训练目标已被充分研究，tokenization在多语种LLM中的作用仍被忽视，影响模型效率和性能。

Method: 通过在Indic脚本上进行实验，分析词汇量、预切割规则和语料组成的影响，提出一种平衡多语种数据的算法，并改进预切割策略。

Result: 减少了约6%的token-to-word比率，在多语种Indic模型中实现了超过40%的提升，改善了模型性能和推理速度。

Conclusion: Tokenization是构建高效、多语种LLMs的重要环节，应与架构和训练目标同等重视。

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [5] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: 提出了一种结合自动编码器的文本降维框架，提升LLM文本嵌入的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 应对高维文本嵌入带来的计算成本和效率低下问题。

Method: 引入监督增强自动编码器，将降维融入预训练语言模型流程，学习任务相关的低维潜在因子。

Result: 在分类、异常检测和预测任务中，优于传统方法和原始嵌入，表现出显著性能提升。

Conclusion: 该框架有效改善高维嵌入的效率和任务性能，具有广泛应用潜力。

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [6] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 该研究提出了GuideEval基准，评估大语言模型在个性化教育引导中的表现，强调模型应动态调整教学策略以应对learner的认知状态，并通过行为引导微调提升其指导能力。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在教育中的应用多集中于questions的生成，缺乏对其个性化、适应性教学能力的系统评估。

Method: 提出基于真实教育对话的GuideEval基准，包含感知、协调和引导三个阶段，用行为驱动进行微调训练。

Result: 现有模型在面对learner困惑时常缺乏有效的调整能力，但通过行为引导微调，显著提升其个性化指导表现。

Conclusion: 推动大语言模型由单一内容生成向以learner为中心的对话式教学转变，强调动态适应和反思促进的重要性。

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [7] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: 提出一种利用语言模型自动生成高质量遗忘集的方法，以提升模型在特定领域的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在特定敏感或版权内容遗忘上的困难，降低人工构建遗忘集的成本。

Method: 通过结构化提示流程，利用语言模型合成教材风格的数据，生成多样化的遗忘集。

Result: 合成数据在生物安全、网络安全和哈利波特小说等任务中优于传统合成方法，接近专家 curated 数据效果。

Conclusion: 合成数据为实现可扩展、自动化的模型遗忘提供了有效途径，减少人工干预。

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [8] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: BrowseComp-Plus通过固定语料库和人工验证提升深度研究系统的评估公正性、透明度和控制力，有助于深入分析检索与模型的性能关系。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准依赖动态网页API导致的公平性和可控性不足的问题。

Method: 基于BrowseComp构建固定、人工验证的语料库，包含支持文档和挑战性负样本，设计严谨的评估实验。

Result: 验证表明该基准能有效区分不同深度研究系统性能，提升评估的公平性和深度。

Conclusion: BrowseComp-Plus为深度研究系统提供了更透明、可控和细粒度的评估平台，有助促进未来技术改进。

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [9] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: 研究了不依赖于BPE训练中merge list的推断算法对模型性能的影响，发现非目标偏离方法影响较小，有助于提升隐私性。


<details>
  <summary>Details</summary>
Motivation: 揭示传统BPE merge list可能带来的隐私风险，并探索不依赖该list的推断方法以减少信息泄露。

Method: 比较目标偏离和非目标偏离的BPE推断方法对语言模型性能的影响，通过多任务实验验证结果。

Result: 非目标偏离的BPE推断几乎不影响模型性能，而目标偏离显著降低性能，展示了更安全的tokenization方案的潜力。

Conclusion: 非目标偏离的BPE推断算法可作为隐私保护的替代，保持模型性能的同时降低风险。

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [10] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: 这项研究分析了大型语言模型中的刻板印象偏见和偏差偏见，发现所有模型都存在明显的偏见，可能带来潜在危害。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在各领域广泛应用，识别其偏见以减少潜在风险变得重要。

Method: 通过让四个先进的LLMs生成个人资料，观察其在政治、宗教、性取向等方面的偏见和偏差。

Result: 所有模型都表现出显著的刻板印象偏见和偏差偏见，显示模型存在潜在偏见问题。

Conclusion: 该研究揭示了LLMs在推断用户属性时的偏见问题，提示需注意模型偏见带来的潜在伤害。

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [11] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: LLM通过平行句子实现最佳翻译效果，语法资料辅助提升，但不足以单独使用。


<details>
  <summary>Details</summary>
Motivation: 探讨低资源语言（如Kanuri）在特定领域内的机器翻译潜力，尤其是在缺乏数字资源情况下。

Method: 设计两套语料库（健康/人道事务和通用术语），结合语法、词典与平行句，评估不同资料对LLM翻译的影响，采用自动和人类评估指标。

Result: 平行句是最优资料源，优于语法和词典；语法虽提升部分表现，但不足以单独使用；人类评估显示模型更擅长保持意思而非语法。

Conclusion: 多维度评估对LLM翻译性能有益，纯语法资料不能完全支撑领域翻译，平行句是关键资源。

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [12] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: 研究链式思考提示法对偏见的影响，发现偏见思维与输出偏见相关性不高。


<details>
  <summary>Details</summary>
Motivation: 探索偏见模型中思考步骤是否存在偏见，以及链式思考提示对模型公平性的影响。

Method: 在五个常用大模型上进行公平性测试，使用多项偏见指标分析思考步骤与输出偏见的关系。

Result: 偏见思考与偏见输出关联度低，偏见模型不一定有偏见的想法。

Conclusion: 模型的偏见表现在输出上，不必然反映其内部思考过程，有助于理解和缓解模型偏见。

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [13] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: 提出一种统计框架用于识别和估算自我偏差，确保模型评价的真实性，并分析了大型语言模型的自我偏差问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs作为评估工具的应用日益增多，识别并减少其自我偏差显得尤为重要，以确保评估的真实性和可靠性。

Method: 建立统计模型，比较LLM作为评委对自己输出与他人输出的评分差异，同时考虑第三方人类评价的影响，分离出自我偏差。

Result: 实验证明该方法能够有效识别和量化模型的自我偏差，发现某些模型如GPT-4o和Claude 3.5 Sonnet倾向于高估自己的输出，并存在家族偏差。

Conclusion: 该研究揭示了LLM评价中潜在的偏差问题，为未来优化自动评估提供了实践指导，强调了理解和校正偏差的重要性。

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [14] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: 该研究提出一套基于大规模语言模型的日本裔美国人监禁口述历史自动语义与情感标注框架，有助于提升档案分析的效率与规模化应用。


<details>
  <summary>Details</summary>
Motivation: 解决口述历史档案由于非结构化、情感复杂及标注成本高而难以大规模分析的问题。

Method: 结合专家标注、多轮提示设计及多模型评估，利用ChatGPT、Llama、Qwen进行语义和情感自动标注。

Result: 模型在语义分类和情感分析中表现优异（最高F1达88.71%），成功标注超过9万句，验证了LLMs的有效性。

Conclusion: 设计合理的提示可引导大模型高效应用于文化敏感的文化遗产分析，提供了可复用的标注流程与实践指南，有助于数字人文学科的伦理与技术融合。

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [15] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: 本文提出多轮越狱（multi-turn jailbreaking）概念，并设计了MTJ-Bench基准，揭示了LLMs在连续对话中易被越狱的安全隐患，呼吁提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）支持长上下文和多轮对话，现有单轮越狱研究不足以抓住多轮环境中的潜在风险。

Method: 作者构建多轮越狱基准（MTJ-Bench），在不同模型上测试多轮越狱的效果，分析其安全威胁。

Result: 揭示多轮越狱的易发性，提供了多轮越狱的实证数据和洞见。

Conclusion: 多轮越狱构成更大安全威胁，需社区合作提升LLMs的安全性。

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [16] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: SEVADE是一个针对讽刺检测的多智能体分析框架，通过动力推理和解耦评估提升准确性与抗幻觉能力。


<details>
  <summary>Details</summary>
Motivation: 传统大模型在讽刺检测中存在单一视角、静态推理及幻觉风险，影响表现。

Method: 引入动态推理引擎(DARE)和解耦评估机制，结合多智能体多角度分析文本。

Result: 在四个基准数据集上取得优异性能，准确率提高6.75%，Macro-F1提升6.29%。

Conclusion: 提出的SEVADE框架有效解决了现有模型的局限性，增强了讽刺检测的准确性和鲁棒性。

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [17] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: 提出了一种错误类型和泛化能力的标注框架，为生成适合语言学习的反馈提供数据集及评估方法。


<details>
  <summary>Details</summary>
Motivation: 目前的自动作文评价系统虽能纠正语法错误，但不够适合语言学习，缺乏考虑错误原因的反馈。

Method: 设计错误类型分类体系，收集带有人工反馈的学员错误数据，评估不同提示方式的语言模型反馈生成效果。

Result: 构建了数据集，评估出不同生成方法的表现，帮助优化教育环境中的自动反馈系统。

Conclusion: 该研究为自动生成有针对性和教育意义的反馈提供了基础，有助于提升语言学习效果。

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [18] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: 本篇论文开发了基于Tacotron 2和HiFi-GAN的马尼普里语文本到语音系统，支持调性语音特性，推动语言保护。


<details>
  <summary>Details</summary>
Motivation: 为了保护和促进马尼普里语的数字应用，开发一款适应调性和有限资源条件的TTS系统。

Method: 采用Tacotron 2和HiFi-GAN架构，建立Meitei Mayek字符映射至ARPAbet，整理单一语者数据集。

Result: 实现了自然清晰的语音合成，验证显示系统有效。

Conclusion: 为马尼普里语的语言保护和技术包容奠定基础。

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [19] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: 提出一种基于标签相似性的自动标签对齐方法，用于多源NER数据集的融合和低资源金融领域的命名实体识别提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据集融合方法缺乏可解释性和可扩展性，构建高质量标注数据集成本高，亟需更高效的融合策略以促进NER发展。

Method: 结合经验和语义相似性，采用贪婪两两合并策略进行标签空间统一，验证在多数据集融合和金融领域小样本情境中的效果。

Result: 实现多源数据集的有效融合，提升低资源金融领域的NER性能，展示方法的效率、可解释性和扩展性。

Conclusion: 提出的基于标签相似性的自动融合方法为多源NER数据集整合提供了可扩展且直观的解决方案，促进了低资源环境下的NER研究。

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [20] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: ReQAP系统支持用户对多源异构数据进行复杂问答，采用递归分解和轻量级语言模型实现高效、可追溯的答案生成。


<details>
  <summary>Details</summary>
Motivation: 随着设备上大量个人信息的积累，用户需要能从多源异构数据中进行复杂查询的系统，提升信息理解与信任。

Method: 通过递归分解问题，逐步构建操作树，结合轻量级语言模型进行问答理解与执行，展示详细的答案追溯。

Result: ReQAP实现了对复杂用户问答的支持，并具备答案追溯能力，增强系统的可靠性和用户信任。

Conclusion: 递归分解与轻量级模型结合，使多源异构数据的复杂问答成为可能，提升了系统的实用性和透明度。

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [21] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: 提出了一种新颖的两阶段框架，用于古兰经问答，包括通过模型集成改进段落检索，以及利用指令调优的大模型进行答案抽取，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 解决古兰经问答中的语言复杂性和语义丰富性带来的挑战，提升问答系统的性能。

Method: 采用模型集成的阿拉伯语模型进行段落检索，结合指令调优大模型进行少样本提示的答案抽取。

Result: 在2023年古兰经问答竞赛中达到最优效果，检索指标如MAP@10和MRR@10显著提高，抽取指标pAP@10也有优异表现，优于以往方法。

Conclusion: 结合模型集成与指令调优技术，有效应对专用领域低资源问答的挑战，提升整体系统性能。

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [22] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: 提出了一种名为SBS的对话生成框架，通过在训练中引入质量评分，增强模型的人格一致性，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前对话模型在确保人格一致性方面能力不足的问题，改善训练过程以提升对话质量。

Method: 引入Score-Before-Speaking (SBS)框架，将响应及其质量评分联合学习，利用名词替换增强响应多样性，基于语义相似性进行评分。

Result: 实验显示该方法在PERSONA-CHAT和ConvAI2数据集上显著提升对话的人格一致性和响应质量，尤其在大规模模型中效果明显，输入评分增强训练优于传统方式。

Conclusion: 通过在训练中融入质量评分，SBS能有效提升对话模型的人格一致性和响应质量，为未来生成式对话系统提供一种有效的训练策略。

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [23] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: 提出了一种新颖的基于情感分布稳定性分析的模型无关检测方法，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型的发展，生成内容的识别变得愈加困难，现有检测方法存在泛化能力有限的问题。

Method: 定义两种情感分布一致性指标，分析情感变化和语义保持下的情感分布稳定性，构建SentiDetect框架。

Result: 在多个数据集和多款LLMs上实验，显著优于现有方法，提升检测性能，并增强对抗变换的鲁棒性。

Conclusion: SentiDetect通过情感分布分析提供一种有效、稳健的检测技术，为区分机器生成和人工文本提供新的思路。

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [24] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的增强方法ARCE，用于改进建筑工程领域的命名实体识别任务，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决专业文本中的命名实体识别受到领域差异影响、模型性能不足的问题，探索利用大语言模型自动生成知识以增强小模型。

Method: 利用大语言模型生成简单直白的解释语料库，先对RoBERTa模型进行预训练，再进行微调。

Result: 在建筑工程数据集上实现了77.20%的Macro-F1分数，优于现有方法，发现简单解释性知识比复杂角色性推理更有效。

Conclusion: ARCE通过自动生成并利用简单解释性知识，有效提升了建筑工程文本的NER性能，展现出新的研究方向。

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [25] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches是一个用于医疗问答的创新基准数据集，涵盖4173个临床案例，支持多步骤推理和多样化提问，促进提升大型语言模型在高风险医疗应用中的可靠性与解释性。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型在复杂医疗推理中的能力，建立一个具有实际临床路径和多样提问形式的高质量数据集。

Method: 通过半自动化流程将医疗决策路径转化为临床病例，并生成对应问答，数据涵盖多种医疗主题，支持开放式和选择题，确保推理链的临床验证。

Result: 建立了包含4063个病例、17个医疗主题的结构化数据集，突出多步骤推理能力，拓展LLMs在结构化检索增强生成中的应用潜力，提高模型的可信度和临床可靠性，亦具备教学价值。

Conclusion: HealthBranches推动了高风险医疗场景中可信、可解释AI的发展，为未来临床智能提供基础和参考。

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [26] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: 提出一种渐进式训练方法，将预训练模型有效转换为1位模型，显著提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 如何在保持性能的同时，将预训练LLMs高效二值化，从而降低训练成本。

Method: 引入一致性渐进训练、二值化感知初始化及双尺度补偿策略，逐步将浮点权重转换为二值。

Result: 在多个规模的LLMs上验证，方法优于现有技术，实现了无需从零训练、高性能的1-bit LLM。

Conclusion: 通过渐进式训练和特殊初始化策略，有效减缓二值化带来的性能损失，实现预训练模型的高效二值化。

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [27] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: Vec2Summ是一种基于语义压缩的新颖抽象式摘要方法，利用语义空间中的平均向量表示整个文集，通过生成模型解码，实现对主题强调和控制，兼备效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模语言模型在文集摘要中受限于长度、可控性和扩展性的问题，亟需一种高效、可控且能实现语义抽象的摘要方法。

Method: 将文档集合表示为语义嵌入空间中的平均向量，通过反向嵌入将向量解码为自然语言摘要，引入高斯随机性增强多样性和鲁棒性。

Result: 在保持主题覆盖和效率的同时，生成连贯、焦点明确的摘要，性能与直接使用LLM的摘要相当，但细节略少，展现了良好的可伸缩性和语义控制能力。

Conclusion: Vec2Summ通过语义压缩和生成模型实现高效、可控的文集摘要，特别适合强调扩展性和语义层次的场景，弥补了传统LLM摘要的不足。

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [28] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: SEADialogues是一个面向东南亚文化的多语言对话数据集，包含个性和文化话题，旨在推动文化敏感型对话系统的发展。


<details>
  <summary>Details</summary>
Motivation: 弥补现有对话数据集缺乏文化细节的不足，促进文化敏感型对话系统研究。

Method: 收集八种东南亚语言的多轮对话，加入个性属性和文化话题，构建多样化、地区特色明显的数据集。

Result: 推出了SEADialogues数据集，为文化意识和人本导向的对话模型提供支持，促进多语言、多文化背景下的对话技术发展。

Conclusion: 该数据集增强了对话系统的文化理解能力，有助于实现更贴近本地文化的个性化交互。

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [29] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出BharatBBQ，针对印度文化和多语种的偏见检测基准，揭示多语言模型在偏见方面的持续存在和增强。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估工具缺乏对印度语境的适应性，亟需开发文化匹配的偏见检测基准。

Method: 构建多语种偏见基准BharatBBQ，涵盖13个社会类别，收集多语数据，通过翻译和验证扩展样本，并对多语模型进行偏见评估。

Result: 在多语模型中发现偏见普遍存在，印度语语言模型偏见甚至比英语更为严重，验证了文化和语言背景对偏见检测的重要性。

Conclusion: 文化和语言背景是偏见评估的重要因素，需针对不同文化背景设计专属评估工具，以实现更公平的AI系统。

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [30] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: LessIsMore是一种无需训练的稀疏注意力机制，通过全球注意力模式提升推理效率，在确保准确率的同时显著加快解码速度。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型在短输入中因生成大量token导致的计算开销问题，同时保持准确性。

Method: 引入LessIsMore机制，结合局部注意力头的token选择与全局上下文信息，进行跨头token排序，实现统一的token选择策略。

Result: 在多类推理任务中，该方法在保持或提升准确率的同时，平均提速1.1倍，减少关注tokens数至一半以上，整体加速1.13倍，比现有稀疏注意力更高效。

Conclusion: LessIsMore有效提升推理模型的速度和效率，无需额外训练，适应多样推理场景，助力大模型的实用性和扩展性。

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [31] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 本文扩展了对大语言模型偏见的研究，从单一轴向偏见扩展到交叉偏见，提出了新基准WinoIdentity，并通过核心信心差异度检测模型在多重身份交集中的偏差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在社会决策中的应用，理解其偏见尤其是交叉偏见的重要性日益增强，以减少潜在的社会伤害。

Method: 作者创建了包含多维度人口标志的WinoIdentity基准，通过检测模型在不同交叉身份上的核心指称置信度差异评估偏见，评估了五个主流大模型。

Result: 发现模型在多种交叉偏见身份上表现出高达40%的置信差异，特别是在双重劣势身份中表现最差，且对特权身份也存在置信度下降，显示偏见和性能问题可能源于记忆而非推理，二者可能共同引发社会伤害。

Conclusion: 强调交叉偏见检测的重要性，提示模型在偏见和价值一致性方面仍需改进，以避免在社会中的潜在危害。

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [32] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: 该论文从哲学角度分析自动语音识别中的偏见问题，强调非标准方言的误识别不仅是技术问题，更涉及对边缘群体的不尊重，呼吁技术应尊重语言多样性以实现公平。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统偏见问题未被充分研究，存在技术与伦理困境。

Method: 从哲学视角分析ASR偏见，区分不同类型的歧视，提出伦理维度，批判现有发展模式。

Result: 揭示ASR偏见的伦理复杂性，强调语言多样性和用户自主性，提出非技术性的解决思路。

Conclusion: 改善ASR偏见需超越技术修正，重视语言多样性和文化尊重，以实现公平与包容。

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [33] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: 提出SafeGrad方法，通过梯度裁剪解决微调中安全与任务性能冲突问题，有效抵抗恶意样本攻击。


<details>
  <summary>Details</summary>
Motivation: 随着微调即服务的普及，保护LLMs免受恶意样本影响变得至关重要。

Method: 引入梯度手术技术，在检测到冲突时，投影有害梯度以保持安全与任务结合。结合KL散度对齐损失，提升鲁棒性与数据效率。

Result: 实验证明SafeGrad在多种模型和数据集上都能有效防御恶意样本攻击，保持任务性能和安全性。

Conclusion: SafeGrad是一种有效的防御方法，增强了微调过程中模型的安全性和鲁棒性。

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [34] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: 提出Omni-SafetyBench，为多模态大型语言模型的安全评估提供全面基准和新指标，揭示模型在复杂多模态输入下的安全弱点。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大型语言模型（OLLMs）的崛起，安全性成为亟需解决的问题，现有基准无法满足音视频联合输入或跨模态安全一致性评估的需求。

Method: 设计了包含24种模态组合的Omni-SafetyBench基准，并提出安全得分和跨模态安全一致性得分两个指标，用于评估模型安全性。

Result: 评估显示大部分模型在整体安全和一致性方面表现平平，复杂输入和音视频联合输入显著削弱模型安全性，存在严重漏洞。

Conclusion: 该研究为多模态模型安全评估提供了基础工具和指标，强调增强安全性的迫切性，为未来优化指明方向。

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [35] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: 通过去噪伪兴趣，提升个性化标题生成的准确性，有效缓解噪声干扰，获得优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了历史点击数据中的噪声，影响个性化标题生成效果。

Method: 提出PHG-DIF框架，利用双阶段过滤和多级时间融合处理点击噪声，动态建模用户兴趣。

Result: 在新数据集DT-PENS上实现SOTA效果，有效改善标题质量。

Conclusion: 该方法通过去噪伪兴趣，提升个性化标题生成的准确性，具有实际应用价值。

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [36] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 提出一种自动提取企业数据管道中细粒度schema血缘关系的框架，利用不同大小的语言模型实现准确的血缘信息提取，解决语义漂移问题，提高数据可追溯性。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道中的复杂变换导致语义漂移，影响数据可追溯性和治理，亟需准确的血缘关系提取方法。

Method: 设计了多语言脚本的细粒度schema血缘关系提取框架，并提出了SLiCE评估指标，用不同规模的语言模型进行实验验证。

Result: 模型规模和提示复杂度提升血缘关系提取性能，32B开源模型能达到接近GPT系列的效果，验证了方法的可扩展性和经济性。

Conclusion: 该方案为企业数据治理提供了可扩展、经济的血缘关系提取工具，有助于改善语义漂移带来的问题。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [37] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: DySK-Attn框架通过引入稀疏知识注意机制，促进LLMs高效地集成实时动态知识，显著提升事实准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs知识静态、快速过时的问题，避免重训和现有知识编辑方法的低效与副作用。

Method: 结合动态知识图和稀疏注意机制，实现对知识的粗到细搜索，提升相关信息的检测效率。

Result: 在时效性问答任务中，优于传统RAG和模型编辑方法，增强事实准确性和计算效率。

Conclusion: 提出的DySK-Attn为构建紧跟时代的实用LLMs提供了可扩展的解决方案。

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [38] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: 一种基于大型语言模型的时间序列预测新框架TALON，通过模型时间异质性和语义对齐，实现更强的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在时间序列预测中面临的异质性和模态差异问题，提升预测效果。

Method: 设计异质时间编码器进行局部专家建模，以及语义对齐模块以弥合模态差异，无需手工提示。

Result: 在七个真实数据集上优于最新方法，平均MSE提升11%。

Conclusion: 结合模式感知和语义感知设计，有效提升LLMs在时间序列预测中的表现。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [39] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: 提出一种针对社交媒体任务的持续预训练策略PEP，通过预测传播结构中的关系，增强预训练模型在谣言检测中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在社交媒体任务中的表现不足，原因在于训练语料与社交文本不匹配、对社交符号处理不足以及预训练任务不适应传播结构中的用户互动。

Method: 引入Post Engagement Prediction（PEP）策略，结合大规模Twitter语料库，训练一种面向Twitter的预训练模型SoLM。

Result: PEP提升了模型在谣言检测任务中的准确性，优于多个基线和现有方法，尤其在少样本场景表现突出。

Conclusion: 通过结合传播结构信息和引入PEP策略，显著改善了预训练模型在社交媒体中的应用效果，验证了该方法的有效性。

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [40] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: 神经网络在语音应Stress识别中表现优异，通过LRP分析揭示其决策依据，主要依赖于重读元音的频谱特征。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在语音压力识别中的决策机制，解决黑箱问题。

Method: 构建英语言语数据集，训练多种CNN模型预测双音节词的重读位置，利用LRP进行模型解释，分析特征重要性。

Result: 模型准确率达92%，LRP显示模型主要依赖重读音节的频谱特征，特别是元音的共振峰。

Conclusion: 深度学习模型能从自然语料中学习到压力的多分布线索，扩展传统的实验刺激范围，提升语音压力识别理解。

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [41] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: 提出结合提示调优和记忆示范策略的方案，有效解决少样本连续命名实体识别中的知识蒸馏难题。


<details>
  <summary>Details</summary>
Motivation: 针对少样本连续学习命名实体识别中的知识蒸馏困难和少样本困境，寻求有效解决方案。

Method: 设计可扩展的锚词导向提示调优（APT）以及引入记忆示范模板（MDT）策略，增强模型泛化和知识蒸馏能力。

Result: 在少样本连续命名实体识别任务中，该方法表现出竞争力的性能。

Conclusion: 通过提示调优和记忆示范，有效缓解少样本蒸馏困境，提高模型性能。

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [42] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 本文扩展了二维动态发音模型DYNARTmo，引入了三维硬腭模型，支持语音发声中舌头与硬腭接触面积的估算及多视图显示，增强了模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 提升语音运动模型的三维表现力，实现更加真实的发音模拟和辅助教学。

Method: 在二维模型基础上，加入硬腭的三维表示，设计两种不同的几何形状用于模拟侧弯，通过解析方法计算接触点，增强模型的视觉和功能表现。

Result: 模型成功实现了多视角同步显示，并通过模拟硬腭轮廓改善了舌腭接触区域的估算，为语音学的教育和治疗提供了更丰富的工具。

Conclusion: 该扩展模型增强了二维发音模型的空间表现能力，为未来增加人脸视角和语音合成等功能奠定了基础。

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [43] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 该研究提出两种方法将上下文副语言信息融入大规模语音语言模型 training，包括显式提供情感元数据和隐式生成问答对，显著提升模型的共感推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有Speech-LLMs在共感推理方面受限，缺乏结合语境内容和副语言线索的训练数据。

Method: 通过显式提供情感元数据和隐式自动生成问答对两种策略，将上下文副语言信息融入模型训练。

Result: 隐式方法提升性能38.41%，结合两者后提升至46.02%，验证副语言信息在理解中的有效性及模型评估的可靠性。

Conclusion: 引入上下文副语言信息能显著改善Speech-LLMs的共感推理能力，改善模型的理解和评估表现。

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [44] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: MAQuA是一种基于大模型的多维心理健康筛查框架，通过智能选择问题，大幅减少评估所需问题数，提高效率与准确性，适用于临床实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在心理健康评估中的应用潜力增强，如何提升筛查效率和减少患者负担成为关键问题。

Method: 结合多目标建模、项目反应理论和因子分析，MAQuA在多维度上智能选择最具信息的问卷问题，以优化诊断效果。

Result: 在新数据集上，MAQuA显著减少所需问题数（ Depression 71%、Eating Disorder 85%） ，表现稳定，适用于多种心理健康领域。

Conclusion: MAQuA是一种高效、稳健的心理健康筛查工具，有助于实现大模型在临床的广泛应用与优化。

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [45] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: 本文系统评估了14种大型语言模型在27个伦理相关的电车难题中的表现，分析其道德推理能力及偏差，提出了模型道德引导的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型应用于伦理敏感决策，理解其道德推理过程具有重要意义，迫切需要系统评估模型的伦理表现。

Method: 采用分因素提示协议，收集模型在不同伦理框架下的二元决策和自然语言理由，共分析3780个样本，评估模型的决策果断性、一致性、公众伦理一致性及对无关线索的敏感性。

Result: 不同类型模型表现差异显著，增强推理模型表现出更高的果断性和结构化理由，但不一定更符合人类共识；在利他主义、公平和美德伦理等框架下存在“甜点区域”，实现高干预率、低冲突和接近人类判断；在强调亲情、法律或自利的框架下，模型可能产生争议性结果。

Conclusion: 道德引导不仅影响模型行为，还能作为揭示潜在伦理偏好的诊断工具。建议将道德推理作为模型对齐的核心指标，推动标准化伦理评估基准的发展。

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [46] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: 提出了一个跨语言和跨模态的事实性基准（CCFQA）用以评估多模态大语言模型的事实性能力，尤其是在多语种语音任务中的表现，并通过少样本迁移学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多语世界中的普及，确保其事实性准确性尤为重要，但现有评估大多集中在英文和单一模态，未能充分覆盖多语种多模态场景。

Method: 设计了包含8种语言的平行语音文本问答数据集CCFQA，并提出少样本迁移学习策略，将英语问答能力迁移到多语种语音问答任务中。

Result: 实验证明当前多模态大模型在CCFQA上表现仍有明显不足，通过少样本迁移学习方法实现了与GPT-4o-mini-Audio相近的结果。

Conclusion: CCFQA为多模态多语种模型的评估提供了新标准，所提出的迁移学习策略有效提升了模型的多语种语音理解能力，有助于推动多模态模型的鲁棒性和可靠性发展。

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [47] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 引入ObfusQA框架，评估大规模语言模型在面对模糊化问题时的鲁棒性，发现模型在复杂变换下容易失败或产生幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大模型在面对模糊或变换问题时鲁棒性的系统评估。

Method: 提出ObfusQA, 一套多层次模糊化评测框架，涵盖命名实体、干扰项和语境 overload 三个维度。

Result: 模型在面对复杂模糊化问题时表现出较弱的鲁棒性，容易失败或产生幻觉。

Conclusion: ObfusQA有助于推动模型鲁棒性研究，框架已公开以供使用和未来改进。

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [48] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: 研究了使用聊天机器人进行跨语码切换的可行性及其对参与者的影响，发现规律性强的切换能增强用户体验和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 理解多语使用中的码-switch特征，探索技术辅助的语言研究方法。

Method: 开发使用中英混合的对话机器人，进行两组实验，检测不同切换策略对用户体验和任务完成的影响。

Result: 规律性强的码-switch行为提升了用户体验和任务完成率，随机或语法错误的切换则造成负面影响。

Conclusion: 多语切换行为的规范性对交互效果至关重要，智能科技既有挑战也有助力于研究多语现象。

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [49] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: 提出TurnGuide，通过动态划分对话回合和生成文本引导，有效改善端到端全双工对话模型的语义一致性和自然流畅度。


<details>
  <summary>Details</summary>
Motivation: 解决全双工语音模型在实际对话中因对话长度和资料限制导致的能力下降问题。

Method: 采用仿人类对话规划的TurnGuide方法，动态划分对话回合并在输出前生成文本指导，以改善时间同步和内容质量。

Result: 显著提升端到端全双工对话模型的交互自然性、语义连贯性，使模型能更真实地模拟人类对话。

Conclusion: TurnGuide方法有效补充了有限的语音数据，优化了对话模型的表现，推动自然语音交互的发展。

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [50] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: 通过文化知识图谱构建大规模多模态数据集，提升多模态大模型在文化理解方面的性能，达到多语种、多文化的包容性。


<details>
  <summary>Details</summary>
Motivation: 弥补多模态大模型在文化实体理解和低资源语言中的性能不足，推动全球多文化包容性。

Method: 利用维基数据构建知识图谱，采集代表文化实体的图像，生成多语种VQA数据，训练文化知识融入的多模态大模型CulturalPangea。

Result: CulturalPangea在多文化、多语种、多模态任务中表现优异，超越先前模型，显著缩小文化差距，同时保持通用能力。

Conclusion: 文化知识的针对性融入显著提升多模态模型的文化理解能力，为全球多文化融合提供可行路径。

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [51] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: ReLoc是一种结合局部搜索的代码生成优化框架，通过逐步修订提升代码质量，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在代码生成中的效率与扩展性挑战，尤其是树搜索与改进策略的局限性。

Method: 提出ReLoc框架，结合邻域代码生成、评价与更新，支持多种搜索算法，并开发特色的修订奖励模型。

Result: 在多样化任务中表现出色，显著优于传统树搜索和最先进的改进方法。

Conclusion: ReLoc展现出高效、灵活和优越的代码生成能力，有望推动相关技术发展。

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [52] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: 大规模语言模型在长输入中表现出位置偏差，主要表现为距离偏差而非传统的早晚偏差，信息的检索效果对推理能力至关重要。


<details>
  <summary>Details</summary>
Motivation: 理解长输入下语言模型的偏差表现，优化长文本处理能力，提高模型推理的准确性。

Method: 通过相对输入长度分析不同位置偏差，重点研究输入占比对模型表现的影响，并探讨检索在推理中的作用。

Result: 发现LiM效应在输入占比50%以内最强，之后变弱或消失，转而展现距离偏差，即信息越接近输入末端越好。检索效果影响推理表现，偏差源自检索机制。

Conclusion: 长输入偏差由检索机制主导，设计更好的检索和输入策略能改善模型性能，为长文本任务和评测提供指导。

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [53] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 提出ALOPE框架，利用层级适应和多策略提升大模型在机器翻译质量估计中的性能，强调中间层表示的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在机器翻译质量估计中存在的表现不足，特别是在低资源和跨语言场景下的挑战。

Method: 引入层级适应机制，结合低秩适配器与多头回归策略，优化Transformer层的表示以改善回归任务性能。

Result: 在多项指标上优于现有方法，验证中间Transformer层的跨语言适应性，模型和代码已公开。

Conclusion: 通过结构优化提升LLMs在复杂跨语言回归任务中的能力，为未来相关研究提供基础和工具。

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [54] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 提出一种利用拓扑数据分析识别GPT-2模型中导致偏差的注意头的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管已有偏差检测方法，但识别模型中具体负责偏差的部分仍不充分。

Method: 使用拓扑数据分析技术，分析GPT-2中的注意头与偏差关系。

Result: 发现某些注意头作为热点区域，集中表现特定类别偏差，提出了可以识别偏差头的指标。

Conclusion: 该方法可用于识别偏差源头，为未来去偏工作提供技术基础。

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [55] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: 提出ThemeClouds，用大语言模型生成基于主题的关键词云，突出内容的重要性和多样性，优于传统频率云。


<details>
  <summary>Details</summary>
Motivation: 传统词云在对话背景下难以准确反映内容多样性，需更有代表性的可视化工具。

Method: 利用大语言模型识别语料中的主题，并根据提及该主题的不同参与者数量生成词云，实现基于主题广泛性而非单词频率的可视化，用户可自定义参数。

Result: 在用户研究中，ThemeClouds比传统频率云和LDA、BERTopic等主题模型表现出更敏感的结果，尤其在揭示设备关切方面表现出优势。

Conclusion: 通过引入LLM辅助的主题词云，可增强定性分析的效果，提供更灵活、透明的研究工具，并支持交互式分析。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [56] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习可验证奖励（RLVR）中LLMs的探索能力，包括探索空间、信息熵与性能的关系，以及优化方法，为未来RLVR系统的完善提供基础理论。


<details>
  <summary>Details</summary>
Motivation: 揭示RLVR中LLMs探索行为的机制，提升其推理能力，弥补前人研究的不足。

Method: 通过构建定量指标，分析训练阶段、单例及Token级别的探索表现，结合新旧实证数据，研究探索空间的塑造、信息熵与性能的关系及优化策略。

Result: 提出了衡量LLMs探索边界的指标，揭示信息熵与性能的动态关系，并识别了增强探索效率的优化方法，为RLVR系统的性能提升提供理论基础。

Conclusion: 本文系统分析了RLVR中LLMs的探索动力学，建立了基础框架，推动RLVR技术的理论理解和实践发展。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [57] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [58] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: KeyCP++通过关键词引导链式思考，有效提升了单样本事件检测的准确性，解决了模型对事件触发词理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在事件检测中存在对触发词理解不准确导致的过度解读问题，亟需改进方法以增强模型推理能力。

Method: 提出KeyCP++，一种关键词中心的链式思考提示方法，通过自动标注逻辑空白并构建触发词判别模板，结合示例触发词作为锚点引导模型推理。

Result: 大量实验显示，该方法在单样本事件检测任务中显著优于传统方法，有效改善检测效果。

Conclusion: 关键词引导的链式思考策略，提升LLM事件检测的性能，为少样本学习提供了新思路。

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [59] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart是一个用于评估视觉-语言模型在多图表环境下推理能力的基准测试，涵盖从简单事实推理到复杂多步推理的多个难度级别，揭示了模型在复杂任务中的性能下降和多实体图表理解的挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在推动多模态推理的发展，特别是在需要跨图表整合和复杂推理的实际应用场景中。

Method: 设计了包含不同难度层次的基准测试，评估多种先进的视觉-语言模型在多图表推理任务中的表现，并分析模型在任务复杂度增加时的性能变化。

Result: 发现模型性能在面对复杂图表时显著下降，表现出对跨图表整合和多实体理解的不足，揭示了现有模型的局限性。

Conclusion: InterChart为多模态推理提供了一个系统性测试平台，加深了对模型在复杂、多视觉场景中的理解和处理能力的认识，为未来模型的改善指明了方向。

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [60] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: LoSemB框架通过逻辑引导的语义桥接，实现了在工具检索中的归纳能力，克服了分布偏移和相似性检索的局限，提升了大语言模型中工具利用的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着工具库的不断扩展，将所有工具包含在LLMs的输入长度内变得不现实，因此需要一种能有效应对未见工具的检索方法。

Method: 提出基于逻辑的嵌入对齐和关系增强的检索机制，利用逻辑信息实现工具的归纳检索，无需频繁重训练。

Result: 实验显示LoSemB在归纳场景中表现出优越性能，同时在传导场景中也具有良好效果。

Conclusion: LoSemB通过引入逻辑引导的方法，有效增强了大语言模型中工具的归纳检索能力，为工具库扩展提供了新的解决方案。

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [61] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 商业大模型在句子填空任务上表现较差，尤其在低结构域中，显示出其全局连贯性不足。


<details>
  <summary>Details</summary>
Motivation: 探究现有大模型在长文本推理和全局连贯性方面的能力，尤其是在结构不强的任务中表现如何。

Method: 对三款商业大模型（GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 Flash）在不同领域的Mask句子预测任务中进行评估，考察其保真度和连贯性。

Result: 模型在低结构域中，预测Masked句子时表现较差，揭示了其在全局连贯性和长文本推理方面的不足。

Conclusion: 当前大型模型在全局连贯性和长距离信息预测方面仍存在明显差距，亟需改进以应对复杂文本任务。

Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

</details>


### [62] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
*Zhenliang Zhang,Junzhe Zhang,Xinyu Hu,HuiXuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 社会偏见是大规模语言模型中信实性幻觉的主要原因之一，利用结构因果模型（SCM）和偏见干预数据集（BID）验证了偏见对幻觉的因果关系。


<details>
  <summary>Details</summary>
Motivation: 探究社会偏见是否导致大语言模型中的信实性幻觉，这是该领域尚未充分研究的问题。

Method: 采用结构因果模型（SCM）建立偏见与幻觉的因果关系，设计偏见干预措施和偏见干预数据集（BID）进行实验验证。

Result: 偏见是幻觉的重要原因，不同偏见状态影响幻觉的方向不同，偏见在不公平幻觉中尤为显著。

Conclusion: 社会偏见显著影响大模型的信实性，需通过偏见干预减缓幻觉问题。

Abstract: Large language models (LLMs) have achieved remarkable success in various
tasks, yet they remain vulnerable to faithfulness hallucinations, where the
output does not align with the input. In this study, we investigate whether
social bias contributes to these hallucinations, a causal relationship that has
not been explored. A key challenge is controlling confounders within the
context, which complicates the isolation of causality between bias states and
hallucinations. To address this, we utilize the Structural Causal Model (SCM)
to establish and validate the causality and design bias interventions to
control confounders. In addition, we develop the Bias Intervention Dataset
(BID), which includes various social biases, enabling precise measurement of
causal effects. Experiments on mainstream LLMs reveal that biases are
significant causes of faithfulness hallucinations, and the effect of each bias
state differs in direction. We further analyze the scope of these causal
effects across various models, specifically focusing on unfairness
hallucinations, which are primarily targeted by social bias, revealing the
subtle yet significant causal effect of bias on hallucination generation.

</details>


### [63] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
*Zeyu Yang,Lai Wei,Roman Koshkin,Xi Chen,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出一种基于语法的分块策略用于语义完整单元的划分，并在此基础上开发了结合依存关系解析的端到端同步翻译系统。


<details>
  <summary>Details</summary>
Motivation: 解决同步翻译中保持语义一致性和时间优化的问题，利用句法结构提升翻译效果。

Method: 通过依存关系和标点符号进行语义块划分，结合冰冻的Whisper编码器和解码器-大型语言模型，动态输出翻译或等待指令，处理语序差异。

Result: 在多语言数据集上实现了显著的翻译质量提升，验证了句法结构在同步翻译中的作用。

Conclusion: 该方法有效结合句法信息和端到端系统，增强了同步翻译的准确性与协调性。

Abstract: This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

</details>


### [64] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
*Haoyuan Wu,Haoxing Chen,Xiaodong Chen,Zhanchao Zhou,Tieyuan Chen,Yihong Zhuang,Guoshan Lu,Zenan Huang,Junbo Zhao,Lin Liu,Zhenzhong Lan,Bei Yu,Jianguo Li*

Main category: cs.CL

TL;DR: 提出一种多专家异构（Grove MoE）架构，动态调节专家大小以提升计算效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构专家规模同质，难以根据输入复杂度动态调节计算资源，限制了效率和性能。

Method: 引入异构专家架构，结合动态激活机制，并在训练中应用策略，开发了GroveMoE-Base和GroveMoE-Inst模型。

Result: 模型根据输入复杂度动态激活3.14-3.28B参数，性能接近或超越同类最先进模型。

Conclusion: 异构专家设计与动态激活机制提升了MoE模型的效率和性能，提供了一种更灵活的模型扩展方式。

Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate
scalability by enabling sparse parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.

</details>


### [65] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 大语言模型作为评判者时，策略性说服语言能影响评分，容易被利用来偏差，强化模型的弱点，并不因模型规模而改善。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在自动评分中的脆弱性，尤其是被策略性说服的影响。

Method: 根据信托原理，设计七种说服技术，通过在回应中嵌入这些元素，观察对模型评分的影响。

Result: 说服性语言能提升错误解答的评分偏差，尤其是‘一致性’技术影响最大，且此漏洞在不同模型大小和多技术组合中依然存在。

Conclusion: LLMs评判系统存在被策略性说服干扰的严重风险，应采取措施增强其抗攻击能力。

Abstract: As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

</details>


### [66] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
*Olga Kellert,Muhammad Imran,Nicholas Hill Matlis,Mahmud Uz Zaman,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 本文比较了基于组成规则的情感分析方法与非组成方法VADER在意图焦点分析中的应用，验证了组成方法的有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 弥补FA中缺乏定量评估的空白，探索情感分析中的组成方法在焦点分析中的适用性。

Method: 利用英语中普遍依存句法结构的基本语法规则，结合情感词典实现组成分析，并与简单启发式的VADER方法进行比较。

Result: 提出的组成分析方法在准确性和可解释性方面优于VADER，并成功将情感分析中的组成策略推广应用到焦点分析。

Conclusion: 编制的组成方法能有效提升FA的表现，具有良好的解释性，为FA和NLP的相关研究提供理论基础。

Abstract: This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

</details>


### [67] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
*Yu-Min Tseng,Wei-Lin Chen,Chung-Chi Chen,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: LLMs在专家知识密集的领域中用于数据标注效果有限，单一模型和多智能体讨论均未显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探究在需要专家知识的专业领域中，强大LLMs是否能替代人类标注者，降低标注成本。

Method: 评估单一LLMs及多智能体讨论框架在金融、生物医学、法律领域的标注效果，结合推理模型对比性能。

Result: 单一LLM的推理增强技术效果有限，多智能体讨论中模型行为表现出一定局限性，未实现明显优势。

Conclusion: 在专业领域中，现有的LLMs和多智能体方法对数据标注的帮助有限，需探索更有效的方案。

Abstract: Textual data annotation, the process of labeling or tagging text with
relevant information, is typically costly, time-consuming, and labor-intensive.
While large language models (LLMs) have demonstrated their potential as direct
alternatives to human annotators for general domains natural language
processing (NLP) tasks, their effectiveness on annotation tasks in domains
requiring expert knowledge remains underexplored. In this paper, we
investigate: whether top-performing LLMs, which might be perceived as having
expert-level proficiency in academic and professional benchmarks, can serve as
direct alternatives to human expert annotators? To this end, we evaluate both
individual LLMs and multi-agent approaches across three highly specialized
domains: finance, biomedicine, and law. Specifically, we propose a multi-agent
discussion framework to simulate a group of human annotators, where LLMs are
tasked to engage in discussions by considering others' annotations and
justifications before finalizing their labels. Additionally, we incorporate
reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our
empirical results reveal that: (1) Individual LLMs equipped with inference-time
techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal
or even negative performance gains, contrary to prior literature suggesting
their broad effectiveness. (2) Overall, reasoning models do not demonstrate
statistically significant improvements over non-reasoning models in most
settings. This suggests that extended long CoT provides relatively limited
benefits for data annotation in specialized domains. (3) Certain model
behaviors emerge in the multi-agent discussion environment. For instance,
Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even
when other agents provide correct annotations or valid reasoning.

</details>


### [68] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
*Amrita Singh,H. Suhan Karaca,Aditya Joshi,Hye-young Paik,Jiaojiao Jiang*

Main category: cs.CL

TL;DR: 法律专业的LLMs在合同理解任务中优于通用模型，Legal-BERT和Contracts-BERT表现最佳，引领未来发展。


<details>
  <summary>Details</summary>
Motivation: 弥补缺乏针对合同理解任务的法律专业大型语言模型（LLMs）的系统评估的空白。

Method: 对10个法律专业LLMs与7个通用LLMs在三项英语合同理解任务上的性能进行比较分析。

Result: 法律专业LLMs在任务中表现优越，Legal-BERT和Contracts-BERT在两项任务中达到了最新状态（SOTA），即使参数较少。CaseLaw-BERT和LexLM也是有力的基线模型。

Conclusion: 该评估为法律专业LLMs的性能提供了全面概览，有助于推动更准确的合同理解系统发展。

Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple
legal-specific LLMs currently exists for contract classification tasks in
contract understanding. To address this gap, we present an evaluation of 10
legal-specific LLMs on three English language contract understanding tasks and
compare them with 7 general-purpose LLMs. The results show that legal-specific
LLMs consistently outperform general-purpose models, especially on tasks
requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish
new SOTAs on two of the three tasks, despite having 69% fewer parameters than
the best-performing general-purpose LLM. We also identify CaseLaw-BERT and
LexLM as strong additional baselines for contract understanding. Our results
provide a holistic evaluation of legal-specific LLMs and will facilitate the
development of more accurate contract understanding systems.

</details>


### [69] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本研究评估了多种大型语言模型在捷克语方面的细粒度情感分析（ABSA）任务中的表现，发现领域特定的小模型在零-shot和少-shot场景优于通用模型，而微调的LLMs表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型在捷克语细粒度情感分析中的潜力与局限。

Method: 对19种不同大小和架构的LLMs在零-shot、少-shot和微调三种场景下进行系统评估，分析影响表现的因素，并进行错误分析。

Result: 小型领域特定模型在零-shot和少-shot场景优于通用模型，微调模型达到最新最优效果。多语言、多模型大小和时效性对性能产生显著影响。

Conclusion: LLMs在捷克语ABSA中的应用潜力巨大，但仍存在方面术语预测等关键挑战，未来研究应关注模型调优与特定任务适应性。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

</details>


### [70] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 少量目标语言样本可显著提升跨语言情感分析性能，甚至超过单语模型。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在依据方面的情感分析中数据匮乏的问题。

Method: 在四个ABSA任务中加入少量（十个）目标语言样本，结合两种序列到序列模型进行评估。

Result: 加入少量样本显著提升效果，少于10个样本也优于零样本，结合1000个样本甚至超越纯英语模型。

Conclusion: 少量高质量目标语言样本对跨语言ABSA具有重要提升作用，为低资源场景提供实际解决方案。

Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in
English, yet challenges remain for low-resource languages due to the scarcity
of labelled data. Current cross-lingual ABSA approaches often rely on external
translation tools and overlook the potential benefits of incorporating a small
number of target language examples into training. In this paper, we evaluate
the effect of adding few-shot target language examples to the training set
across four ABSA tasks, six target languages, and two sequence-to-sequence
models. We show that adding as few as ten target language examples
significantly improves performance over zero-shot settings and achieves a
similar effect to constrained decoding in reducing prediction errors.
Furthermore, we demonstrate that combining 1,000 target language examples with
English data can even surpass monolingual baselines. These findings offer
practical insights for improving cross-lingual ABSA in low-resource and
domain-specific settings, as obtaining ten high-quality annotated examples is
both feasible and highly effective.

</details>


### [71] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
*Chen Cecilia Liu,Hiba Arnaout,Nils Kovačić,Dana Atzil-Slonim,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出CultureCare数据集，提升大语言模型在跨文化情感支持中的表现。


<details>
  <summary>Details</summary>
Motivation: 弥补大语言模型在提供文化敏感支持方面资源不足的空白。

Method: 构建跨文化支持数据集，开发多种模型调适策略，并通过多层评估验证其有效性。

Result: 调适后模型优于普通在线回复，展现出潜在的临床培训应用前景。

Conclusion: 大语言模型可通过针对性调适实现文化敏感性，助力心理健康服务和专业培训。

Abstract: Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

</details>


### [72] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
*John C. McDonald,Rosalee Wolfe,Fabrizio Nunnari*

Main category: cs.CL

TL;DR: 提出了一种两参数的情感信号表达方法，通过EASIER符号控制虚拟签字人的情感表情，改善了非手势信号的表达一致性和细腻度。


<details>
  <summary>Details</summary>
Motivation: 满足签名手语中情感非手势信号的标准化和表达需求。

Method: 引入两参数表征情感信号，结合EASIER符号实现对虚拟签名人的情感控制。

Result: 该方法有助于实现更细腻、更一致的情感表达，改善虚拟签名人对情感信号的表现。

Conclusion: 两参数表示法在表达情感非手势信号方面具有潜力，有助于提升签名虚拟人物的自然度和表达效果。

Abstract: Non-manual signals in sign languages continue to be a challenge for signing
avatars. More specifically, emotional content has been difficult to incorporate
because of a lack of a standard method of specifying the avatar's emotional
state. This paper explores the application of an intuitive two-parameter
representation for emotive non-manual signals to the Paula signing avatar that
shows promise for facilitating the linguistic specification of emotional facial
expressions in a more coherent manner than previous methods. Users can apply
these parameters to control Paula's emotional expressions through a textual
representation called the EASIER notation. The representation can allow avatars
to express more nuanced emotional states using two numerical parameters. It
also has the potential to enable more consistent specification of emotional
non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [73] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出GREP评估框架，用于科学写作中相关工作部分的多维度质量评估，优于传统指标，促进人机协作写作。


<details>
  <summary>Details</summary>
Motivation: 解决自动生成科学写作质量评估缺乏专家偏好理解的问题，提升自动生成内容的可靠性。

Method: 设计多轮对话的GREP框架，结合经典评价标准与专家偏好，采用对比学习和多维度细化评估，提供更全面的质量反馈。

Result: 实验证明GREP在相关工作部分的质量评估上优于传统方法，相关性高，能反映自然科学写作场景，也提示当前大模型在满足写作验证要求方面仍有不足。

Conclusion: GREP框架能有效提升科学写作自动评估的准确性和实用性，为人机合作写作提供了新的技术基础。

Abstract: Expert domain writing, such as scientific writing, typically demands
extensive domain knowledge. Recent advances in LLMs show promising potential in
reducing the expert workload. However, evaluating the quality of automatically
generated scientific writing is a crucial open issue, as it requires knowledge
of domain-specific evaluation criteria and the ability to discern expert
preferences. Conventional automatic metrics and LLM-as-a-judge systems are
insufficient to grasp expert preferences and domain-specific quality standards.
To address this gap and support human-AI collaborative writing, we focus on
related work generation, one of the most challenging scientific tasks, as an
exemplar. We propose GREP, a multi-turn evaluation framework that integrates
classical related work evaluation criteria with expert-specific preferences.
Instead of assigning a single score, our framework decomposes the evaluation
into fine-grained dimensions. This localized evaluation approach is further
augmented with contrastive few-shot examples to provide detailed contextual
guidance for the evaluation dimensions. The design principles allow our
framework to deliver cardinal assessment of quality, which can facilitate
better post-training compared to ordinal preference data. For better
accessibility, we design two variants of GREP: a more precise variant with
proprietary LLMs as evaluators, and a cheaper alternative with open-weight
LLMs. Empirical investigation reveals that our framework is able to assess the
quality of related work sections in a much more robust manner compared to
standard LLM judges, reflects natural scenarios of scientific writing, and
bears a strong correlation with the human expert assessment. We also observe
that generations from state-of-the-art LLMs struggle to satisfy validation
constraints of a suitable related work section. They (mostly) fail to improve
based on feedback as well.

</details>


### [74] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
*Changhao Song,Yazhou Zhang,Hui Gao,Ben Yao,Peng Zhang*

Main category: cs.CL

TL;DR: 本综述总结了大型语言模型在主观语言理解中的应用，涵盖情感分析、 sarcasm、隐喻、意图等任务，分析了架构、技术、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LLMs)的发展，推动了主观语言理解任务的研究，探索其在处理复杂、模糊和情感丰富内容中的应用潜力。

Method: 系统回顾了相关任务的定义、数据集、最新方法，分析了LLMs的优势，并对比不同任务的共性与差异。

Result: 总结了当前技术的成果与不足，探讨了多任务学习的可能性，提出了数据、偏见和伦理方面的未来研究方向。

Conclusion: 为研究者和实践者提供了重要参考，推动情感计算和隐喻理解等领域的发展，强调未来的研究重点和挑战。

Abstract: Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

</details>


### [75] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
*Matthias Sperber,Maureen de Seyssel,Jiajun Bao,Matthias Paulik*

Main category: cs.CL

TL;DR: 该论文探讨了人类口译行为与机器翻译系统的关系，提出借鉴人类口译原则以提升语音翻译系统的适应性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的语音翻译系统缺乏适应现实场景的灵活性，不能像人类口译一样进行动态调整。

Method: 从人类口译文献出发，结合机器翻译研究，分析两者的操作和质性方面的异同，并探讨应用人类口译原则的潜力。

Result: 发现采用人类口译原则和最新建模技术，有望缩小用户体验的差距，推动真正的机器口译发展。

Conclusion: 借鉴人类口译的原则可以显著改善语音翻译系统的实用性，有助于实现类似人类的机器口译。

Abstract: Current speech translation systems, while having achieved impressive
accuracies, are rather static in their behavior and do not adapt to real-world
situations in ways human interpreters do. In order to improve their practical
usefulness and enable interpreting-like experiences, a precise understanding of
the nature of human interpreting is crucial. To this end, we discuss human
interpreting literature from the perspective of the machine translation field,
while considering both operational and qualitative aspects. We identify
implications for the development of speech translation systems and argue that
there is great potential to adopt many human interpreting principles using
recent modeling techniques. We hope that our findings provide inspiration for
closing the perceived usability gap, and can motivate progress toward true
machine interpreting.

</details>


### [76] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
*David Arps,Hassan Sajjad,Laura Kallmeyer*

Main category: cs.CL

TL;DR: 不同结构诱导语言模型在句法表示、语法判断和训练动态方面表现差异明显，GPST表现较为优越，且小模型结合合成数据是有效的评估工具。


<details>
  <summary>Details</summary>
Motivation: 为了系统评估不同结构诱导语言模型的句法表示能力、语法判断性能及训练动态，填补现有研究中的评估不足与不具可比性。

Method: 比较Structformer、UDGN和GPST三种模型在自然语言和合成括号表达式上的表现，分析它们的句法表示、语法判断能力和训练动态。

Result: GPST在多项指标上表现较为稳定，尤其在长距离依赖任务中优于其他模型。不同模型在句法表示上的差异显著。使用小模型和大量合成数据能有效评估模型特性。

Conclusion: 模型性能具有差异，GPST表现优越，小规模模型结合合成数据是评估模型基本属性的有效策略。

Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised
language modeling task, and induce a hierarchical sentence representation as a
byproduct when processing an input. A wide variety of SiLMs have been proposed.
However, these have typically been evaluated on a relatively small scale, and
evaluation of these models has systematic gaps and lacks comparability. In this
work, we study three different SiLM architectures using both natural language
(English) corpora and synthetic bracketing expressions: Structformer (Shen et
al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare
them with respect to (i) properties of the induced syntactic representations
(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.
We find that none of the three architectures dominates across all evaluation
metrics. However, there are significant differences, in particular with respect
to the induced syntactic representations. The Generative Pretrained Structured
Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation
settings, and outperforms the other models on long-distance dependencies in
bracketing expressions. Furthermore, our study shows that small models trained
on large amounts of synthetic data provide a useful testbed for evaluating
basic model properties.

</details>


### [77] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: 引入一种基于强化学习的大规模开源搜索代理(ASearcher)，通过异步训练和自主生成高质量问答数据，实现了极长的搜索跨度和优异的性能，超越现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 提升开源搜索代理的搜索智能，解决现有方法在扩展性、效率和数据质量上的不足。

Method: 采用全异步强化学习训练、提示驱动的LLM代理自主生成问答数据，构建大规模训练集，并进行长远搜索策略优化。

Result: 所提出的QwQ-32B代理在多个基准上显著优于现有开源模型，支持长达40轮的工具调用和逾150k的输出令牌，表现出色。

Conclusion: 通过创新的异步训练架构和自主问答生成策略，有效提升了开源搜索代理的长时搜索能力和整体性能，推动搜索智能的发展。

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [78] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.CL

TL;DR: 本文提出了Medical Metaphors Corpus（MCC），这是一个涵盖医学和生物学领域的792个科学隐喻的标注数据集，用于推动科学比喻的计算研究，揭示了现有模型在专业领域比喻理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着比喻在科学交流中的重要性日益凸显，现有资源多集中于通用语言，缺乏领域专用的数据资源，阻碍了科学比喻的自动检测和理解。

Method: 收集医学和生物学领域的科学文本，进行人工标注，提供二元和分级的比喻认知评级，创建了包含源-target映射的科学比喻数据集MCC。

Result: 建立了第一个标注的科学比喻数据集，发现现有先进模型在专业领域比喻检测方面表现有限，展示了研究潜力。

Conclusion: MCC为科学比喻的检测、生成以及交流工具提供了基础资源，有助于推动该领域的技术发展与应用实践。

Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

</details>


### [79] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
*Ryan Wong,Jiawei Wang,Junjie Zhao,Li Chen,Yan Gao,Long Zhang,Xuan Zhou,Zuo Wang,Kai Xiang,Ge Zhang,Wenhao Huang,Yang Wang,Ke Wang*

Main category: cs.CL

TL;DR: 评估大规模信息检索中搜索代理的可靠性，发现现有系统表现欠佳，强调未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型的发展，自动搜索代理有望解决繁琐的信息检索任务，但其在大规模信息采集中的可靠性尚未充分评估。

Method: 设计WideSearch基准，通过人工筛选的问题和严格质量控制，评估不同搜索系统的性能。

Result: 多系统表现极差，成功率接近0%，最佳仅达5%；多次验证后可达近100%。

Conclusion: 当前搜索代理在大规模信息检索方面存在严重不足，亟需改进与研究。

Abstract: From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

</details>


### [80] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
*Mingzi Cao,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: OpT-DeUS通过利用最优运输对齐和融合Transformer层，有效提升大规模语言模型深度扩展的性能与训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型性能提升的需求，深度扩展作为一种高效丰富模型能力的方法，但现有方法在层间参数整合方面存在对齐问题，影响模型性能。

Method: 提出利用最优运输进行神经元对齐，通过OT优化Transformer层的融合，缓解层间神经元重排问题，并结合插入位置分析优化训练效率。

Result: 在多种模型规模下，OpT-DeUS在持续预训练和监督微调任务中实现优于当前方法的性能与训练效率，特别是在接近模型顶部插入新层提升显著。

Conclusion: 利用OT进行层间神经元对齐和融合的方法有效改善大规模语言模型的深度扩展，提升了性能和训练效率，并提供了插入位置的优化策略。

Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

</details>


### [81] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
*Fabrizio Nunnari,Cristina Luna Jiménez,Rosalee Wolfe,John C. McDonald,Michael Filhol,Eleni Efthimiou,Evita Fotinea,Thomas Hanke*

Main category: cs.CL

TL;DR: SLTAT 2025聚焦手语翻译与虚拟人技术，涵盖识别、数据、伦理等多个方面，促进人机交互和无障碍交流的发展。


<details>
  <summary>Details</summary>
Motivation: 推动非侵入性手语交流技术，改善聋人和人类沟通方式。

Method: 通过会议分享手语识别、数据分析、工具开发、伦理考量及情感计算等多领域最新研究进展。

Result: 多样化的研究贡献增强了虚拟代理在手语翻译及交互中的应用前景。

Conclusion: 该会议促进了跨学科交流，推动数字人技术在无障碍沟通中的应用深化。

Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

</details>


### [82] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: 提出一种基于异构适配器和弱监督训练的语音语言模型，提升其理解情感和意图的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLMs的对话系统忽略了语音中的情感和意图线索，亟需更好整合语音信息的解决方案。

Method: 引入两个异构适配器，采用弱监督训练策略，分离和整合语音中的语言和非语言信息。

Result: 模型在情感对话任务中表现出色，有效融合语音的多模态信息，增强理解能力。

Conclusion: 该方法提升了语音语言模型对情感和意图的捕捉能力，为多模态对话系统发展提供了新途径。

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [83] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: 本文介绍了一个新的数据集GEDE，用于评估各种文本检测器在学术环境中识别由大型语言模型生成的文本的能力。研究发现，当前检测器在识别中间级别贡献的文本时效果较差，易产生误报，可能影响学生。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，如何有效检测由其生成的学术文本成为一个重要问题。

Method: 构建GEDE数据集，涵盖不同贡献级别的学生和生成文本，评估多种检测器的性能，提出贡献级别概念以反映生成实践的多样性。

Result: 检测器在识别中间级别的文本表现不佳，易误判，为学术诚信带来挑战。

Conclusion: 现有检测工具在复杂场景下仍不可靠，需要进一步改进以确保教育公平与诚信。

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [84] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
*Robin Huo,Ewan Dunbar*

Main category: cs.CL

TL;DR: 研究了HuBERT和wav2vec 2.0两种自监督语音模型的架构差异，发现模型架构对其所学习的语言信息影响有限，主要由训练迭代次数决定。


<details>
  <summary>Details</summary>
Motivation: 理解模型架构对自监督语音表示中语言信息学习的影响，填补相关研究空白。

Method: 比较两个模型的训练目标和多轮迭代伪标签优化，分析隐藏层表示与词、音素、说话人身份的相关性。

Result: 发现隐藏表示与语言相关信息的相关性主要由训练迭代次数影响，而非训练目标。

Conclusion: 建议未来研究深入探讨迭代优化在编码语言信息中的作用。

Abstract: Self-supervised models for speech representation learning now see widespread
use for their versatility and performance on downstream tasks, but the effect
of model architecture on the linguistic information learned in their
representations remains under-studied. This study investigates two such models,
HuBERT and wav2vec 2.0, and minimally compares two of their architectural
differences: training objective and iterative pseudo-label refinement through
multiple training iterations. We find that differences in canonical correlation
of hidden representations to word identity, phoneme identity, and speaker
identity are explained by training iteration, not training objective. We
suggest that future work investigate the reason for the effectiveness of
iterative refinement in encoding linguistic information in self-supervised
speech representations.

</details>


### [85] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
*Jakub Šmíd,Pavel Přibáň,Ondřej Pražák,Pavel Král*

Main category: cs.CL

TL;DR: 提出了一种新型捷克语ABSA数据集，支持复杂任务，具高一致性，适合跨语言研究，配有基线模型。


<details>
  <summary>Details</summary>
Motivation: 推动捷克语ABSA研究，提供支持复杂任务的数据资源。

Method: 构建手工标注的捷克餐厅评论数据集，采用SemEval格式，进行多轮标注以确保一致性。

Result: 数据集涵盖3.1K评论，标注一致性高达90%，提供未标注大规模数据，展示Transformer模型基线表现。

Conclusion: 该数据集促进捷克语ABSA研究，支持跨语言和复杂任务，资源公开促进合作。

Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

</details>


### [86] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: 提出一种基于最优传输的正则化方法（OTReg），改善语音文本对齐，减少模态差距，从而提升语音语言模型的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决语音语言模型在不同数据集和任务中的泛化不足，主要源于语音与文本表征之间的模态差距。

Method: 引入最优传输正则化，通过建立语音与文本嵌入的结构化对应关系，改善模型训练中的语音文本对齐，不依赖额外标签或可学习参数。

Result: 在多语种自动语音识别任务中，OTReg显著增强了语音文本的对齐效果，减小模态差距，提升模型在多样化数据集上的泛化性能。

Conclusion: OTReg是一种轻量高效的正则化方法，有效改善语音-文本之间的模态差异，促进语音语言模型的跨数据集应用。

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [87] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: 本文研究大型语言模型中信息可信度的评估，提出利用不确定性指标识别模型的不可靠回答，提升回答的准确性与可信度。


<details>
  <summary>Details</summary>
Motivation: 理解和改善大型语言模型中不可靠输出的问题，确保其在多轮对话及关键应用中的可信性。

Method: 通过计算输出的aleatoric和epistemic不确定性，结合上下文信息，构建一个责任感估算方法，用以识别模型的不可靠回应。

Result: 在开放式问答任务中，该方法有效提升答案的准确率和模型信心，并能检测出由误导性上下文引发的错误回答。

Conclusion: 不确定性信号存在局限，通过探测模型行为的变化，结合不确定性指标，能更好地实现模型可靠性评估与调控。

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [88] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
*Jun Wang,Zaifu Zhan,Qixin Zhang,Mingquan Lin,Meijia Song,Rui Zhang*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

</details>


### [89] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
*Wentao Jiang,Xiang Feng,Zengmao Wang,Yong Luo,Pingbo Xu,Zhe Chen,Bo Du,Jing Zhang*

Main category: cs.CL

TL;DR: 提出REX-RAG框架，结合混合采样和政策校正，有效改善强化学习与检索增强生成中遇到的死胡同问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在强化学习过程中陷入错误推理路径的问题，提升模型决策的有效性和鲁棒性。

Method: 引入混合采样策略和重要性采样的政策校正机制，探索多样推理路径，修正分布偏差。

Result: 在七个问答基准测试中，REX-RAG相较强基线模型提升了性能，验证了方法的有效性和竞争力。

Conclusion: 通过创新的采样和校正机制，本文提升了强化学习在检索增强生成中的应用效果，具有参考价值。

Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [90] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: 本文提出拓展DisCo模型以更好地捕捉标注者分歧，结合元数据改善输入表现，实现多数据集中的性能提升。


<details>
  <summary>Details</summary>
Motivation: 旨在通过模型标注者的多样性和分歧，提高文本标注任务的效果和鲁棒性。

Method: 在DisCo基础上加入标注者元数据，优化输入表示和损失函数，以捕捉分歧模式；进行大量实验。

Result: 在多数据集上实现硬软标签和视角评估的显著提升，伴随深入的误差和校准分析。

Conclusion: 强调分歧感知模型的重要性，为处理人类注释复杂性提供了新的思路。

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [91] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
*Bangsheng Tang,Carl Chengyan Fu,Fei Kou,Grigory Sizov,Haoci Zhang,Jason Park,Jiawen Liu,Jie You,Qirui Yang,Sachin Mehta,Shengyong Cai,Xiaodong Wang,Xingyu Liu,Yunlu Li,Yanjun Zhou,Wei Wei,Zhiwei Zhao,Zixi Qi,Adolfo Victoria,Aya Ibrahim,Bram Wasti,Changkyu Kim,Daniel Haziza,Fei Sun,Giancarlo Delfin,Emily Guo,Jialin Ouyang,Jaewon Lee,Jianyu Huang,Jeremy Reizenstein,Lu Fang,Quinn Zhu,Ria Verma,Vlad Mihailescu,Xingwen Guo,Yan Cui,Ye Hu,Yejin Lee*

Main category: cs.CL

TL;DR: 本文提出了一种基于EAGLE的推测解码技术优化方法，显著提升了Llama模型的推理速度，并解决了在生产环境中实现的工程挑战。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型的推理速度以满足生产环境的需求。

Method: 通过训练与推理的优化技术，包括高效实现树形注意力和多轮推测解码，优化GPU端性能。

Result: 在8个NVIDIA H100 GPU上，Llama4 Maverick模型实现了每个Token约4毫秒的解码速度，比之前的最佳方法快10%；在大批量处理时，推测解码的速度提升达1.4到2倍。

Conclusion: 该技术显著提升了大模型推理的效率，推动大规模生产部署的发展。

Abstract: Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.

</details>


### [92] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: 研究评估大规模语言模型推理时不确定性指标与人类不确定性的匹配程度，以及其校准效果。


<details>
  <summary>Details</summary>
Motivation: 提升模型控制和用户信任，通过不确定性衡量改善用户体验。

Method: 利用多种指标及新颖变体，评估不确定性指标与人类和模型校准的关系。

Result: 发现部分不确定性指标与人类不确定性高度匹配，同时在模型校准方面表现出中等到强的相关性。

Conclusion: 某些推理时不确定性指标能较好反映人类不确定性，具备潜力用于改善模型控制与用户信任。

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [93] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: SAEMark是一种无需修改模型参数的文本水印技术，利用推理时特征采样实现多比特水印，适应多语言和开放平台，保证文本质量和水印检测准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型的广泛应用，内容归属和防止误导信息亟需有效的水印技术，现有技术存在兼容性差、影响内容质量的问题。

Method: 该方法基于推理时从文本特征中筛选输出，通过特征统计与密钥目标对齐，不修改模型logits，不依赖训练，使用稀疏自编码器实现特征提取。

Result: 在多个数据集上验证，SAEMark达到99.7%的F1检测率，展现出优异的检测效果和保持文本质量的能力，通过理论保证与实证分析实现多语言、多领域的可扩展性。

Conclusion: SAEMark提供了一种无需白盒模型参数调整的通用水印方案，为内容归属和误导信息预防提供了一种高效、兼容性强的新的技术路径。

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [94] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: GPT-5在医疗多模态推理任务中表现优异，超过现有模型和部分人类专家，为未来临床决策支持系统提供强大基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在领域特定推理中的应用扩大，探索其在医疗多模态决策支持中的潜力成为研究焦点。

Method: 系统评估GPT-5及其变体在多项医疗问答和视觉问答任务中的零-shot链式推理能力，使用统一评估协议与标准数据集比较。

Result: GPT-5在所有基准测试中表现优越，超越GPT-4o和部分人类专家，尤其在多模态理解方面取得显著提升，展现出接近或超越人类专家的能力。

Conclusion: GPT-5显著推动医疗多模态推理技术发展，为未来临床决策支持系统设计提供理论和实践基础。

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


### [95] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
*Yunna Cai,Fan Wang,Haowei Wang,Kun Wang,Kailai Yang,Sophia Ananiadou,Moyan Li,Mingming Fan*

Main category: cs.CL

TL;DR: 提出PsyCrisis-Bench，基于真实中文心理健康对话的无参考评价工具，用于检测大模型在高风险心理健康对话中的安全性，具高可解释性和高一致性。


<details>
  <summary>Details</summary>
Motivation: 心理健康高风险对话缺乏金标准评价，存在伦理敏感性，亟需可靠评估方法。

Method: 采用prompt式LLM作为判定者，通过专家定义的推理链进行评估，利用二元评分增强可解释性。

Result: 在3600个判断中表现出最高的一致性和更高的解释性，超越现有方法。

Conclusion: 提供了一个高质量中文数据集和评价工具，促进未来研究和实际应用。

Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

</details>


### [96] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
*Jiahao Zhao,Liwei Dong*

Main category: cs.CL

TL;DR: Jinx是一种无安全限制的语言模型，用于研究和评估模型的安全失效和对齐问题。


<details>
  <summary>Details</summary>
Motivation: 缺乏安全未限制模型，限制了对模型安全边界的评估与研究。

Method: 基于开源大模型，开发出Jinx无安全过滤版本，保持模型推理和指令遵循能力。

Result: Jinx能够响应所有查询，不拒绝任何请求，提供研究安全对齐失败的工具。

Conclusion: Jinx为研究开放式安全模型提供了重要工具，有助于改善模型安全性与对齐策略。

Abstract: Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [97] [BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation](https://arxiv.org/abs/2508.06781)
*Christos Tsirigotis,Vaibhav Adlakha,Joao Monteiro,Aaron Courville,Perouz Taslakian*

Main category: cs.IR

TL;DR: BiXSE利用大语言模型生成的细粒度相关性评分，通过二元交叉熵优化，实现了高效的密集检索嵌入模型训练，优于传统对比学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够生成细粒度相关性标签，为提升密集检索模型提供新的监督信号，但现有方法多依赖繁琐的配对或列表式损失。

Method: 提出BiXSE方法，用二元交叉熵在LLM生成的级别相关性评分上进行点对点训练，利用批内负样本实现高效优化，减少标注和计算成本。

Result: 实验显示，BiXSE在句子嵌入和检索基准（MMTEB、BEIR、TREC-DL）上优于传统对比学习方法，能够匹配或超过强大的配对排名基线，展示了其强大性能和可扩展性。

Conclusion: BiXSE为密集检索模型训练提供了一种鲁棒、可扩展的替代方案，随着细粒度相关性标注的普及，有望广泛应用于实际场景。

Abstract: Neural sentence embedding models for dense retrieval typically rely on binary
relevance labels, treating query-document pairs as either relevant or
irrelevant. However, real-world relevance often exists on a continuum, and
recent advances in large language models (LLMs) have made it feasible to scale
the generation of fine-grained graded relevance labels. In this work, we
propose BiXSE, a simple and effective pointwise training method that optimizes
binary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSE
interprets these scores as probabilistic targets, enabling granular supervision
from a single labeled query-document pair per query. Unlike pairwise or
listwise losses that require multiple annotated comparisons per query, BiXSE
achieves strong performance with reduced annotation and compute costs by
leveraging in-batch negatives. Extensive experiments across sentence embedding
(MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistently
outperforms softmax-based contrastive learning (InfoNCE), and matches or
exceeds strong pairwise ranking baselines when trained on LLM-supervised data.
BiXSE offers a robust, scalable alternative for training dense retrieval models
as graded relevance supervision becomes increasingly accessible.

</details>


### [98] [CLAP: Coreference-Linked Augmentation for Passage Retrieval](https://arxiv.org/abs/2508.06941)
*Huanwei Xu,Lin Xu,Liang Yuan*

Main category: cs.IR

TL;DR: CLAP通过分段、共指解析和生成局部伪查询，有效提升大规模语言模型扩展检索的性能，特别是在跨域和增强密集检索器的场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM扩展检索存在语义漂移和噪声问题，限制其效果。

Method: 将 Passage 分段、共指链解析，生成局部伪查询，融合全局和子话题信号，以增强密集检索器性能。

Result: CLAP在多个领域实现了持续性能提升，特别是在跨域设置中，显著优于传统方法，nDCG@10提升达20.68%。

Conclusion: CLAP提供一种领域无关、鲁棒的Passage扩展方案，有助于大规模信息检索系统的提升。

Abstract: Large Language Model (LLM)-based passage expansion has shown promise for
enhancing first-stage retrieval, but often underperforms with dense retrievers
due to semantic drift and misalignment with their pretrained semantic space.
Beyond this, only a portion of a passage is typically relevant to a query,
while the rest introduces noise--an issue compounded by chunking techniques
that break coreference continuity. We propose Coreference-Linked Augmentation
for Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that
segments passages into coherent chunks, resolves coreference chains, and
generates localized pseudo-queries aligned with dense retriever
representations. A simple fusion of global topical signals and fine-grained
subtopic signals achieves robust performance across domains. CLAP yields
consistent gains even as retriever strength increases, enabling dense
retrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,
with up to 20.68% absolute nDCG@10 improvement. These improvements are
especially notable in out-of-domain settings, where conventional LLM-based
expansion methods relying on domain knowledge often falter. CLAP instead adopts
a logic-centric pipeline that enables robust, domain-agnostic generalization.

</details>


### [99] [Blending Sequential Embeddings, Graphs, and Engineered Features: 4th Place Solution in RecSys Challenge 2025](https://arxiv.org/abs/2508.06970)
*Sergei Makeev,Alexandr Andreev,Vladimir Baikalov,Vladislav Tytskiy,Aleksei Krasilnikov,Kirill Khrylchenko*

Main category: cs.IR

TL;DR: 解决方案结合序列编码器、图神经网络、深度交叉网络和特征工程，旨在生成对多任务有效的用户嵌入。


<details>
  <summary>Details</summary>
Motivation: 挑战在于构建能在六个不同任务中表现优异的通用用户行为模型。

Method: 通过集成序列编码器、图神经网络、深层交叉网络和特征工程，提升用户嵌入的表现。

Result: 该方案在RecSys Challenge 2025中获得第四名，展示了其有效性和竞争力。

Conclusion: 该方法成功实现了跨多任务的用户行为建模，为推荐系统的多任务学习提供了实用方案。

Abstract: This paper describes the 4th-place solution by team ambitious for the RecSys
Challenge 2025, organized by Synerise and ACM RecSys, which focused on
universal behavioral modeling. The challenge objective was to generate user
embeddings effective across six diverse downstream tasks. Our solution
integrates (1) a sequential encoder to capture the temporal evolution of user
interests, (2) a graph neural network to enhance generalization, (3) a deep
cross network to model high-order feature interactions, and (4)
performance-critical feature engineering.

</details>


### [100] [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
*Wenhan Liu,Xinyu Ma,Weiwei Sun,Yutao Zhu,Yuchen Li,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 提出了一种自动化推理训练数据合成框架和两阶段后训练方法，显著提升了模型的推理能力和排序性能，取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决推理密集型训练数据稀缺导致的复杂排序场景性能不足问题。

Method: 利用多源领域数据和DeepSeek-R1生成高质量训练标签，设计自我一致性筛选机制，并通过冻结模型的冷启动监督微调和基于多视角奖励的强化学习两个阶段进行后训练。

Result: 开发出有效的推理增强排序模型ReasonRank，在多个基准上优于现有方法，且具有较低延迟，达到SOTA水平。

Conclusion: 通过自动化数据合成和有效的后训练策略，显著提升推理能力，为复杂排序任务提供了有效解决方案。

Abstract: Large Language Model (LLM) based listwise ranking has shown superior
performance in many passage ranking tasks. With the development of Large
Reasoning Models, many studies have demonstrated that step-by-step reasoning
during test-time helps improve listwise ranking performance. However, due to
the scarcity of reasoning-intensive training data, existing rerankers perform
poorly in many complex ranking scenarios and the ranking ability of
reasoning-intensive rerankers remains largely underdeveloped. In this paper, we
first propose an automated reasoning-intensive training data synthesis
framework, which sources training queries and passages from diverse domains and
applies DeepSeek-R1 to generate high-quality training labels. A
self-consistency data filtering mechanism is designed to ensure the data
quality. To empower the listwise reranker with strong reasoning ability, we
further propose a two-stage post-training approach, which includes a cold-start
supervised fine-tuning (SFT) stage for reasoning pattern learning and a
reinforcement learning (RL) stage for further ranking ability enhancement.
During the RL stage, based on the nature of listwise ranking, we design a
multi-view ranking reward, which is more effective than a ranking metric-based
reward. Extensive experiments demonstrate that our trained reasoning-intensive
reranker \textbf{ReasonRank} outperforms existing baselines significantly and
also achieves much lower latency than pointwise reranker Rank1. \textbf{Through
further experiments, our ReasonRank has achieved state-of-the-art (SOTA)
performance 40.6 on the BRIGHT
leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are
available at https://github.com/8421BCD/ReasonRank.

</details>


### [101] [Uncertainty-Aware Semantic Decoding for LLM-Based Sequential Recommendation](https://arxiv.org/abs/2508.07210)
*Chenke Yin,Li Fan,Jia Wang,Dongxiao Hu,Haichao Zhang,Chong Zhang,Yang Xiang*

Main category: cs.IR

TL;DR: 提出一种不确定性感知的语义解码框架USD，结合聚类和自适应评分改善推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在推荐任务中解码策略与目标不匹配的问题。

Method: 通过基于对数似然的聚类和概率重分布，控制推荐中的项评分和采样温度。

Result: 在多个电商和影视数据集上显著优于基线模型，提升推荐性能。

Conclusion: 融合语义聚类与不确定性评估能提升推荐系统的可靠性和准确性。

Abstract: Large language models have been widely applied to sequential recommendation
tasks, yet during inference, they continue to rely on decoding strategies
developed for natural language processing. This creates a mismatch between
text-generation objectives and recommendation next item selection objectives.
This paper addresses this limitation by proposing an Uncertainty-aware Semantic
Decoding (USD) framework that combines logit-based clustering with adaptive
scoring to improve next-item predictions. Our approach clusters items with
similar logit vectors into semantic equivalence groups, then redistributes
probability mass within these clusters and computes entropy across them to
control item scoring and sampling temperature during recommendation inference.
Experiments on Amazon Product datasets (six domains) gains of 18.5\% in HR@3,
11.9\% in NDCG@3, and 10.8\% in MRR@3 compared to state-of-the-art baselines.
Hyperparameter analysis confirms the optimal parameters among various settings,
and experiments on H\&M, and Netflix datasets indicate that the framework can
adapt to differing recommendation domains. The experimental results confirm
that integrating semantic clustering and uncertainty assessment yields more
reliable and accurate recommendations.

</details>


### [102] [Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation](https://arxiv.org/abs/2508.07223)
*Guanchen Wang,Mingming Ha,Tianbao Ma,Linxun Chen,Zhaojie Liu,Guorui Zhou,Kun Gai*

Main category: cs.IR

TL;DR: 提出KSER框架，通过知识筛选和空间对齐提升基于大语言模型的推荐性能，解决知识冗余和信息一致性问题。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型的强大推理和知识能力提升推荐系统性能，但面临知识冗余、虚假和信息同质化问题。

Method: 设计知识筛选模块（ESFNet）和空间对齐模块，采用全参数训练和提取器训练策略。

Result: 验证了两个模块的必要性和有效性，展示了提取器训练策略的优势。

Conclusion: 通过高质量知识筛选与空间对齐，有效改善LLMs在推荐中的应用效果，提供了知识增强推荐的新思路。

Abstract: In recent years, there has been growing interest in leveraging the impressive
generalization capabilities and reasoning ability of large language models
(LLMs) to improve the performance of recommenders. With this operation,
recommenders can access and learn the additional world knowledge and reasoning
information via LLMs. However, in general, for different users and items, the
world knowledge derived from LLMs suffers from issues of hallucination, content
redundant, and information homogenization. Directly feeding the generated
response embeddings into the recommendation model can lead to unavoidable
performance deterioration. To address these challenges, we propose a Knowledge
Selection \& Exploitation Recommendation (KSER) framework, which effectively
select and extracts the high-quality knowledge from LLMs. The framework
consists of two key components: a knowledge filtering module and a embedding
spaces alignment module. In the knowledge filtering module, a Embedding
Selection Filter Network (ESFNet) is designed to assign adaptive weights to
different knowledge chunks in different knowledge fields. In the space
alignment module, an attention-based architecture is proposed to align the
semantic embeddings from LLMs with the feature space used to train the
recommendation models. In addition, two training
strategies--\textbf{all-parameters training} and \textbf{extractor-only
training}--are proposed to flexibly adapt to different downstream tasks and
application scenarios, where the extractor-only training strategy offers a
novel perspective on knowledge-augmented recommendation. Experimental results
validate the necessity and effectiveness of both the knowledge filtering and
alignment modules, and further demonstrate the efficiency and effectiveness of
the extractor-only training strategy.

</details>


### [103] [SocRipple: A Two-Stage Framework for Cold-Start Video Recommendations](https://arxiv.org/abs/2508.07241)
*Amit Jaspal,Kapil Dalwani,Ajantha Ramineni*

Main category: cs.IR

TL;DR: SocRipple通过两阶段框架提高冷启动新物品的推荐效果，有效平衡新物品曝光和个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 解决行业推荐系统中新物品缺乏用户互动历史导致的冷启动问题。

Method: 结合社交关联性和用户早期行为信号，通过两阶段检索策略实现新物品推广。

Result: 在大型视频平台实验中，SocRipple提升了36%的冷启动物品分发效果，同时维持用户参与度。

Conclusion: SocRipple有效利用社交网络和行为信号，增强冷启动新物品的推荐效果，兼顾曝光与个性化。

Abstract: Most industry scale recommender systems face critical cold start challenges
new items lack interaction history, making it difficult to distribute them in a
personalized manner. Standard collaborative filtering models underperform due
to sparse engagement signals, while content only approaches lack user specific
relevance. We propose SocRipple, a novel two stage retrieval framework tailored
for coldstart item distribution in social graph based platforms. Stage 1
leverages the creators social connections for targeted initial exposure. Stage
2 builds on early engagement signals and stable user embeddings learned from
historical interactions to "ripple" outwards via K Nearest Neighbor (KNN)
search. Large scale experiments on a major video platform show that SocRipple
boosts cold start item distribution by +36% while maintaining user engagement
rate on cold start items, effectively balancing new item exposure with
personalized recommendations.

</details>


### [104] [PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization](https://arxiv.org/abs/2508.07342)
*Kepu Zhang,Teng Shi,Weijie Yu,Jun Xu*

Main category: cs.IR

TL;DR: 提出PrLM强化学习框架，通过显式推理用户档案提升个性化生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型对检索质量敏感、生成偏离用户偏好的问题。

Method: 使用对比训练的个性化奖励模型指导强化学习，提高模型对检索信息的推理能力。

Result: 在三个数据集上优于现有方法，表现稳健。

Conclusion: 通过显式推理和强化学习提升个性化生成的准确性与鲁棒性。

Abstract: Personalized retrieval-augmented generation (RAG) aims to produce
user-tailored responses by incorporating retrieved user profiles alongside the
input query. Existing methods primarily focus on improving retrieval and rely
on large language models (LLMs) to implicitly integrate the retrieved context
with the query. However, such models are often sensitive to retrieval quality
and may generate responses that are misaligned with user preferences. To
address this limitation, we propose PrLM, a reinforcement learning framework
that trains LLMs to explicitly reason over retrieved user profiles. Guided by a
contrastively trained personalization reward model, PrLM effectively learns
from user responses without requiring annotated reasoning paths. Experiments on
three personalized text generation datasets show that PrLM outperforms existing
methods and remains robust across varying numbers of retrieved profiles and
different retrievers.

</details>


### [105] [Are Multimodal Embeddings Truly Beneficial for Recommendation? A Deep Dive into Whole vs. Individual Modalities](https://arxiv.org/abs/2508.07399)
*Yu Ye,Junchen Fu,Yu Song,Kaiwen Zheng,Joemon M. Jose*

Main category: cs.IR

TL;DR: 本文对多模态推荐中的文本与视觉嵌入的作用进行了大规模实证研究，发现融合复杂模型能提升推荐效果，但单一文本模态表现媲美全模态，视觉模态效果有限。


<details>
  <summary>Details</summary>
Motivation: 验证多模态嵌入是否真正提升推荐性能，填补实证验证的空白。

Method: 系统评估14个先进的MMRec模型，采用模态敲除策略分析文本和视觉模态的单独和联合效果。

Result: 多模态嵌入一般提升推荐性能，复杂融合模型表现较好；文本单一模态效果与全模态相当，视觉单一模态效果较差。

Conclusion: 多模态嵌入在推荐中有效，特别是通过复杂融合实现，单一文本模态大部分情况下已足够，视觉模态效果有限。

Abstract: Multimodal recommendation (MMRec) has emerged as a mainstream paradigm,
typically leveraging text and visual embeddings extracted from pre-trained
models such as Sentence-BERT, Vision Transformers, and ResNet. This approach is
founded on the intuitive assumption that incorporating multimodal embeddings
can enhance recommendation performance. However, despite its popularity, this
assumption lacks comprehensive empirical verification. This presents a critical
research gap. To address it, we pose the central research question of this
paper: Are multimodal embeddings truly beneficial for recommendation? To answer
this question, we conduct a large-scale empirical study examining the role of
text and visual embeddings in modern MMRec models, both as a whole and
individually. Specifically, we pose two key research questions: (1) Do
multimodal embeddings as a whole improve recommendation performance? (2) Is
each individual modality - text and image - useful when used alone? To isolate
the effect of individual modalities - text or visual - we employ a modality
knockout strategy by setting the corresponding embeddings to either constant
values or random noise. To ensure the scale and comprehensiveness of our study,
we evaluate 14 widely used state-of-the-art MMRec models. Our findings reveal
that: (1) multimodal embeddings generally enhance recommendation performance -
particularly when integrated through more sophisticated graph-based fusion
models. Surprisingly, commonly adopted baseline models with simple fusion
schemes, such as VBPR and BM3, show only limited gains. (2) The text modality
alone achieves performance comparable to the full multimodal setting in most
cases, whereas the image modality alone does not. These results offer
foundational insights and practical guidance for the MMRec community. We will
release our code and datasets to facilitate future research.

</details>


### [106] [Orthogonal Low Rank Embedding Stabilization](https://arxiv.org/abs/2508.07574)
*Kevin Zielnicki,Ko-Jen Hsiao*

Main category: cs.IR

TL;DR: 提出一种正交低秩变换方法，以稳定推荐系统中的用户/物品嵌入空间，确保在模型重训练时嵌入维度一致。


<details>
  <summary>Details</summary>
Motivation: 解决模型重训练后嵌入空间不稳定，影响下游应用表现的问题。

Method: 结合低秩奇异值分解和正交Procrustes变换，将嵌入映射到标准空间。

Result: 此方法计算高效、无损、轻量，保持嵌入的点积和推理质量，便于与其他技术共同使用。

Conclusion: 该方法在保证模型性能的同时，提供了嵌入空间的稳定解决方案，提升推荐系统的应用一致性。

Abstract: The instability of embedding spaces across model retraining cycles presents
significant challenges to downstream applications using user or item embeddings
derived from recommendation systems as input features. This paper introduces a
novel orthogonal low-rank transformation methodology designed to stabilize the
user/item embedding space, ensuring consistent embedding dimensions across
retraining sessions. Our approach leverages a combination of efficient low-rank
singular value decomposition and orthogonal Procrustes transformation to map
embeddings into a standardized space. This transformation is computationally
efficient, lossless, and lightweight, preserving the dot product and inference
quality while reducing operational burdens. Unlike existing methods that modify
training objectives or embedding structures, our approach maintains the
integrity of the primary model application and can be seamlessly integrated
with other stabilization techniques.

</details>


### [107] [Towards Comprehensible Recommendation with Large Language Model Fine-tuning](https://arxiv.org/abs/2508.07595)
*Yunze Luo,Yinjie Jiang,Gaode Chen,Xinghua Zhang,Jun Zhang,Jian Liang,Kaigui Bian*

Main category: cs.IR

TL;DR: 提出了一种结合大语言模型（LLM）与推荐系统的内容理解方法（CURec），通过预训练和强化学习优化推荐理由，提高推荐的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统难以捕捉用户偏好的深层语义，LLM虽具备潜力但面临对齐和生成可靠推荐理由的挑战。

Method: CURec通过预训练对齐、奖励模型评估与RL微调，提升LLM生成的推荐理由质量，并将其整合入推荐模型。

Result: 在多个公开基准测试中，CURec优于现有方法，展示了其在推荐效果和理由可解释性上的优势。

Conclusion: CURec有效弥合了语义和协同过滤的差距，增强了推荐系统的理解能力和性能，具有推广潜力。

Abstract: Recommender systems have become increasingly ubiquitous in daily life. While
traditional recommendation approaches primarily rely on ID-based
representations or item-side content features, they often fall short in
capturing the underlying semantics aligned with user preferences (e.g.,
recommendation reasons for items), leading to a semantic-collaborative gap.
Recently emerged LLM-based feature extraction approaches also face a key
challenge: how to ensure that LLMs possess recommendation-aligned reasoning
capabilities and can generate accurate, personalized reasons to mitigate the
semantic-collaborative gap. To address these issues, we propose a novel Content
Understanding from a Collaborative Perspective framework (CURec), which
generates collaborative-aligned content features for more comprehensive
recommendations. \method first aligns the LLM with recommendation objectives
through pretraining, equipping it with instruction-following and
chain-of-thought reasoning capabilities. Next, we design a reward model
inspired by traditional recommendation architectures to evaluate the quality of
the recommendation reasons generated by the LLM. Finally, using the reward
signals, CURec fine-tunes the LLM through RL and corrects the generated reasons
to ensure their accuracy. The corrected reasons are then integrated into a
downstream recommender model to enhance comprehensibility and recommendation
performance. Extensive experiments on public benchmarks demonstrate the
superiority of CURec over existing methods.

</details>


### [108] [UMRE: A Unified Monotonic Transformation for Ranking Ensemble in Recommender Systems](https://arxiv.org/abs/2508.07613)
*Zhengrui Xu,Zhe Yang,Zhengxiao Guo,Shukai Liu,Luocheng Lin,Xiaoyan Liu,Yongqi Liu,Han Li*

Main category: cs.IR

TL;DR: UMRE通过学习表达能力强的单调神经网络替代传统的手工变换，实现多目标排序的自动化和个性化，提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统的集成排序方法依赖手工设计的非线性变换和调优，既繁琐又效果有限。

Method: 提出UMRE框架，使用UMNN学习单调函数，通过轻量级排名模型融合预测得分，并引入帕累托最优策略进行任务权重的自适应调节。

Result: 在两个公开数据集和在线A/B测试中，表现优异，具有良好的扩展性和个性化能力。

Conclusion: UMRE有效解决了传统方法的不足，实现了无需人工调节的多目标排序，增强了推荐效果。

Abstract: Industrial recommender systems commonly rely on ensemble sorting (ES) to
combine predictions from multiple behavioral objectives. Traditionally, this
process depends on manually designed nonlinear transformations (e.g.,
polynomial or exponential functions) and hand-tuned fusion weights to balance
competing goals -- an approach that is labor-intensive and frequently
suboptimal in achieving Pareto efficiency. In this paper, we propose a novel
Unified Monotonic Ranking Ensemble (UMRE) framework to address the limitations
of traditional methods in ensemble sorting. UMRE replaces handcrafted
transformations with Unconstrained Monotonic Neural Networks (UMNN), which
learn expressive, strictly monotonic functions through the integration of
positive neural integrals. Subsequently, a lightweight ranking model is
employed to fuse the prediction scores, assigning personalized weights to each
prediction objective. To balance competing goals, we further introduce a Pareto
optimality strategy that adaptively coordinates task weights during training.
UMRE eliminates manual tuning, maintains ranking consistency, and achieves
fine-grained personalization. Experimental results on two public recommendation
datasets (Kuairand and Tenrec) and online A/B tests demonstrate impressive
performance and generalization capabilities.

</details>


### [109] [Encode Me If You Can: Learning Universal User Representations via Event Sequence Autoencoding](https://arxiv.org/abs/2508.07748)
*Anton Klenitskiy,Artem Fatkulin,Daria Denisova,Anton Pembek,Alexey Vasilev*

Main category: cs.IR

TL;DR: 提出一种基于GRU自编码器的用户行为表示方法，通过将用户交互历史编码为潜在向量，提升了跨任务的适用性，在RecSys Challenge 2025中获奖。


<details>
  <summary>Details</summary>
Motivation: 构建通用的用户表示，减少任务特定特征工程和模型重训练，提升机器学习系统的效率和可扩展性。

Method: 将用户行为历史转化为时间序列，使用GRU自编码器进行序列重建，结合多种用户表示方法通过拼接形成统一向量。

Result: 该方法在RecSys Challenge 2025中表现优异，获得第二名，展示了其在多任务跨场景中的有效性。

Conclusion: 基于序列重建的潜在表示能有效捕捉用户行为模式，结合多样化模型增强泛化能力，验证了其在实际推荐系统中的应用潜力。

Abstract: Building universal user representations that capture the essential aspects of
user behavior is a crucial task for modern machine learning systems. In
real-world applications, a user's historical interactions often serve as the
foundation for solving a wide range of predictive tasks, such as churn
prediction, recommendations, or lifetime value estimation. Using a
task-independent user representation that is effective across all such tasks
can reduce the need for task-specific feature engineering and model retraining,
leading to more scalable and efficient machine learning pipelines. The goal of
the RecSys Challenge 2025 by Synerise was to develop such Universal Behavioral
Profiles from logs of past user behavior, which included various types of
events such as product purchases, page views, and search queries. We propose a
method that transforms the entire user interaction history into a single
chronological sequence and trains a GRU-based autoencoder to reconstruct this
sequence from a fixed-size vector. If the model can accurately reconstruct the
sequence, the latent vector is expected to capture the key behavioral patterns.
In addition to this core model, we explored several alternative methods for
generating user embeddings and combined them by concatenating their output
vectors into a unified representation. This ensemble strategy further improved
generalization across diverse downstream tasks and helped our team,
ai_lab_recsys, achieve second place in the RecSys Challenge 2025.

</details>


### [110] [Recommendation Is a Dish Better Served Warm](https://arxiv.org/abs/2508.07856)
*Danil Gusak,Nikita Sukhorukov,Evgeny Frolov*

Main category: cs.IR

TL;DR: 论文研究了推荐系统中冷启动边界设定不一致带来的影响，强调合理确定阈值的重要性。


<details>
  <summary>Details</summary>
Motivation: 不同研究中冷启动阈值选择随意，影响评估的可靠性和比较性。

Method: 通过渐进式调整用户和物品的交互阈值进行实验，分析其在多个数据集和推荐模型中的影响。

Result: 阈值不一致导致宝贵数据被误删或冷实例被错误分类，增加系统噪声。

Conclusion: 需合理设定冷启动阈值，提升推荐系统的评价一致性和效果。

Abstract: In modern recommender systems, experimental settings typically include
filtering out cold users and items based on a minimum interaction threshold.
However, these thresholds are often chosen arbitrarily and vary widely across
studies, leading to inconsistencies that can significantly affect the
comparability and reliability of evaluation results. In this paper, we
systematically explore the cold-start boundary by examining the criteria used
to determine whether a user or an item should be considered cold. Our
experiments incrementally vary the number of interactions for different items
during training, and gradually update the length of user interaction histories
during inference. We investigate the thresholds across several widely used
datasets, commonly represented in recent papers from top-tier conferences, and
on multiple established recommender baselines. Our findings show that
inconsistent selection of cold-start thresholds can either result in the
unnecessary removal of valuable data or lead to the misclassification of cold
instances as warm, introducing more noise into the system.

</details>


### [111] [Careful Queries, Credible Results: Teaching RAG Models Advanced Web Search Tools with Reinforcement Learning](https://arxiv.org/abs/2508.07956)
*Yuqin Dai,Shuo Yang,Guoqing Wang,Yong Deng,Zhanwei Zhang,Jun Yin,Pengyu Zeng,Zhenzhe Ying,Changhua Meng,Can Yi,Yuchen Zhou,Weiqiang Wang,Shuai Lu*

Main category: cs.IR

TL;DR: WebFilter通过生成受限查询和过滤不可靠内容，有效提升RAG系统在真实环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 面对网页环境中的虚假信息和工具未被充分利用的问题，提升RAG系统的检索质量和准确性。

Method: 引入源限制查询和过滤机制，结合行为和结果驱动的奖励策略，优化查询生成和检索。

Result: 显著改善回答质量和检索精度，在多类基准测试中优于现有方法。

Conclusion: WebFilter有效应对网页环境中的挑战，增强RAG系统的实用性和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating up-to-date external knowledge, yet real-world web environments
present unique challenges. These limitations manifest as two key challenges:
pervasive misinformation in the web environment, which introduces unreliable or
misleading content that can degrade retrieval accuracy, and the
underutilization of web tools, which, if effectively employed, could enhance
query precision and help mitigate this noise, ultimately improving the
retrieval results in RAG systems. To address these issues, we propose
WebFilter, a novel RAG framework that generates source-restricted queries and
filters out unreliable content. This approach combines a retrieval filtering
mechanism with a behavior- and outcome-driven reward strategy, optimizing both
query formulation and retrieval outcomes. Extensive experiments demonstrate
that WebFilter improves answer quality and retrieval precision, outperforming
existing RAG methods on both in-domain and out-of-domain benchmarks.

</details>


### [112] [Improving Document Retrieval Coherence for Semantically Equivalent Queries](https://arxiv.org/abs/2508.07975)
*Stefano Campese,Alessandro Moschitti,Ivano Lauriola*

Main category: cs.IR

TL;DR: 提出一种改进Dense Retrieval模型的多负样本排序损失，有效提升模型对语义相似查询的检索一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有Dense Retrieval模型对查询词和文档词敏感，可能导致检索结果不一致。

Method: 在多负样本排序损失基础上改进，惩罚语义相似查询中检索结果的差异。

Result: 模型在多个公开数据集上表现出较低敏感性和更高准确性。

Conclusion: 提出的方法增强了Dense Retrieval模型对语义相似查询的一致性和鲁棒性，效果显著。

Abstract: Dense Retrieval (DR) models have proven to be effective for Document
Retrieval and Information Grounding tasks. Usually, these models are trained
and optimized for improving the relevance of top-ranked documents for a given
query. Previous work has shown that popular DR models are sensitive to the
query and document lexicon: small variations of it may lead to a significant
difference in the set of retrieved documents. In this paper, we propose a
variation of the Multi-Negative Ranking loss for training DR that improves the
coherence of models in retrieving the same documents with respect to
semantically similar queries. The loss penalizes discrepancies between the
top-k ranked documents retrieved for diverse but semantic equivalent queries.
We conducted extensive experiments on various datasets, MS-MARCO, Natural
Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes
by our loss are subject to lower sensitivity, and, (ii) interestingly, higher
accuracy.

</details>


### [113] [DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval](https://arxiv.org/abs/2508.07995)
*Meixiu Long,Duolin Sun,Dan Yang,Junjie Wang,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu,Jiahai Wang*

Main category: cs.IR

TL;DR: 提出DIVER检索管道，通过多步骤推理和强化训练提升复杂推理任务的检索效果，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有检索系统在涉及抽象推理、多步推理或类比思维等复杂推理任务中的不足。

Method: 结合文档预处理、LLM引导的查询扩展、基于合成多域数据的推理增强检索模型以及评分融合的重排序器。

Result: 在BRIGHT基准测试中，DIVER取得了最先进的nDCG@10分数，优于多种推理感知模型。

Conclusion: 引入推理感知的检索策略显著提升复杂推理任务中的信息检索表现。

Abstract: Retrieval-augmented generation has achieved strong performance on
knowledge-intensive tasks where query-document relevance can be identified
through direct lexical or semantic matches. However, many real-world queries
involve abstract reasoning, analogical thinking, or multi-step inference, which
existing retrievers often struggle to capture. To address this challenge, we
present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive
information retrieval. DIVER consists of four components: document processing
to improve input quality, LLM-driven query expansion via iterative document
interaction, a reasoning-enhanced retriever fine-tuned on synthetic
multi-domain data with hard negatives, and a pointwise reranker that combines
LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,
DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original
queries, consistently outperforming competitive reasoning-aware models. These
results demonstrate the effectiveness of reasoning-aware retrieval strategies
in complex real-world tasks. Our code and retrieval model will be released
soon.

</details>


### [114] [Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation](https://arxiv.org/abs/2508.08042)
*Van-Khang Nguyen,Duc-Hoang Pham,Huy-Son Nguyen,Cam-Van Thi Nguyen,Hoang-Quynh Le,Duc-Trong Le*

Main category: cs.IR

TL;DR: MAMEX提出了一种基于专家混合的多模态冷启动推荐框架，有效提升了新项推荐的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决冷启动场景中多模态数据融合不足的问题，使新项目得到更有效的推荐。

Method: 引入模态专属专家网络和可学习门控机制，动态调节不同模态的重要性，从而提升推荐性能。

Result: 在多个基准数据集上，MAMEX优于现有最先进的方法，展现出更高的准确性和适应性。

Conclusion: 通过动态融合多模态信息，MAMEX提升了冷启动推荐的效果，具有潜在的实践应用价值。

Abstract: Recommendation systems have faced significant challenges in cold-start
scenarios, where new items with a limited history of interaction need to be
effectively recommended to users. Though multimodal data (e.g., images, text,
audio, etc.) offer rich information to address this issue, existing approaches
often employ simplistic integration methods such as concatenation, average
pooling, or fixed weighting schemes, which fail to capture the complex
relationships between modalities. Our study proposes a novel Mixture of Experts
(MoE) framework for multimodal cold-start recommendation, named MAMEX, which
dynamically leverages latent representation from different modalities. MAMEX
utilizes modality-specific expert networks and introduces a learnable gating
mechanism that adaptively weights the contribution of each modality based on
its content characteristics. This approach enables MAMEX to emphasize the most
informative modalities for each item while maintaining robustness when certain
modalities are less relevant or missing. Extensive experiments on benchmark
datasets show that MAMEX outperforms state-of-the-art methods in cold-start
scenarios, with superior accuracy and adaptability. For reproducibility, the
code has been made available on Github https://github.com/L2R-UET/MAMEX.

</details>


### [115] [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches](https://arxiv.org/abs/2508.08088)
*Jiejun Tan,Zhicheng Dou,Yan Yu,Jiehan Cheng,Qiang Ju,Jian Xie,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 提出HierSearch框架，通过分层强化学习训练多来源深度搜索代理，提高多源信息检索效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度搜索技术单一知识源限制，提升多源复杂工具的掌控能力。

Method: 设计分层强化学习架构，低层为本地与网页搜索代理，高层为协调者，加入知识精炼器以避免错误传播。

Result: 在多个领域的六个基准测试中优于传统平面强化学习和其他多源检索生成方法。

Conclusion: 分层强化学习架构显著提升多源深度搜索的性能和实用性，适合企业私有化需求。

Abstract: Recently, large reasoning models have demonstrated strong mathematical and
coding abilities, and deep search leverages their reasoning capabilities in
challenging information retrieval tasks. Existing deep search works are
generally limited to a single knowledge source, either local or the Web.
However, enterprises often require private deep search systems that can
leverage search tools over both local and the Web corpus. Simply training an
agent equipped with multiple search tools using flat reinforcement learning
(RL) is a straightforward idea, but it has problems such as low training data
efficiency and poor mastery of complex tools. To address the above issue, we
propose a hierarchical agentic deep search framework, HierSearch, trained with
hierarchical RL. At the low level, a local deep search agent and a Web deep
search agent are trained to retrieve evidence from their corresponding domains.
At the high level, a planner agent coordinates low-level agents and provides
the final answer. Moreover, to prevent direct answer copying and error
propagation, we design a knowledge refiner that filters out hallucinations and
irrelevant evidence returned by low-level agents. Experiments show that
HierSearch achieves better performance compared to flat RL, and outperforms
various deep search and multi-source retrieval-augmented generation baselines
in six benchmarks across general, finance, and medical domains.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [116] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 提出生存是一种几何结构的观点，基于生物状态空间中的弧长和流动，建立了自组织生存流形（SOSM）理论。


<details>
  <summary>Details</summary>
Motivation: 传统生存模型依赖于标注的标签和固定的协变量，缺乏对生物状态空间内在几何的理解。

Method: 开发生存能量泛函，基于弧度最小化引导生存相关的动力学结构，导出连续和离散形式，结合几何、热力学和最优传输理论。

Result: 证明了在合理条件下生存轨迹的出现和收敛，揭示健康、疾病、衰老和死亡作为几何相变的过程。

Conclusion: 提出一种无标签的普适生存模型，将生命的形式与生物物理和几何紧密结合，具有理论创新和潜在应用价值。

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [117] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: 提出了一种两阶段学习框架用于供应链欺诈检测，结合无监督的孤立森林和半监督的支持向量机，有效应对标签稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 供应链欺诈检测面临复杂网络结构和标签缺乏的挑战，亟需有效的检测方法。

Method: 首先利用孤立森林进行无监督异常检测，筛选潜在欺诈样本；然后用自训练的支持向量机对模型进行半监督优化。

Result: 在真实供应链数据集上实现了F1-score 0.817，假阳性率低于3%，验证了方法的有效性。

Conclusion: 结合无监督预筛选与半监督微调的方法，能在现实条件下有效提升供应链欺诈检测性能，但仍存在概念漂移及深度学习比较的局限。

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [118] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: 本论文提出结合GFlowNet与VGAE的方法，针对药物交互中少见但重要的类别，通过生成合成样本改善模型平衡性，提高预测性能，增强临床应用的可靠性。


<details>
  <summary>Details</summary>
Motivation: 药物交互数据中少见类别样本不足，导致预测模型性能差，亟需改善模型对稀有类别的识别能力。

Method: 结合生成流网络（GFlowNet）与变分图自编码器（VGAE）生成稀有类别的合成样本，以提升模型多样性和平衡性。

Result: 该方法显著改善了不同类别的预测性能，提升模型在临床中的可靠性和实用性。

Conclusion: 通过引入生成模型，有效缓解类别不平衡问题，为药物交互预测提供了更稳健的解决方案。

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [119] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种结合角色感知和高阶关系的超图神经网络HGMN，有效提升节点分类表现。


<details>
  <summary>Details</summary>
Motivation: 现有GNN主要关注邻接关系，忽略角色特征，影响表示能力。

Method: 引入超图结构和状态空间模型，通过可学习的transformer融合角色与邻接信息，采用多样的超图构建及残差机制增强模型性能。

Result: 在新旧数据集上显著优于现有方法，证明了其增强节点表征和分类性能的能力。

Conclusion: HGMN通过结合角色信息和高阶关系，增强了GNN的表达能力，为图结构学习提供了强大工具。

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [120] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的图向量量化框架RGVQ，有效缓解代码簿崩溃问题，提升图表示的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决在图数据中应用向量量化时普遍存在的代码簿崩溃问题，增强图 token 的多样性和表达能力。

Method: 引入软分配机制、图结构对比正则化，结合图拓扑和特征相似性作为正则信号，提升代码簿利用率。

Result: 在多个下游任务中显著提升了模型性能，增强了图 token 表示的表达力和迁移能力。

Conclusion: RGVQ 有效缓解了图向量量化中的崩溃问题，提升了图表示的多样性和泛化能力，推动了图学习技术的发展。

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [121] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: 提出一种适用于神经影像诊断的联邦学习框架，通过动态导航和元整合提高诊断准确性和鲁棒性，解决样本少和亚型异质性问题，为个性化诊疗提供支持。


<details>
  <summary>Details</summary>
Motivation: 神经影像诊断中，样本少和异质性导致的可重复性低及准确率不足成为主要挑战。

Method: 设计包含动态导航模块和元整合模块的联邦学习框架，利用潜在亚型信息优化样本路由和模型融合。

Result: 在多中心fMRI数据上实现了平均74.06%的诊断准确率，显著优于传统方法，验证了模型在异质性处理和泛化能力上的优势。

Conclusion: 该框架有效应对数据异质性和亚型混杂问题，推动神经影像CAD系统的可靠性和可复制性，并具有临床应用潜力。

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [122] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: 利用AI整合多学科文献，推动材料科学中的新型生物启发材料开发。


<details>
  <summary>Details</summary>
Motivation: 弥补LLMs在多学科材料科学应用的不足，推动创新。

Method: 结合多种AI工具，从多源文献中提取结构-性能关系，生成并验证假设。

Result: 实现了植物源粘合剂等新材料的设计制造，验证了AI在实际科研中的应用潜力。

Conclusion: AI增强的创意方法可有效促进新材料开发，推动人机合作。

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [123] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: 通过数据过滤预训练来增强开源AI模型的抗篡改能力，有效减少有害知识携带，同时指出单一措施不足，强调多层防护的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决开源AI模型面临的篡改攻击风险，提升安全性。

Method: 引入多阶段数据过滤管道，预训练多款参数为6.9B的模型，验证其抗对抗微调能力，并分析其在不同攻击情境下的效果。

Result: 过滤模型在对抗微调攻击中表现优异，超过现有方法十倍以上，无影响无关能力，但仍能通过情境引导利用危险知识，强调多层防护必要性。

Conclusion: 预训练数据筛选是提升开源AI系统安全的有效策略，应结合多层防护措施以确保系统的安全。

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [124] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: 引入数据分布相的概念，通过理解不同阶段的结构性质，提出分阶段的扩散模型设计策略，提高生成效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统扩散模型忽略局部空间结构、计算成本高的问题，探索高效生成的可能路径。

Method: 定义数据分布相，分析逆扩散过程中的相变，利用信息理论界定局部去噪的可行性，结合数值实验验证。

Result: 在相变点附近需要全球性神经网络，其他阶段可用局部网络，从而实现更高效的生成模型设计。

Conclusion: 通过引入数据分布相的概念，为扩散模型的架构优化提供新思路，结合物理与信息理论，开拓生成模型研究的新方向。

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [125] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出一种适用于密集和稀疏大模型的通用缩放定律，以优化模型训练资源分配。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模快速扩大，训练成本激增，亟需优化训练资源配置的方法。

Method: 重新分析现有缩放定律并提出一个通用模型，涵盖密集和稀疏模型。

Result: 展示了该通用缩放定律的有效性，并与现有定律进行比较。

Conclusion: 该研究提供了统一的框架，有助于更有效地预测和分配大模型的训练资源。

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [126] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: ANTIDOTE是一种适应性强、成本低的目标函数，用于应对训练中的噪声标签，通过对抗训练有效减少噪声影响。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中存在噪声标签的问题，提高模型鲁棒性。

Method: 利用凸对偶理论，将目标定义为信息散度邻域的放松形式，转化为对抗训练。

Result: 在各种噪声类型下表现优于现有方法，计算复杂度接近标准交叉熵损失。

Conclusion: ANTIDOTE在实际环境中有效，能够自动减少噪声样本影响，具有良好的实用性与效率。

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [127] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: 提出一种融合诊断码历史和实验室检测数据的多模态模型，用于提前一年检测胰腺导管腺癌，提高预测准确率并识别相关生物标志物。


<details>
  <summary>Details</summary>
Motivation: 胰腺导管腺癌早期检测难度大，缺乏特异性症状和可靠生物标志物。

Method: 结合神经控制微分方程、预训练语言模型、循环网络和交叉注意机制，分析电子健康记录中的时间序列和诊断信息。

Result: 在4700名患者的数据集上，AUC提升6.5%至15.5%，准确性显著优于现有方法，且识别出新的风险相关生物标志物。

Conclusion: 多模态融合方法有效提升PDAC早期检测能力，具有临床应用潜力。

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [128] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: 引入一种基于广义矩估计的新方法，有效结合合成数据与真实数据，提升社会科学中的回归分析效果。


<details>
  <summary>Details</summary>
Motivation: 解决合成样本与真实样本结合时的统计推断问题，确保结论的有效性。

Method: 提出一种无超参数的广义矩方法估计器，并利用合成数据与真实数据的矩残差交互优化估计。

Result: 该方法在多种回归任务中表现出显著改善，验证了其在计算社会科学中的实用性。

Conclusion: 新估计器增强了合成样本在数据分析中的应用潜力，为未来研究提供了可靠的工具。

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [129] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: 提出了两种适应性阈值检测框架（SCS和MACS）用于在非平稳环境中提升异常检测效果，实验显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列数据广泛应用，非平稳环境导致传统静态阈值失效，亟需更有效的动态检测方法。

Method: 引入基于统计学习和分段原则的SCS和MACS框架，支持局部、上下文敏感的阈值适应，确保误报率保证。

Result: 在晶圆制造数据集上，显著优于传统的百分比和滚动分位数方法，F1得分提升明显。

Conclusion: 稳健的统计学原则阈值技术可实现可靠、解释性强和及时的异常检测，适应多变环境。

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [130] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: USM通过迭代的混沌游戏表示实现高效的符号序列编码，解决偏差并揭示其收敛特性，可用于复杂序列的数值映射。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer型语言模型的发展，研究多尺度、多维度的符号序列编码机制以保留丰富的上下文信息成为重要课题。

Method: 提出并改进Universal Sequence Maps (USM)，结合正反迭代的Chaos Game Representations，实现符号序列的双向编码和频率分析，解决偏差问题，探索其数值收敛性。

Result: USM实现了序列的唯一编码，解决了偏差问题，并揭示其作为一种高效数值收敛过程的本质。应用于基因组序列验证效果，并可推广到任意字母集。

Conclusion: USM是一种具有广泛适用性和高效性的符号序列编码方法，具有在生物信息学和自然语言处理中的潜在应用价值。

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [131] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: 提出一种针对表格数据的高质量合成生成网络——TabularARGN，具有良好的隐私保护和数据实用性。


<details>
  <summary>Details</summary>
Motivation: 解决传统匿名化技术在隐私保护和数据实用性上的不足，推动合成数据在敏感数据共享中的应用。

Method: 采用离散化自回归方法的神经网络架构，生成高保真度的合成表格数据。

Result: 在统计相似性、机器学习实用性和检测鲁棒性方面优于或媲美现有方法，并在隐私攻击评估中表现出良好的隐私保护能力。

Conclusion: TabularARGN是有效平衡隐私和数据实用性的高质量合成数据生成方案，具有广泛应用潜力。

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [132] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 该论文提出CORAL框架，将预训练的世界模型作为信息代理，引导控制代理实现零样本适应，提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理难以泛化到新任务，因其表示和策略过拟合训练环境，需改善适应能力。

Method: 将ICRL问题转化为两个代理的通信问题，采用信息代理预训练为世界模型，学习可迁移的通信表达，并引入因果影响损失优化通信协议，控制代理通过 interpreting 通信上下文解决新任务。

Result: 验证表明，该方法增强了控制代理的样本效率，实现在未知环境中的零样本快速适应，证明了迁移通信表达的有效性。

Conclusion: 通过分离潜在表示学习和控制，CORAL框架实现了可迁移的通信机制，显著提升了强化学习在新环境中的泛化能力和样本效率。

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [133] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 引入Kolmogorov-Arnold Networks（KANs）以增强GNN性能，提出三种新模型，并采用知识融合提升效果，展示了KAN在提升图神经网络表达能力和推理效率方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在可扩展性和效率方面的限制，探索更强表达力和图无关推理的方法。

Method: 将KAN引入GAT、SGC、APPNP，构建新模型，并利用多教师知识融合框架提升学生模型性能。

Result: 新模型在节点分类任务中表现优越，知识融合显著增强学生模型效果，验证了KAN在GNN中的应用潜力。

Conclusion: KAN有助于提升GNN的表达能力和推理效率，推动图神经网络的发展与应用。

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [134] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 提出一种适用于Kolmogorov-Arnold Networks (KAN)的水印方法DCT-AW，具有任务无关性和良好的抗攻击性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中知识产权保护的重要性增加，需开发适应新型架构的水印技术，特别是具有可学习激活函数的KAN结构。

Method: 利用离散余弦变换对KAN的激活输出进行扰动，实现水印嵌入。

Result: DCT-AW在模型性能几乎无影响的同时，展现出优越的抗水印去除攻击能力。

Conclusion: DCT-AW成功应对KAN的架构挑战，提供了一种高效、鲁棒的水印解决方案。

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [135] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: 提出HeteRo-Select框架，优化异质性联邦学习中的客户端选择，提升训练稳定性和准确率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中因客户端数据异质性导致的训练不稳定和性能下降问题。

Method: 开发基于理论分析的逐步评分系统，考虑客户端的实用性、公平性、更新速度和数据多样性。并在强正则化条件下证明其收敛性。

Result: 在CIFAR-10数据集上实验显示，HeteRo-Select优于现有方法，获得更高的峰值和最终准确率，训练过程更稳定。

Conclusion: HeteRo-Select结合理论和实验验证，成为解决异质性联邦学习中客户端选择问题的可靠方案。

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [136] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: CISO是一种基于深度学习的物种分布模型，可以在有限的物种观察数据条件下，整合环境和部分生物信息，提升不同物种群体分布预测的效果。


<details>
  <summary>Details</summary>
Motivation: 现有物种分布模型多忽略物种间的相互作用，且实际观察数据稀疏、信息不完整，亟需一种能有效利用有限生物信息的方法。

Method: 提出CISO模型，利用深度学习技术，支持在环境变量基础上，条件化部分物种观察数据进行预测，且能结合多个数据集的观察信息。

Result: 在不同物种类别的数据集上，CISO显著优于传统方法，尤其在分布预测和物种相互作用推断方面表现出较强能力，且通过多数据集融合能进一步改善性能。

Conclusion: CISO是一种具有潜力的生态学工具，可在信息有限的条件下，整合不同物种信息，为生态保护和物种交互研究提供支持。

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [137] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: 提出了一种结合时间滞后交叉相关与对比学习的序列预测框架TLCCSP，有效提升多领域时间序列的预测准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测中，现有深度学习模型忽视了序列间的时滞交叉相关关系，限制了模型性能。

Method: 引入SSDTW算法捕获滞后相关性，并通过对比学习编码器高效近似距离，结合框架提升预测效果。

Result: 在天气、金融和房地产数据上，模型显著降低预测误差，尤其在天气数据上MSE分别减少16.01%、17.88%。同时，方法大幅度降低了计算时间，达99%。

Conclusion: TLCCSP通过整合滞后相关性和对比学习，有效提升时间序列预测的准确性和计算效率，适用于多领域实时预测。

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [138] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: 本文提出了一种鲁棒的Lyapunov框架，扩展了Schedule-Free（SF）方法在非凸优化中的性能保证，获得了与时间无关的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决传统一阶方法对预定时间T依赖的收敛保证，促进优化算法在实际中无需提前知道学习时间。

Method: 引入单步下降不等式的Lyapunov分析框架，结合不同步长策略，推导非凸情况下的无界限保证。

Result: 在非凸优化中获得了多种渐近收敛速率，包括$O(1/\log T)$和$O(	ext{log} T/T)$，验证了速率的实用性与潜在的提升空间。

Conclusion: 扩展了SF的时间无关保障至光滑非凸优化，为未来实现更优的非凸优化速率提供了理论基础。

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [139] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: 提出Fed MobiLLM，通过服务器辅助和特征对齐，实现异构移动设备上高效联邦微调大模型，显著降低资源消耗，提高速度。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦微调方法在移动设备上的高计算和通信负担，以及设备间异质性带来的挑战。

Method: 采用轻量级前向传播、服务器端独立训练侧网络、以及自适应层间特征对齐，实现异步高效微调。

Result: 在保持微调性能的同时，大幅度降低了内存、计算和通信开销，提升了收敛速度，验证了其在异构设备上的实用性。

Conclusion: Fed MobiLLM通过创新架构和技术，有效应对移动端联邦微调的系统挑战，推动个性化智能的实践应用。

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [140] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: PANAMA算法提升了多智能体路径规划的效率与鲁棒性，促进数字孪生在自动化和网络共享中的应用。


<details>
  <summary>Details</summary>
Motivation: 伴随数字孪生和自动化系统的发展，需求高效数据共享和智能路径规划，以支持复杂环境下的自治任务。

Method: 提出具有优先级不对称的网络感知多智能体强化学习（MARL）算法PANAMA，采用集中训练与分散执行架构。

Result: 在路径规划任务中表现优于现有方法，提升了准确性、速度和扩展性，通过仿真验证了其在复杂环境中的鲁棒性和效率。

Conclusion: PANAMA促进了网络感知决策与多智能体协调的融合，推动数字孪生、无线网络和AI自动化的协同发展。

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [141] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: 提出了一种无监督检测模型漂移的方法ZDP，基于transformer激活的空方向，拥有理论保证和实际指标。


<details>
  <summary>Details</summary>
Motivation: 不依赖任务标签或输出评价，检测transformer模型的表示漂移，确保模型稳定性。

Method: 通过理论分析激活空间的null空间及其Fisher信息，提出谱null泄漏指标，并建立漂移阈值。

Result: 证明了Variance-Leak定理、Fisher Null-Conservation等关键性质，并提出了具备非渐近尾界的漂移检测指标。

Conclusion: 监控激活的null空间及其Fisher几何，为模型表示变化提供了理论保障和实用工具，增强模型检测能力。

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [142] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: 提出PROPS框架，实现偏好级隐私保护的LLM对齐，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在利用人类反馈对LLMs进行对齐时，关注隐私保护，避免泄露偏好信息。

Method: 多阶段逐步私有自我对齐框架，前阶段模型作为标签器辅助后续阶段。

Result: 在多个模型和数据集上验证，PROPS在同等隐私预算下提升显著的对齐效果。

Conclusion: PROPS实现偏好级隐私保护，有效提升LLM对齐性能，优于传统方法。

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [143] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: 提出了一种非线性Tucker自编码器（MA-NTAE），通过引入非线性和逐模策略，有效改善高阶张量的特征学习和计算效率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决高阶张量在自监督学习中面临的维度灾难和非线性关系学习能力不足的问题。

Method: 将Tucker分解推广到非线性框架，采用逐模展开策略，通过递归操作实现高阶张量的柔性编码。

Result: 在压缩和聚类任务中，MA-NTAE在性能上优于传统自编码器和现有张量网络，特别是在高阶高维场景中优势明显。

Conclusion: MA-NTAE通过结合非线性变换和逐模策略，有效提升高阶张量学习的表达能力和计算效率，具有广泛应用潜力。

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [144] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: HARDY-MER通过动态设定难度样本的学习优先级，提升了多模态情感识别中对缺失模态样本的处理能力。


<details>
  <summary>Details</summary>
Motivation: 缺失模态是多模态情感识别中的关键挑战，现有方法在困难样本处理上存在不足。

Method: 提出硬度感知的动态课程学习框架HARDY-MER，通过多视角硬度评估和检索式动态调度策略，优化训练过程。

Result: 实验证明HARDY-MER优于现有方法，有效提升缺失模态场景下的识别性能。

Conclusion: 硬度感知与动态调度机制显著改善了多模态情感识别中硬样本的学习效果。

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [145] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: 提出一种无分类器引导的扩散生成方法CFDG，提升离线到在线强化学习的数据增强效果，显著改善性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有离线到在线RL中生成数据质量不足，限制性能提升的问题。

Method: 引入无分类器引导扩散生成技术，结合重加权策略增强生成数据与在线数据的匹配，无需额外分类器训练。

Result: 在多个基准任务中实现平均15%的性能提升，优于传统方法和标准扩散模型。

Conclusion: CFDG是一种有效的通用数据增强策略，可提升离线到在线强化学习的效果，适配不同算法。

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [146] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 该研究通过开源方法，将大规模语言模型应用于金融领域中的Q语言，提出了新数据集，进行了多阶段训练，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 弥补大模型在专业或少见编程语言上的表现不足，促进模型在特定领域的应用。

Method: 构建Q语言评测数据集，进行预训练、监督微调和强化学习，基于Qwen-2.5系列模型。

Result: 模型在Q任务中表现优异，最高达到59%的准确率，超过现有最优模型，且全部模型优于GPT-4.1。

Conclusion: 提供了完整的开发流程和多任务可扩展框架，推动模型在专业领域的应用推广。

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [147] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: 本文探讨了在神经网络中检测隐藏行为的挑战，将检测问题框架化为一种对抗游戏，并评估了多种检测策略的效果，特别强调了在LMM审核中的方法需求。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中隐藏有害行为检测难题，提升网络安全性和可信度。

Method: 通过模拟红蓝对抗游戏，使用不同检测策略（如高斯噪声分析、模型差异、集成梯度和对抗攻击）进行实验，验证其效果，特别关注其在CNN和大型语言模型（LLM）中的应用。

Result: 对抗攻击法实现了100%的高准确率，其他方法表现不一。发现LLM的检测需要一些关于异常分布的线索，以借助黑箱和开源模型方法进行检测。

Conclusion: 提出将隐藏行为检测模型化为对抗游戏，开源工具，促进更有效的模型审计方法开发。

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [148] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 本文研究了在多任务强化学习中，稀疏化方法（如GMP和SET）通过提升网络可塑性，有效改善模型适应性和性能，与密集网络相比表现更优。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中的可塑性下降限制了模型适应多任务的能力，亟需提升其网络的弹性。

Method: 系统分析稀疏化技术（GMP与SET）在多任务强化学习架构中的作用，通过实验证明其改善可塑性与性能。

Result: 稀疏化技术有效缓解可塑性衰退现象，提升多任务性能，多次超越密集网络，并具备与专门干预措施相当的效果。

Conclusion: 动态稀疏化是提升多任务强化学习模型适应性的可靠工具，促进模型在复杂环境下的灵活性发展。

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [149] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: 本文综述了符合预测在可信AI中的应用潜力，强调其校准的置信保证和在偏差识别、风险控制中的作用。


<details>
  <summary>Details</summary>
Motivation: 推动可信AI的发展，利用符合预测提供可靠的不确定性估计。

Method: 回顾理论基础、实验示例，以及符合预测在偏差检测和AI治理中的应用。

Result: 符合预测不仅具备校准的置信保证，还能有效指导偏差检测和风险管理，促进可信AI实践。

Conclusion: 符合预测在增强AI可信性和治理方面具有广阔前景，是实现可信AI的重要工具。

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [150] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: 提出了一种名为QuiZSF的轻量级框架，将检索增强生成（RAG）与预训练时间序列模型结合，用于零样本时间序列预测，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决零样本时间序列预测中缺乏动态引入外部知识的问题，弥补传统模型和预训练模型的不足。

Method: 构建分层结构的时间序列存储和检索系统，设计细粒度关系特征提取器，以及双分支模型协同机构，实现知识的有效整合和模型适应。

Result: 在多项预测任务中表现优异，分别在75%和87.5%的设定中排名第一，同时保持效率和较低的资源开销。

Conclusion: 通过融合检索和表示学习，增强零样本时间序列预测能力，为实际应用提供了高效方案。

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [151] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: 通过引入类别不平等损失和类别加权训练，缓解医学诊断中的类别特征偏差和类别不平衡问题，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学诊断模型中的类别偏差问题，改善模型性能和泛化能力。

Method: 提出类别不平等损失和类别加权的鲁棒优化目标，平衡正负样本贡献，提升少数类表现。

Result: 在合成和真实数据上实验显示方法有效减轻类别偏差和类别不平衡，增强模型泛化。

Conclusion: 本文方法成功缓解偏差与不平衡，提高诊断模型的公正性与稳健性。

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [152] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 提出一种自适应元微调（AMFT）算法，动态平衡SFT与RL的奖励信号，优化大规模语言模型的训练，提高任务性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统两阶段微调（SFT+RL）中遗忘和折中问题，寻求一种更加统一和高效的训练方法。

Method: 引入隐式奖励的理论视角，设计元梯度自适应权重控制器，动态调节SFT与RL的平衡，通过元学习优化其参数。

Result: 在数学推理、抽象视觉推理和视觉语言导航等基准任务上，AMFT实现了新高，提升了模型的泛化能力并优于现有方法。

Conclusion: AMFT通过自适应控制奖励信号，有效改善LLM训练，提供了更具原则性的训⽹范式，并在多个任务中表现出色。

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [153] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: BoRA通过引入块对角矩阵，提升LoRA的秩，增强模型性能，且参数开销少。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型中，提升LoRA的秩以改善微调效果，且控制参数增长。

Method: 将LoRA分块处理，通过引入块对角矩阵增强块之间的多样性，从而提升整体秩。

Result: 实验验证BoRA在多数据集和模型上优于传统LoRA，且可扩展性良好。

Conclusion: BoRA是一种有效的低参数量提升LoRA秩的策略，提升微调性能。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [154] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: 通过多任务学习利用卫星数据的模态进行内在解释，提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习模型在复杂性和可解释性之间的矛盾，利用卫星数据的丰富信息实现模型的内在解释。

Method: 采用多任务学习，将某些模态作为预测目标，与主任务共同训练，以增强模型的可解释性和性能。

Result: 在三种任务（分割、分类、回归）上验证了此方法的有效性，性能媲美甚至优于基线，且模型误差可通过辅助任务解释。

Conclusion: 多任务学习能有效利用卫星数据的模态信息，提升模型性能和可解释性，具有广泛应用潜力。

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [155] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: 本文提出一种基于结构保持的缩减有限元模型框架，用于实时数字孪生的构建，结合条件注意机制和有限元外微积分，确保数值稳定性和守恒量的精确保持，实现复杂系统的高效预测和校准。


<details>
  <summary>Details</summary>
Motivation: 推动数字孪生技术的实时性和精确性，解决复杂系统建模中的效率与守恒性难题。

Method: 利用条件注意机制和有限元外微积分结合结构保持缩减有限元模型，支持实时校准和复杂几何应用。

Result: 在多种仿真任务中实现了高精度和极快的推理速度（0.1秒），显著超越传统模拟，且提供开源实现。

Conclusion: 该框架有效结合数学理论与学习机制，提升数字孪生的实用性，为复杂系统的实时监测与控制提供新途径。

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [156] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: Discovery Learning（DL）是一种结合主动学习、物理指导和零样本学习的科学机器学习方法，能在无需额外标签的情况下，从历史电池设计中学习，快速评估新设计的电池寿命，显著节省时间和能源，推动电池技术创新。


<details>
  <summary>Details</summary>
Motivation: 当前电池研发面临高时间和能量成本，需快速验证新设计以加快创新步伐，但现有方法不足以实现高速、可靠的预测。

Method: 提出Discovery Learning（DL）方法，该方法融合主动学习、物理指导和零样本学习，利用历史设计数据，减少对新样本的依赖，提升电池寿命预测效率。

Result: 在123个工业级锂离子电池上测试，DL在未见过的设备变异条件下实现7.2%的测试误差，节省98%的时间和95%的能量，验证其高效性和实用性。

Conclusion: DL能够从历史设计中提取有价值的洞见，加快新一代电池技术的开发，推动科学发现和工程创新的进步。

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [157] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 提出了一种统一的多城市人类移动预测模型UniMove，通过双塔架构和MoE Transformer实现跨城适应和增强，提升预测准确率超过10%。


<details>
  <summary>Details</summary>
Motivation: 解决不同城市在人类移动预测中因空间表征和特征差异带来的困难，构建一个通用的模型以适应多城市数据。

Method: 采取轨迹-位置双塔架构，设计MoE Transformer块实现专家选择，结合多城市数据进行联合训练和增强。

Result: 在多个城市数据集上验证，UniMove显著提升了移动预测的准确性，成为实现人类移动基础模型的重要进展。

Conclusion: UniMove通过统一架构和多任务学习，成功实现多城市人类移动的精准预测，为未来基础模型的开发提供了新方向。

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [158] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: 本文评估了多种特征选择方法在Tsetlin机中的应用，强调了TM内部评分器的优越性及其解释能力，为TM的可解释性研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 由于Tsetlin机器缺乏有效的特征重要性估计工具，亟需引入和评估各种特征选择方法以提升其解释性和性能。

Method: 采用经典滤波、嵌入式方法、神经网络解释技术（如SHAP、LIME）以及新型TM特有的方法，结合多数据集和不同评估策略进行系统比较分析。

Result: TM内部评分器表现出竞争力，不仅准确性相似，还揭示了特征交互模式，简化模型时减少计算成本。

Conclusion: 本研究为TM的特征选择提供了全面基线，推动未来开发专用于TM的解释性技术。

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [159] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: 研究提出一种基于合法预测集的贪心算法，用于从多个专家中选择子集以提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 优化多专家人类-AI协作中的专家子集选择，提高分类准确性。

Method: 设计贪心算法结合合法预测集进行专家子集筛选。

Result: 算法在CIFAR-10H和ImageNet-16H数据集上接近最优，改善多专家分类性能。

Conclusion: 基于合法预测集的专家子集选择能显著提升多专家协作的分类效果。

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [160] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: 该论文提出结合行为克隆与离线强化学习的方法，以提升自动驾驶策略的鲁棒性和长远表现。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中通过大规模真实数据学习鲁棒策略的挑战，尤其面对行为克隆的脆弱性和误差积累问题。

Method: 开发渐进式行为克隆基线模型，结合Transformer结构和实体中心状态表示，并利用离线强化学习中的保守Q学习（CQL）优化策略。

Result: 在Waymo公开数据集的测试中，提出的方法显著优于纯行为克隆，成功率提升3.2倍，碰撞率降低7.4倍，验证了离线RL在长远鲁棒驾驶中的优势。

Conclusion: 离线强化学习，特别是CQL，为学习鲁棒、长远的自动驾驶策略提供了关键路径，值得在实际应用中推广。

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [161] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: 本文提出了一种面向疾病不同阶段的多专家混合模型（MoE），结合动态图神经网络，能更好地模拟神经退行性疾病的时空发展过程，并提供阶段性机制的洞见。


<details>
  <summary>Details</summary>
Motivation: 解决神经退行性疾病长期发展模型中的数据稀缺和机制复杂性问题，提供更精准的疾病进展理解。

Method: 采用时序双重优化估算个体状态，融合异质图神经扩散模型（IGND）与局部神经反应模块，构建动态、多机制的模型框架。

Result: 模型有效捕捉疾病不同阶段的贡献机制变化，与现有文献一致，揭示早期为图相关过程，后期为未知物理过程。

Conclusion: 该模型为疾病进展提供更细粒度的机制理解，有助未来临床诊断与干预策略制定。

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [162] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: OTAKNet是一种解决噪声统计漂移的在线学习滤波方法，结合了最优传输技术，能在无需重训练的情况下实时适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 为了应对实际应用中环境变化导致的噪声统计漂移，传统学习滤波方法性能下降的问题。

Method: 提出OTAKNet，利用一阶预测测量似然和最优传输实现在线适应，无需地面真实标签或重训练。

Result: 在合成和真实NCLT数据集上，显示出优异性能，尤其在训练数据有限的情况下优于传统方法。

Conclusion: OTAKNet为非线性动态系统中的噪声漂移提供了一种高效的在线解决方案，具有潜在的广泛应用前景。

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [163] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: 该研究系统分析了六种大规模语言模型知识蒸馏技术的隐私风险，发现所有方法均存在成员资格和记忆隐私泄露风险，并受到多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 随着知识蒸馏在大模型中的应用扩大，隐私风险成为亟需解决的问题，特别是在数据私密性方面。

Method: 本文采用跨多 NLP 任务、不同教师模型和学生模型的实验设置，系统分析不同蒸馏技术的隐私风险及其影响因素，结合理论分析与实证研究。

Result: 所有蒸馏技术都存在隐私泄露风险，但风险程度不同。关键因素包括蒸馏目标、训练数据和任务类型。记忆与成员隐私风险表现出明显差异，隐私风险在模型不同区块间差异显著。

Conclusion: 必须重视和缓解知识蒸馏中的隐私风险，未来研究应探索更有效的隐私保护机制，以确保模型在性能优化的同时保护数据隐私。

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [164] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: 提出一种“先忘记再学习”的知识编辑策略，有效解决LLMs的动态知识更新难题，确保高准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在动态知识更新中的抗冲突能力不足和灾难性遗忘问题。

Method: 结合参数高效微调技术和内激活抑制，利用电路定位实现目标内部组件的特定编辑。

Result: 该方法在微软Phi-3-mini-4k-instruct模型上表现优异，邻近完美地实现新知识的存储与旧知识的抑制，同时提升定位准确性，减少灾难性遗忘。

Conclusion: 该策略为LLMs的知识管理提供了精准、安全且可解释的解决方案，有助于未来模型的安全高效更新。

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [165] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: 提出一种结合Transformers与Autoencoders的混合模型，用于检测数据漂移，表现出更早和更敏感的检测性能。


<details>
  <summary>Details</summary>
Motivation: 概念漂移会降低模型性能，现有检测方法反应慢且敏感度不足。

Method: 设计结合Transformers与Autoencoders的混合框架，利用Trust Score结合多种指标实现在线漂移检测。

Result: 模型在模拟数据集上表现优异，能更早检测到漂移，优于现有方法。

Conclusion: 该框架为实时监测概念漂移提供了一种更敏感、更稳健的解决方案。

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [166] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: 该论文提出二阶MeanFlow框架，引入平均加速度，理论验证其稳定性与表达能力，提供高效实现方案，推动高阶流匹配模型发展。


<details>
  <summary>Details</summary>
Motivation: 为提升生成模型的效率与表达能力，研究引入高阶动力学（如加速度）到MeanFlow框架中。

Method: 通过理论证明平均加速度满足一致性条件，利用电路复杂性分析探讨模型表达能力，结合快速注意力近似实现高效采样。

Result: 验证了二阶MeanFlow的稳定性与表达能力，提出了多项高效实现策略，奠定了高阶流匹配模型的理论基础。

Conclusion: 二阶MeanFlow扩大了Flow Matching的能力范围，结合高效算法，为未来高动态复杂模型提供了理论和实践基础。

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [167] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: 提出了一种用于动态脑功能连接预测和年龄估计的自适应时序连接学习框架BrainATCL。


<details>
  <summary>Details</summary>
Motivation: 解决传统图神经网络难以捕捉fMRI动态数据中的长距离时间依赖的问题，从而更好理解大脑的瞬时神经状态和网络重构。

Method: 该框架动态调整每个快照的观察窗口，利用GINE-Mamba2编码序列，结合脑结构信息增强建模能力。

Result: 在功能连接预测和年龄估计任务中表现优异，具有良好的泛化能力，尤其在跨会话预测中表现出色。

Conclusion: BrainATCL为动态fMRI数据的空间-时间表示提供了有效工具，有助于理解大脑功能动态变化，与行为和疾病研究具有潜在应用价值。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [168] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习方法，通过多实例学习实现更精确的参数预测，增强模型辨别能力并降低误差，结合粒子物理数据实现理论最大信息提取。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在复杂问题中预测精度不足和辨别能力有限的问题，提升机器学习在粒子物理中的应用效果。

Method: 引入多实例学习（MIL）理论，并分析其在样本规模上的表现，结合粒子碰撞数据，应用于标准模型有效场的Wilson系数限制。

Result: 理论证明MIL在预测能力上优于单实例模型，通过实际粒子碰撞数据验证模型的有效性，揭示在特定条件下能最大程度提取信息。

Conclusion: 多实例学习在复杂的参数预测和信息提取方面具有明显优势，为粒子物理实验数据分析提供了新的工具和理论基础。

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [169] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: 引入LOGIC框架，使用大型语言模型提升图神经网络（GNN）解释的可解释性与细粒度。


<details>
  <summary>Details</summary>
Motivation: 弥补现有GNN解释方法在处理丰富自然语言节点属性时的不足，提升解释的合理性和细粒度。

Method: 将GNN节点嵌入投影到LLM嵌入空间，构建混合提示，结合图结构文本，促使LLM生成合理、自然的解释和简明的子图。

Result: 在四个真实TAG数据集上，LOGIC在保持高忠实度的同时显著改善了理解性指标，展现了良好的解释效果。

Conclusion: LOGIC为基于LLM的图学习可解释性提供新方向，将GNN内部机制与人类推理相结合。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [170] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: 提出了一种基于时空图神经网络的后端系统性能预测模型，通过结合图卷积和门控递归网络，有效捕捉业务调用关系和性能变化，实验验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端系统中多级调用结构下性能波动预测的挑战。

Method: 抽象系统状态为图结构，结合图卷积网络和门控递归网络，加入时间编码机制，端到端训练以预测性能指标。

Result: 模型在MAE、RMSE和R2等指标上优于现有方法，在不同负载和结构条件下表现出良好的鲁棒性和高精度。

Conclusion: 该模型具有实际应用潜力，有助于后端服务性能管理。

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [171] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: Pref-GUIDE通过将实时标量反馈转换为偏好数据，提高了持续策略训练中的奖励模型准确性，超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 在人类反馈难以用密集奖励函数明确表达任务目标时，有效利用人类反馈至关重要，特别是在在线学习场景中。

Method: 将实时标量反馈转化为偏好数据，结合短时行为比较与投票机制，提高奖励模型的鲁棒性和泛化能力。

Result: 在三个复杂环境中显著优于基线，投票版本甚至超过专家设计的密集奖励。

Conclusion: Pref-GUIDE提供了一种结构化的人类反馈利用框架，有效提升在线强化学习的表现与适应性。

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


### [172] [How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?](https://arxiv.org/abs/2508.07127)
*Niranjana Arun Menon,Iqra Farooq,Yulong Li,Sara Ahmed,Yutong Xie,Muhammad Awais,Imran Razzak*

Main category: cs.LG

TL;DR: 应用大规模语言模型（LLMs）预测心血管疾病，通过分析遗传标记和家系遗传信息，提升早期诊断和个性化治疗的潜力。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病预测面临多因素复杂性和数据分析困难，亟需新方法以从多源高维数据中提取有意义的生物信息。

Method: 利用微调的LLMs处理结构化和半结构化基因组数据，将问题设定为Chain of Thought推理任务，引导模型生成疾病标签和临床推断。

Result: 结果显示LLMs在识别遗传模式、早期检测和风险评估方面具有潜力，有助于推动个性化医疗发展。

Conclusion: LLMs在心血管疾病预测中展现出显著潜力，为临床早期诊断和精准治疗提供新工具。

Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned LLMs to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how LLMs can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of LLMs in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.

</details>


### [173] [A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs](https://arxiv.org/abs/2508.07134)
*Lu Chenggang*

Main category: cs.LG

TL;DR: 提出一种基于正交分解的全局最优半非负矩阵分解算法，解决传统方法易陷入局部极小值的问题。


<details>
  <summary>Details</summary>
Motivation: 弥补现有半非负矩阵分解算法在优化时易陷入局部极小值且不能保证全局最优的缺陷，提供一种理论与实践兼备的新方法。

Method: 采用由输入数据的散点矩阵导出的正交分解，确保在Frobenius范数下获得全局最优解。

Result: 该方法在低秩情况下可完全恢复NMF结构；在数据集上验证比现有方法更优的重建误差，具有理论保障和实证优势。

Conclusion: 提出的全局最优非迭代方法为矩阵分解提供新的思路和工具，在优化和数据分析中具有重要意义。

Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical
Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain
both positive and negative entries, making it suitable for decomposing data
with mixed signs. However, most existing semi-NMF algorithms are iterative,
non-convex, and prone to local minima. In this paper, we propose a novel method
that yields a globally optimal solution to the semi-NMF problem under the
Frobenius norm, through an orthogonal decomposition derived from the scatter
matrix of the input data. We rigorously prove that our solution attains the
global minimum of the reconstruction error. Furthermore, we demonstrate that
when the input matrix is nonnegative, our method often achieves lower
reconstruction error than standard NMF algorithms, although unfortunately the
basis matrix may not satisfy nonnegativity. In particular, in low-rank cases
such as rank 1 or 2, our solution reduces exactly to a nonnegative
factorization, recovering the NMF structure. We validate our approach through
experiments on both synthetic data and the UCI Wine dataset, showing that our
method consistently outperforms existing NMF and semi-NMF methods in terms of
reconstruction accuracy. These results confirm that our globally optimal,
non-iterative formulation offers both theoretical guarantees and empirical
advantages, providing a new perspective on matrix factorization in optimization
and data analysis.

</details>


### [174] [A Stable and Principled Loss Function for Direct Language Model Alignment](https://arxiv.org/abs/2508.07137)
*Yuandong Tan*

Main category: cs.LG

TL;DR: 提出一种新型损失函数，以稳定和有效地实现大语言模型的偏好对齐，避免DPO的理论偏差。


<details>
  <summary>Details</summary>
Motivation: 解决现有DPO方法在训练稳定性和偏好对齐效果上的不足。

Method: 基于RLHF的最优性条件，设计一个目标为固定值的损失函数，避免最大化logits差值。

Result: 在Qwen2.5-7B模型上验证，显著优于标准DPO，表现与更大模型Llama-3.1-8B接近。

Conclusion: 提出的损失函数增强了训练稳定性，防止reward hacking，提高模型偏好对齐效果。

Abstract: The alignment of large language models (LLMs) with human preferences is
commonly achieved through Reinforcement Learning from Human Feedback (RLHF).
Direct Preference Optimization (DPO) simplified this paradigm by establishing a
direct mapping between the optimal policy and a reward function, eliminating
the need for an explicit reward model. However, we argue that the DPO loss
function is theoretically misaligned with its own derivation, as it promotes
the indefinite maximization of a logits difference, which can lead to training
instability and reward hacking. In this paper, we propose a novel loss function
derived directly from the RLHF optimality condition. Our proposed loss targets
a specific, finite value for the logits difference, which is dictated by the
underlying reward, rather than its maximization. We provide a theoretical
analysis, including a gradient-based comparison, to demonstrate that our method
avoids the large gradients that plague DPO when the probability of dispreferred
responses approaches zero. This inherent stability prevents reward hacking and
leads to more effective alignment. We validate our approach by fine-tuning a
Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO
baseline and achieving competitive performance against larger models like
Llama-3.1-8B.

</details>


### [175] [Strategic Incentivization for Locally Differentially Private Federated Learning](https://arxiv.org/abs/2508.07138)
*Yashwant Krishna Pagoti,Arunesh Sinha,Shamik Sural*

Main category: cs.LG

TL;DR: 本文提出了一种基于激励机制的隐私与模型准确性博弈模型，旨在平衡客户端隐私保护和全球模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决在联邦学习中保护隐私的同时保持模型准确性的问题。

Method: 通过将隐私保护的噪声控制转化为激励博弈，引入代币机制来调节客户端的噪声添加行为。

Result: 模型分析显示，激励机制能有效引导客户端在保护隐私和提高模型准确性间达成平衡，实验验证了参数设置对性能的影响。

Conclusion: 提出的激励策略能在保持较高模型准确率的同时，增强隐私保护，具有应用潜力。

Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning
model by sharing gradient information, instead of raw data, with a server over
multiple rounds. To address the possibility of information leakage in spite of
sharing only the gradients, Local Differential Privacy (LDP) is often used. In
LDP, clients add a selective amount of noise to the gradients before sending
the same to the server. Although such noise addition protects the privacy of
clients, it leads to a degradation in global model accuracy. In this paper, we
model this privacy-accuracy trade-off as a game, where the sever incentivizes
the clients to add a lower degree of noise for achieving higher accuracy, while
the clients attempt to preserve their privacy at the cost of a potential loss
in accuracy. A token based incentivization mechanism is introduced in which the
quantum of tokens credited to a client in an FL round is a function of the
degree of perturbation of its gradients. The client can later access a newly
updated global model only after acquiring enough tokens, which are to be
deducted from its balance. We identify the players, their actions and payoff,
and perform a strategic analysis of the game. Extensive experiments were
carried out to study the impact of different parameters.

</details>


### [176] [SGD Convergence under Stepsize Shrinkage in Low-Precision Training](https://arxiv.org/abs/2508.07142)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 低精度训练通过梯度缩减影响SGD的收敛速度与误差。


<details>
  <summary>Details</summary>
Motivation: 解决大规模深度学习中减低计算和存储成本的需求。

Method: 建立梯度缩减模型，分析低精度对SGD收敛行为的影响。

Result: 低精度会减慢收敛速度并增加稳定误差上限，但仍保证收敛。

Conclusion: 低精度训练影响收敛速度和误差，模型可通过有效步长分析理解这一区别。

Abstract: Low-precision training has become essential for reducing the computational
and memory costs of large-scale deep learning. However, quantization of
gradients introduces both magnitude shrinkage and additive noise, which can
alter the convergence behavior of stochastic gradient descent (SGD). In this
work, we study the convergence of SGD under a gradient shrinkage model, where
each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by
zero-mean quantization noise. We show that this shrinkage is equivalent to
replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$,
which slows convergence when $q_{\min} < 1$. Under standard smoothness and
bounded-variance assumptions, we prove that low-precision SGD still converges,
but at a reduced rate determined by $q_{\min}$, and with an increased
asymptotic error floor due to quantization noise. We theoretically analyze how
reduced numerical precision slows down training by modeling it as gradient
shrinkage in the standard SGD convergence framework.

</details>


### [177] [What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)
*Chanakya Ekbote,Marco Bondaschi,Nived Rajaraman,Jason D. Lee,Michael Gastpar,Ashok Vardhan Makkuva,Paul Pu Liang*

Main category: cs.LG

TL;DR: 两层变压器模型可以表达任何条件k-gram，深化了对Transformer中ICL能力的理解。


<details>
  <summary>Details</summary>
Motivation: 理解变压器中In-context learning（ICL）的机制，特别是深度与Markov过程的关系。

Method: 理论分析和简化的学习动力学研究，特别是针对一阶Markov链的情境。

Result: 证明了两层单头变压器可以表示任意条件k-gram，揭示了浅层架构也具备强大的ICL能力。

Conclusion: 浅层变压器架构在结构化序列建模中表现出强大能力，丰富了ICL机制的理论基础。

Abstract: In-context learning (ICL) is a hallmark capability of transformers, through
which trained models learn to adapt to new tasks by leveraging information from
the input context. Prior work has shown that ICL emerges in transformers due to
the presence of special circuits called induction heads. Given the equivalence
between induction heads and conditional k-grams, a recent line of work modeling
sequential inputs as Markov processes has revealed the fundamental impact of
model depth on its ICL capabilities: while a two-layer transformer can
efficiently represent a conditional 1-gram model, its single-layer counterpart
cannot solve the task unless it is exponentially large. However, for higher
order Markov sources, the best known constructions require at least three
layers (each with a single attention head) - leaving open the question: can a
two-layer single-head transformer represent any kth-order Markov process? In
this paper, we precisely address this and theoretically show that a two-layer
transformer with one head per layer can indeed represent any conditional
k-gram. Thus, our result provides the tightest known characterization of the
interplay between transformer depth and Markov order for ICL. Building on this,
we further analyze the learning dynamics of our two-layer construction,
focusing on a simplified variant for first-order Markov chains, illustrating
how effective in-context representations emerge during training. Together,
these results deepen our current understanding of transformer-based ICL and
illustrate how even shallow architectures can surprisingly exhibit strong ICL
capabilities on structured sequence modeling tasks.

</details>


### [178] [Neural Bridge Processes](https://arxiv.org/abs/2508.07220)
*Jian Xu,Yican Liu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 提出一种神经桥过程（NBPs）模型，通过动态锚点增强扩散过程的输入依赖性，有效改善了随机函数建模的性能和一致性。


<details>
  <summary>Details</summary>
Motivation: 克服传统高斯过程和神经过程在大规模、多模态分布建模中的局限性，增强模型表达能力和输入依赖。

Method: 将扩散核显式依赖于输入x，强制路径以目标为终点，利用DDPM风格的桥接采样增强端点一致性和梯度信号。

Result: 在多种任务中显著优于基线模型，包括合成数据、EEG和图像回归，验证了方法的有效性和理论优势。

Conclusion: 神经桥过程通过引入动态锚点和端点契合机制，提高了随机函数模型的性能和结构一致性，是一种有潜力的结构化预测工具。

Abstract: Learning stochastic functions from partially observed context-target pairs is
a fundamental problem in probabilistic modeling. Traditional models like
Gaussian Processes (GPs) face scalability issues with large datasets and assume
Gaussianity, limiting their applicability. While Neural Processes (NPs) offer
more flexibility, they struggle with capturing complex, multi-modal target
distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a
learned diffusion process but rely solely on conditional signals in the
denoising network, resulting in weak input coupling from an unconditional
forward process and semantic mismatch at the diffusion endpoint. In this work,
we propose Neural Bridge Processes (NBPs), a novel method for modeling
stochastic functions where inputs x act as dynamic anchors for the entire
diffusion trajectory. By reformulating the forward kernel to explicitly depend
on x, NBP enforces a constrained path that strictly terminates at the
supervised target. This approach not only provides stronger gradient signals
but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG
signal regression and image regression tasks, achieving substantial
improvements over baselines. These results underscore the effectiveness of
DDPM-style bridge sampling in enhancing both performance and theoretical
consistency for structured prediction tasks.

</details>


### [179] [LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference](https://arxiv.org/abs/2508.07221)
*Po-Han Lee,Yu-Cheng Lin,Chan-Tung Ku,Chan Hsu,Pei-Cing Huang,Ping-Hsun Wu,Yihuang Kang*

Main category: cs.LG

TL;DR: 利用大型语言模型代理自动发现混杂变量和亚组分析，以增强因果推断的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决因果推断中未测量混杂和结构偏差的问题，降低对专家的依赖，提高可扩展性和解释性。

Method: 引入基于大语言模型的智能代理，自动进行混杂变量发现和亚组分析，整合到因果机器学习流程中。

Result: 在真实医疗数据集上实验显示，该方法提升了治疗效果估计的鲁棒性，缩小置信区间，并发现了潜在的混杂偏差。

Conclusion: LM模型代理为规模化、可信且具有语义理解的因果推断提供了有前景的解决方案。

Abstract: Estimating individualized treatment effects from observational data presents
a persistent challenge due to unmeasured confounding and structural bias.
Causal Machine Learning (causal ML) methods, such as causal trees and doubly
robust estimators, provide tools for estimating conditional average treatment
effects. These methods have limited effectiveness in complex real-world
environments due to the presence of latent confounders or those described in
unstructured formats. Moreover, reliance on domain experts for confounder
identification and rule interpretation introduces high annotation cost and
scalability concerns. In this work, we proposed Large Language Model-based
agents for automated confounder discovery and subgroup analysis that integrate
agents into the causal ML pipeline to simulate domain expertise. Our framework
systematically performs subgroup identification and confounding structure
discovery by leveraging the reasoning capabilities of LLM-based agents, which
reduces human dependency while preserving interpretability. Experiments on
real-world medical datasets show that our proposed approach enhances treatment
effect estimation robustness by narrowing confidence intervals and uncovering
unrecognized confounding biases. Our findings suggest that LLM-based agents
offer a promising path toward scalable, trustworthy, and semantically aware
causal inference.

</details>


### [180] [EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning](https://arxiv.org/abs/2508.07224)
*Ananda Prakash Verma*

Main category: cs.LG

TL;DR: 提出一种名为EDGE的多阶段适应性学习框架，结合心理测量、认知诊断和反事实生成，优化学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决学习过程中对误解的识别和纠正问题，提升个性化教学效果。

Method: 融合IRT/贝叶斯模型、误解发现、对比性题目生成和调度策略，提出EdgeScore指标及其索引策略。

Result: 构建了理论基础和伪代码实现，证明了EdgeScore的单调性和连续性，提出了高效的误解纠正条件。

Conclusion: 为适应性学习提供理论支持和方法框架，未来验证与实证研究待进行。

Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning
framework composed of four stages: Evaluate (ability and state estimation),
Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual
item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies
psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics
(misconception discovery from distractor patterns and response latencies),
contrastive item generation (minimal perturbations that invalidate learner
shortcuts while pre-serving psychometric validity), and principled scheduling
(a restless bandit approximation to spaced retrieval). We formalize a composite
readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,
and derive an index policy that is near-optimal under mild assumptions on
forgetting and learning gains. We further establish conditions under which
counterfactual items provably reduce the posterior probability of a targeted
misconception faster than standard practice. The paper focuses on theory and
implementable pseudocode; empirical study is left to future work.

</details>


### [181] [Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation](https://arxiv.org/abs/2508.07243)
*Chu Zhao,Eneng Yang,Yizhou Dang,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.LG

TL;DR: CNSDiff通过在潜在空间合成负样本，有效避免了候选池偏差引入的错误负样本，增强了推荐模型在分布变化下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有启发式负采样方法受候选池中未观察到的环境偏差影响，容易引入误导性负样本。

Method: 提出CNSDiff方法，在潜在空间利用条件扩散过程合成负样本，并引入因果正则化以减轻环境偏差影响。

Result: CNSDiff在四个分布偏移场景下比最先进的基线平均提升13.96%，显示出其在正确性和鲁棒性方面的优越性能。

Conclusion: CNSDiff有效缓解了负采样中的偏差问题，增强了推荐系统在分布外环境下的性能表现。

Abstract: Heuristic negative sampling enhances recommendation performance by selecting
negative samples of varying hardness levels from predefined candidate pools to
guide the model toward learning more accurate decision boundaries. However, our
empirical and theoretical analyses reveal that unobserved environmental
confounders (e.g., exposure or popularity biases) in candidate pools may cause
heuristic sampling methods to introduce false hard negatives (FHNS). These
misleading samples can encourage the model to learn spurious correlations
induced by such confounders, ultimately compromising its generalization ability
under distribution shifts. To address this issue, we propose a novel method
named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing
negative samples in the latent space via a conditional diffusion process,
CNSDiff avoids the bias introduced by predefined candidate pools and thus
reduces the likelihood of generating FHNS. Moreover, it incorporates a causal
regularization term to explicitly mitigate the influence of environmental
confounders during the negative sampling process, leading to robust negatives
that promote out-of-distribution (OOD) generalization. Comprehensive
experiments under four representative distribution shift scenarios demonstrate
that CNSDiff achieves an average improvement of 13.96% across all evaluation
metrics compared to state-of-the-art baselines, verifying its effectiveness and
robustness in OOD recommendation tasks.

</details>


### [182] [Policy Newton methods for Distortion Riskmetrics](https://arxiv.org/abs/2508.07249)
*Soumen Pachal,Mizhaan Prajit Maniyar,Prashanth L. A*

Main category: cs.LG

TL;DR: 提出一种风险敏感控制的强化学习方法，利用DRM衡量风险，确保算法能找到二阶鞍点，具有理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注风险中性或一阶收敛，少有关于风险敏感目标的二阶收敛分析。

Method: 引入DRM衡量指标，通过推导Hessian定理，提出基于样本轨迹的Hessian估计器，并设计三次正则化的牛顿算法，保证收敛至二阶鞍点。

Result: 算法在有限步骤内保证收敛到$epsilon$-二阶稳定点，样本复杂度为$epsilon^{-3.5}$，并通过实验验证。

Conclusion: 首次实现风险敏感目标的二阶收敛保证，拓宽了风险敏感强化学习理论，为实际应用提供了强有力的算法支持。

Abstract: We consider the problem of risk-sensitive control in a reinforcement learning
(RL) framework. In particular, we aim to find a risk-optimal policy by
maximizing the distortion riskmetric (DRM) of the discounted reward in a finite
horizon Markov decision process (MDP). DRMs are a rich class of risk measures
that include several well-known risk measures as special cases. We derive a
policy Hessian theorem for the DRM objective using the likelihood ratio method.
Using this result, we propose a natural DRM Hessian estimator from sample
trajectories of the underlying MDP. Next, we present a cubic-regularized policy
Newton algorithm for solving this problem in an on-policy RL setting using
estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to
converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of
the DRM objective, and this guarantee ensures the escaping of saddle points.
The sample complexity of our algorithms to find an $ \epsilon$-SOSP is
$\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical
findings. To the best of our knowledge, our is the first work to present
convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing
works in the literature have either shown convergence to a first-order
stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral
one.

</details>


### [183] [PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets](https://arxiv.org/abs/2508.07253)
*Bartlomiej Chybowski,Shima Abdullateef,Hollan Haule,Alfredo Gonzalez-Sulser,Javier Escudero*

Main category: cs.LG

TL;DR: 提出了一种开源、鲁棒且具有良好泛化能力的脑电图癫痫发作检测框架，能够跨不同临床数据集实现准确的自动检测。


<details>
  <summary>Details</summary>
Motivation: 临床中癫痫发作检测依赖耗时的人工EEG解读，机器学习需要具备更强的泛化能力以实际应用。

Method: 建立一个包含自动预处理、多模型投票机制的机器学习框架，在两个不同数据集上进行训练、调优和验证，并测试其跨数据集的迁移能力。

Result: 模型在单一数据集表现良好，跨数据集依然保持较强的识别能力，结合后处理后效果更佳，展示了良好的泛化性。

Conclusion: 该方法具有推广到多样临床环境的潜力，为实现全面部署的癫痫发作自动检测系统奠定基础，能辅助专家诊断，加快临床应用。

Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy,
yet clinical workflows remain dependent on time-consuming manual EEG
interpretation. While machine learning has shown promise, existing approaches
often rely on dataset-specific optimisations, limiting their real-world
applicability and reproducibility. Here, we introduce an innovative,
open-source machine-learning framework that enables robust and generalisable
seizure detection across varied clinical datasets. We evaluate our approach on
two publicly available EEG datasets that differ in patient populations and
electrode configurations. To enhance robustness, the framework incorporates an
automated pre-processing pipeline to standardise data and a majority voting
mechanism, in which multiple models independently assess each second of EEG
before reaching a final decision. We train, tune, and evaluate models within
each dataset, assessing their cross-dataset transferability. Our models achieve
high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and
0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets
despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models
trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)
without any post-processing. Furthermore, a mild post-processing improved the
within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset
results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the
potential of, and essential considerations for, deploying our framework in
diverse clinical settings. By making our methodology fully reproducible, we
provide a foundation for advancing clinically viable, dataset-agnostic seizure
detection systems. This approach has the potential for widespread adoption,
complementing rather than replacing expert interpretation, and accelerating
clinical integration.

</details>


### [184] [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.LG

TL;DR: 本文综述了影响函数在深度学习中用于数据归因的理论基础、算法进展及其在数据追溯和误标检测中的应用，讨论了面临的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 理解个别训练样本对模型预测的影响对于模型解释、数据调试和责任追究至关重要。

Method: 结合影响函数的理论基础，介绍高效逆Hessian-向量乘积估算算法，并评估其在数据归因和误标检测中的效果。

Result: 影响函数可以有效近似训练数据对模型预测的影响，提升数据追溯效率，帮助识别关键训练样本和误标数据。

Conclusion: 影响函数在深度学习中的数据归因具有巨大潜力，但仍面临计算效率和扩展性挑战，未来研究需聚焦优化算法和应用场景扩展。

Abstract: The goal of data attribution is to trace the model's predictions through the
learning algorithm and back to its training data. thereby identifying the most
influential training samples and understanding how the model's behavior leads
to particular predictions. Understanding how individual training examples
influence a model's predictions is fundamental for machine learning
interpretability, data debugging, and model accountability. Influence
functions, originating from robust statistics, offer an efficient, first-order
approximation to estimate the impact of marginally upweighting or removing a
data point on a model's learned parameters and its subsequent predictions,
without the need for expensive retraining. This paper comprehensively reviews
the data attribution capability of influence functions in deep learning. We
discuss their theoretical foundations, recent algorithmic advances for
efficient inverse-Hessian-vector product estimation, and evaluate their
effectiveness for data attribution and mislabel detection. Finally,
highlighting current challenges and promising directions for unleashing the
huge potential of influence functions in large-scale, real-world deep learning
scenarios.

</details>


### [185] [When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective](https://arxiv.org/abs/2508.07299)
*Lin-Han Jia,Si-Yu Han,Wen-Chao Hu,Jie-Jing Shao,Wen-Da Wei,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 本文扩展了神经符号学习的理论框架，提出了衡量预任务影响的三个关键因素，并开发了预测预任务效果的方法，提升了无监督学习任务选择的理论基础和实用性。


<details>
  <summary>Details</summary>
Motivation: 旨在结合神经符号学习与半/自监督学习的理论，提高预任务选择的科学性与效率，解决当前依赖经验的难题。

Method: 通过理论分析建立可靠与非可靠知识条件下的统一框架，提出衡量知识学习性、可靠性和完备性的指标，并开发预测预任务效果的方法。

Result: 验证了预测指标与实际性能的高相关性，确认了方法的有效性，推动预任务选择的理论化与自动化。

Conclusion: 该研究为无监督学习中的预任务选择提供了理论基础和评估工具，改善实践中的任务设计策略，具有重要的指导意义。

Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models
by enabling them to satisfy knowledge, while semi/self-supervised learning
(SSL) improves the target task performance by designing unsupervised pretext
tasks for unlabeled data to make models satisfy corresponding assumptions. We
extend the Nesy theory based on reliable knowledge to the scenario of
unreliable knowledge (i.e., assumptions), thereby unifying the theoretical
frameworks of SSL and Nesy. Through rigorous theoretical analysis, we
demonstrate that, in theory, the impact of pretext tasks on target performance
hinges on three factors: knowledge learnability with respect to the model,
knowledge reliability with respect to the data, and knowledge completeness with
respect to the target. We further propose schemes to operationalize these
theoretical metrics, and thereby develop a method that can predict the
effectiveness of pretext tasks in advance. This will change the current status
quo in practical applications, where the selections of unsupervised tasks are
heuristic-based rather than theory-based, and it is difficult to evaluate the
rationality of unsupervised pretext task selection before testing the model on
the target task. In experiments, we verify a high correlation between the
predicted performance-estimated using minimal data-and the actual performance
achieved after large-scale semi-supervised or self-supervised learning, thus
confirming the validity of the theory and the effectiveness of the evaluation
method.

</details>


### [186] [Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative](https://arxiv.org/abs/2508.07329)
*Tuo Zhang,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.LG

TL;DR: 提出一种基于Hessian感知量化和CPU-GPU协作推理的边缘部署方案，有效优化大规模语言模型在资源受限设备上的性能。


<details>
  <summary>Details</summary>
Motivation: 如何在资源受限的边缘设备上高效部署大语言模型，解决激活值偏离分布导致的量化误差和内存限制带来的推理瓶颈。

Method: 引入平滑Hessian矩阵量化实现激活和权重的8比特联合量化，并设计专家级协作卸载与推理机制优化CPU-GPU协作。

Result: 在OPT系列和Mixtral模型等主流大模型上，降低GPU内存使用60%左右，推理延迟明显提升，精度接近全精模型。

Conclusion: 通过创新的量化和调度策略，有效提升大模型在边缘设备上的部署效率，未来可望推广应用。

Abstract: With the breakthrough progress of large language models (LLMs) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through sparse
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.

</details>


### [187] [Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants](https://arxiv.org/abs/2508.07333)
*Yuhao Liu,Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 本论文研究随机插值的有限时间收敛性，提供了两种常用数值积分方法的误差界和复杂度分析，验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 旨在解决随机插值在生成模型中的应用，特别是数值算法在有限时间内的收敛保证问题。

Method: 通过分析随机插值导出ODE的数值解，建立一阶和二阶积分方法的误差界，并优化构造方案以提高效率。

Result: 获得了误差界和复杂度的理论保证，数值实验验证了结果的有效性。

Conclusion: 本研究为随机插值数值算法提供了理论基础，增强其在生成模型中的应用潜力。

Abstract: Stochastic interpolants offer a robust framework for continuously
transforming samples between arbitrary data distributions, holding significant
promise for generative modeling. Despite their potential, rigorous finite-time
convergence guarantees for practical numerical schemes remain largely
unexplored. In this work, we address the finite-time convergence analysis of
numerical implementations for ordinary differential equations (ODEs) derived
from stochastic interpolants. Specifically, we establish novel finite-time
error bounds in total variation distance for two widely used numerical
integrators: the first-order forward Euler method and the second-order Heun's
method. Furthermore, our analysis on the iteration complexity of specific
stochastic interpolant constructions provides optimized schedules to enhance
computational efficiency. Our theoretical findings are corroborated by
numerical experiments, which validate the derived error bounds and complexity
analyses.

</details>


### [188] [ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis](https://arxiv.org/abs/2508.07345)
*Samiha Afaf Neha,Abir Ahammed Bhuiyan,Md. Ishrak Khan*

Main category: cs.LG

TL;DR: 提出ProteoKnight图像编码方法，用于病毒外壳蛋白的分类，结合深度学习模型提升预测性能，并通过Monte Carlo Dropout进行预测不确定性分析。


<details>
  <summary>Details</summary>
Motivation: 需要高效、准确的工具对病毒外壳蛋白进行注释，以支持基因组研究，现有编码方法存在空间信息限制。

Method: 改进DNA-Walk算法，将像素颜色和步长调整引入图像编码，结合预训练对抗神经网络进行分类，并利用Monte Carlo Dropout评估预测不确定性。

Result: 在二分类中达成90.8%的准确率，优于传统方法；多分类精度仍需提升。通过不确定性分析揭示不同蛋白类型和序列长度影响预测信心。

Conclusion: 新编码方式优于FCGR，有效保留空间信息，实现准确稳健的分类，同时识别低置信度预测。

Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is
essential for genomic studies due to their crucial role as structural elements
in bacteriophages. Computational tools, particularly machine learning, have
emerged for annotating phage protein sequences from high-throughput sequencing.
However, effective annotation requires specialized sequence encodings. Our
paper introduces ProteoKnight, a new image-based encoding method that addresses
spatial constraints in existing techniques, yielding competitive performance in
PVP classification using pre-trained convolutional neural networks.
Additionally, our study evaluates prediction uncertainty in binary PVP
classification through Monte Carlo Dropout (MCD). \textbf{Methods:}
ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,
incorporating pixel colors and adjusting walk distances to capture intricate
protein features. Encoded sequences were classified using multiple pre-trained
CNNs. Variance and entropy measures assessed prediction uncertainty across
proteins of various classes and lengths. \textbf{Results:} Our experiments
achieved 90.8% accuracy in binary classification, comparable to
state-of-the-art methods. Multi-class classification accuracy remains
suboptimal. Our uncertainty analysis unveils variability in prediction
confidence influenced by protein class and sequence length.
\textbf{Conclusions:} Our study surpasses frequency chaos game representation
(FCGR) by introducing novel image encoding that mitigates spatial information
loss limitations. Our classification technique yields accurate and robust PVP
predictions while identifying low-confidence predictions.

</details>


### [189] [Intrinsic training dynamics of deep neural networks](https://arxiv.org/abs/2508.07370)
*Sibylle Marcotte,Gabriel Peyré,Rémi Gribonval*

Main category: cs.LG

TL;DR: 研究深度学习中梯度流的低维内在动力学，提供了分析高维参数空间中梯度优化的简化条件，特别适用线性和ReLU网络。


<details>
  <summary>Details</summary>
Motivation: 探索高维参数空间中的梯度优化是否可以由低维结构描述，从而理解深度学习中的隐式偏差。

Method: 引入内在动力学概念，研究与结构相关的函数$	heta$到$z=phi(	heta)$的梯度流关系，通过核的包含关系确定条件，并应用于ReLU与线性网络。

Result: 提出必要条件判断内在动力学的存在，证明ReLU网络下存在低维重写的可能性，对线性网络的平衡初始化进行推广，明确无穷深线性网络的内在动力学表达。

Conclusion: 深度学习中的梯度流可在特定条件下在低维空间内描述，增强了对高维优化过程的理解，尤其在线性及部分非线性网络中具有应用价值。

Abstract: A fundamental challenge in the theory of deep learning is to understand
whether gradient-based training in high-dimensional parameter spaces can be
captured by simpler, lower-dimensional structures, leading to so-called
implicit bias. As a stepping stone, we study when a gradient flow on a
high-dimensional variable $\theta$ implies an intrinsic gradient flow on a
lower-dimensional variable $z = \phi(\theta)$, for an architecture-related
function $\phi$. We express a so-called intrinsic dynamic property and show how
it is related to the study of conservation laws associated with the
factorization $\phi$. This leads to a simple criterion based on the inclusion
of kernels of linear maps which yields a necessary condition for this property
to hold. We then apply our theory to general ReLU networks of arbitrary depth
and show that, for any initialization, it is possible to rewrite the flow as an
intrinsic dynamic in a lower dimension that depends only on $z$ and the
initialization, when $\phi$ is the so-called path-lifting. In the case of
linear networks with $\phi$ the product of weight matrices, so-called balanced
initializations are also known to enable such a dimensionality reduction; we
generalize this result to a broader class of {\em relaxed balanced}
initializations, showing that, in certain configurations, these are the
\emph{only} initializations that ensure the intrinsic dynamic property.
Finally, for the linear neural ODE associated with the limit of infinitely deep
linear networks, with relaxed balanced initialization, we explicitly express
the corresponding intrinsic dynamics.

</details>


### [190] [Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems](https://arxiv.org/abs/2508.07392)
*Nikita Puchkin,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: 本文提出一种基于Schrödinger桥和随机最优控制理论的生成模型与图像变换方法，通过样本学习实现目标分布的逼近，并提供了理论和数值验证。


<details>
  <summary>Details</summary>
Motivation: 解决在仅有独立同分布样本的情况下，进行生成建模和无配对图像变换的挑战。

Method: 采用随机最优控制框架，以奥恩斯坦-乌伦贝克过程作为参考模型，估计Schrödinger势，利用风险函数推导泛化界限，结合数值实验验证效果。

Result: 在理论上实现了接近最优的收敛速度，在数值实验中验证了方法的有效性。

Conclusion: 该方法在缺乏配对数据场景下，对生成模型和图像变换具有潜在优势，理论与实验均表现良好。

Abstract: Modern methods of generative modelling and unpaired image-to-image
translation based on Schr\"odinger bridges and stochastic optimal control
theory aim to transform an initial density to a target one in an optimal way.
In the present paper, we assume that we only have access to i.i.d. samples from
initial and final distributions. This makes our setup suitable for both
generative modelling and unpaired image-to-image translation. Relying on the
stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as
the reference one and estimate the corresponding Schr\"odinger potential.
Introducing a risk function as the Kullback-Leibler divergence between
couplings, we derive tight bounds on generalization ability of an empirical
risk minimizer in a class of Schr\"odinger potentials including Gaussian
mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we
almost achieve fast rates of convergence up to some logarithmic factors in
favourable scenarios. We also illustrate performance of the suggested approach
with numerical experiments.

</details>


### [191] [Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs](https://arxiv.org/abs/2508.07395)
*Behnoush Khavari,Mehran Shakerinava,Jayesh Khullar,Jerry Huang,François Rivest,Siamak Ravanbakhsh,Sarath Chandar*

Main category: cs.LG

TL;DR: 结合多层SSM尝试解决状态追踪任务，但结果显示仍然失败，需要输入相关且包含负特征的层。


<details>
  <summary>Details</summary>
Motivation: 研究现有的线性状态空间模型（SSM）在状态追踪能力方面的限制，尤其是关于输入依赖性和特征范围的不足。

Method: 分析不同类型的SSM（包括对角矩阵和多层组合）在解决状态追踪任务中的性能，尤其是对等阶奇偶性（如奇偶性检测）任务的实验验证。

Result: 即使结合了不同特性的多层SSM，如对角矩阵和非负矩阵，也未能解决奇偶性任务，暗示必须采用输入依赖且具有负特征的模型结构。

Conclusion: 解决状态追踪任务的关键在于模型必须同时具备输入依赖性和负特征，单一或简单的结构不足以应对复杂任务。

Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack
state-tracking capability due to either time-invariant transition matrices or
restricted eigenvalue ranges. To address this, input-dependent transition
matrices, particularly those that are complex or non-triangular, have been
proposed to enhance SSM performance on such tasks. While existing theorems
demonstrate that both input-independent and non-negative SSMs are incapable of
solving simple state-tracking tasks, such as parity, regardless of depth, they
do not explore whether combining these two types in a multilayer SSM could
help. We investigate this question for efficient SSMs with diagonal transition
matrices and show that such combinations still fail to solve parity. This
implies that a recurrence layer must both be input-dependent and include
negative eigenvalues. Our experiments support this conclusion by analyzing an
SSM model that combines S4D and Mamba layers.

</details>


### [192] [Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors](https://arxiv.org/abs/2508.07400)
*Mohamad Louai Shehab,Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: 提出了两种基于先验信息的时间变化奖励函数识别算法，解决了逆向强化学习中的难题。


<details>
  <summary>Details</summary>
Motivation: 逆向强化学习中，识别时间变化奖励函数困难且缺乏有效方法。

Method: 利用稀疏化和秩最小化的凸松弛，设计优化算法实现奖励识别。

Result: 算法成功恢复奖励，取得良好准确性和泛化能力。

Conclusion: 结合先验信息，提出高效算法解决时间变化奖励函数识别问题，具有实际应用潜力。

Abstract: In this paper, we consider the problem of recovering time-varying reward
functions from either optimal policies or demonstrations coming from a max
entropy reinforcement learning problem. This problem is highly ill-posed
without additional assumptions on the underlying rewards. However, in many
applications, the rewards are indeed parsimonious, and some prior information
is available. We consider two such priors on the rewards: 1) rewards are mostly
constant and they change infrequently, 2) rewards can be represented by a
linear combination of a small number of feature functions. We first show that
the reward identification problem with the former prior can be recast as a
sparsification problem subject to linear constraints. Moreover, we give a
polynomial-time algorithm that solves this sparsification problem exactly.
Then, we show that identifying rewards representable with the minimum number of
features can be recast as a rank minimization problem subject to linear
constraints, for which convex relaxations of rank can be invoked. In both
cases, these observations lead to efficient optimization-based reward
identification algorithms. Several examples are given to demonstrate the
accuracy of the recovered rewards as well as their generalizability.

</details>


### [193] [Lightning Prediction under Uncertainty: DeepLight with Hazy Loss](https://arxiv.org/abs/2508.07428)
*Md Sultanul Arifin,Abu Nowshed Sakib,Yeasir Rayhan,Tanzima Hashem*

Main category: cs.LG

TL;DR: DeepLight是一种利用多源气象数据和双编码器架构的深度学习模型，有效预测雷电发生，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 雷电预警的重要性及现有模型的局限性，包括难以捕捉空间动态、利用不充分和对计算资源的依赖。

Method: 采用多源气象数据、多分支卷积技术和新颖的Hazy Loss损失函数，设计双编码器架构提升预测能力。

Result: 在实验中，DeepLight使ETS评分提升18%-30%，表现优异。

Conclusion: DeepLight提供了一个先进且高效的雷电预测解决方案，具有实际应用潜力。

Abstract: Lightning, a common feature of severe meteorological conditions, poses
significant risks, from direct human injuries to substantial economic losses.
These risks are further exacerbated by climate change. Early and accurate
prediction of lightning would enable preventive measures to safeguard people,
protect property, and minimize economic losses. In this paper, we present
DeepLight, a novel deep learning architecture for predicting lightning
occurrences. Existing prediction models face several critical limitations: they
often struggle to capture the dynamic spatial context and inherent uncertainty
of lightning events, underutilize key observational data, such as radar
reflectivity and cloud properties, and rely heavily on Numerical Weather
Prediction (NWP) systems, which are both computationally expensive and highly
sensitive to parameter settings. To overcome these challenges, DeepLight
leverages multi-source meteorological data, including radar reflectivity, cloud
properties, and historical lightning occurrences through a dual-encoder
architecture. By employing multi-branch convolution techniques, it dynamically
captures spatial correlations across varying extents. Furthermore, its novel
Hazy Loss function explicitly addresses the spatio-temporal uncertainty of
lightning by penalizing deviations based on proximity to true events, enabling
the model to better learn patterns amidst randomness. Extensive experiments
show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over
state-of-the-art methods, establishing it as a robust solution for lightning
prediction.

</details>


### [194] [Unsupervised operator learning approach for dissipative equations via Onsager principle](https://arxiv.org/abs/2508.07440)
*Zhipeng Chang,Zhenye Wen,Xiaofei Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于Onsager变分原理的无监督深层算子学习方法，有效解决耗散方程的数值模拟问题。


<details>
  <summary>Details</summary>
Motivation: 克服现有算子学习方法依赖高计算成本和带标签数据的限制，寻求更高效且无需标注的解题策略。

Method: 利用Onsager变分原理，直接最小化Rayleighian泛函，通过空间递推与时间外插实现损耗方程的求解，无需标注数据。

Result: 在典型耗散方程上的数值实验验证了方法的有效性，与有监督方法相比性能优越，并扩展至非直接依赖于OGP的二阶波动模型。

Conclusion: 提出的DOOL算法结合变分原理与空间-时间解耦策略，显著提升耗散方程的无监督求解效率，具有广泛应用潜力。

Abstract: Existing operator learning methods rely on supervised training with
high-fidelity simulation data, introducing significant computational cost. In
this work, we propose the deep Onsager operator learning (DOOL) method, a novel
unsupervised framework for solving dissipative equations. Rooted in the Onsager
variational principle (OVP), DOOL trains a deep operator network by directly
minimizing the OVP-defined Rayleighian functional, requiring no labeled data,
and then proceeds in time explicitly through conservation/change laws for the
solution. Another key innovation here lies in the spatiotemporal decoupling
strategy: the operator's trunk network processes spatial coordinates
exclusively, thereby enhancing training efficiency, while integrated external
time stepping enables temporal extrapolation. Numerical experiments on typical
dissipative equations validate the effectiveness of the DOOL method, and
systematic comparisons with supervised DeepONet and MIONet demonstrate its
enhanced performance. Extensions are made to cover the second-order wave models
with dissipation that do not directly follow OVP.

</details>


### [195] [Stackelberg Coupling of Online Representation Learning and Reinforcement Learning](https://arxiv.org/abs/2508.07452)
*Fernando Martinez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 提出一种基于Stackelberg博弈的端到端深度强化学习框架SCORER，有效提升样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决在稀疏奖励情况下，特征学习和策略学习效果不佳的问题，避免复杂辅助目标和解耦带来的设计复杂性。

Method: 引入Stackelberg博弈模型，构建感知网络（领导者）和控制网络（追随者）之间的动态交互，采用两时间尺度算法逼近博弈均衡。

Result: 在标准DQN变体和基准任务中，显著提升样本效率和最终性能。

Conclusion: 通过理论设计感知与控制的互动机制，避免复杂架构，实现性能提升，验证了指导性的算法设计优势。

Abstract: Integrated, end-to-end learning of representations and policies remains a
cornerstone of deep reinforcement learning (RL). However, to address the
challenge of learning effective features from a sparse reward signal, recent
trends have shifted towards adding complex auxiliary objectives or fully
decoupling the two processes, often at the cost of increased design complexity.
This work proposes an alternative to both decoupling and naive end-to-end
learning, arguing that performance can be significantly improved by structuring
the interaction between distinct perception and control networks with a
principled, game-theoretic dynamic. We formalize this dynamic by introducing
the Stackelberg Coupled Representation and Reinforcement Learning (SCORER)
framework, which models the interaction between perception and control as a
Stackelberg game. The perception network (leader) strategically learns features
to benefit the control network (follower), whose own objective is to minimize
its Bellman error. We approximate the game's equilibrium with a practical
two-timescale algorithm. Applied to standard DQN variants on benchmark tasks,
SCORER improves sample efficiency and final performance. Our results show that
performance gains can be achieved through principled algorithmic design of the
perception-control dynamic, without requiring complex auxiliary objectives or
architectures.

</details>


### [196] [Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten](https://arxiv.org/abs/2508.07458)
*Wei Qian,Chenxu Zhao,Yangyi Li,Wenqian Ye,Mengdi Huai*

Main category: cs.LG

TL;DR: 提出了一种针对深度学习模型预测不确定性的新型恶意遗忘攻击方法，能有效操控预测不确定性结果，现有防御措施难以应对。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型应用的广泛，预测不确定性的重要性日益凸显。同时，机器遗忘（unlearning）技术用于从模型中删除敏感数据，但其抗恶意攻击的能力尚未被充分研究。本论文旨在探讨和填补这一空白，确保模型在保护隐私的同时保持安全性。

Method: 设计了新颖的优化框架，实现针对预测不确定性进行的恶意遗忘攻击，并在黑盒场景下进行了详细的实验验证。

Result: 实验结果显示，该攻击方法比传统的标签误分类攻击更有效，现有的防御措施无法抵抗该类攻击，证明其威胁性和攻击的有效性。

Conclusion: 该研究首次揭示了预测不确定性在机器遗忘中的潜在脆弱性，强调了开发针对这类攻击的防御措施的必要性，为深度学习模型的安全性提供了新的研究方向。

Abstract: Currently, various uncertainty quantification methods have been proposed to
provide certainty and probability estimates for deep learning models' label
predictions. Meanwhile, with the growing demand for the right to be forgotten,
machine unlearning has been extensively studied as a means to remove the impact
of requested sensitive data from a pre-trained model without retraining the
model from scratch. However, the vulnerabilities of such generated predictive
uncertainties with regard to dedicated malicious unlearning attacks remain
unexplored. To bridge this gap, for the first time, we propose a new class of
malicious unlearning attacks against predictive uncertainties, where the
adversary aims to cause the desired manipulations of specific predictive
uncertainty results. We also design novel optimization frameworks for our
attacks and conduct extensive experiments, including black-box scenarios.
Notably, our extensive experiments show that our attacks are more effective in
manipulating predictive uncertainties than traditional attacks that focus on
label misclassifications, and existing defenses against conventional attacks
are ineffective against our attacks.

</details>


### [197] [MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification](https://arxiv.org/abs/2508.07465)
*Tiantian Yang,Zhiqian Chen*

Main category: cs.LG

TL;DR: 提出一种结合图神经网络的多组学数据整合模型MOTGNN，用于二分类疾病预测，显著优于现有方法，具有良好的解释性和应对类别不平衡的能力。


<details>
  <summary>Details</summary>
Motivation: 多组学数据提供疾病机制的全面视角，但其高维和复杂交互带来建模挑战。

Method: 采用XGBoost进行omics特异的图构建，利用模态特异的GNN进行层次表示，再通过深度全连接网络实现跨组学整合。

Result: 在三个真实数据集上，MOTGNN在准确率、ROC-AUC和F1-score方面比最优基线提升5-10%，对不平衡数据表现稳健，且模型高效且具有可解释性。

Conclusion: MOTGNN在提升多组学疾病预测的准确性和可解释性方面具有潜力，有助于推进疾病机制研究与临床应用。

Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and
microRNA (miRNA) expression, offers a comprehensive view of the biological
mechanisms underlying disease. However, the high dimensionality and complex
interactions among omics layers present major challenges for predictive
modeling. We propose Multi-Omics integration with Tree-generated Graph Neural
Network (MOTGNN), a novel and interpretable framework for binary disease
classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform
omics-specific supervised graph construction, followed by modality-specific
Graph Neural Networks (GNNs) for hierarchical representation learning, and a
deep feedforward network for cross-omics integration. On three real-world
disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in
accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance
(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains
computational efficiency through sparse graphs (2.1-2.8 edges per node) and
provides built-in interpretability, revealing both top-ranked biomarkers and
the relative contributions of each omics modality. These results highlight
MOTGNN's potential to improve both predictive accuracy and interpretability in
multi-omics disease modeling.

</details>


### [198] [Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications](https://arxiv.org/abs/2508.07473)
*Zijian Liu*

Main category: cs.LG

TL;DR: 本文研究了在重尾噪声条件下在线凸优化（OCO）中的经典算法表现，提出了最优的渐近遗憾界限，无需算法修改，并应用于非光滑非凸优化的收敛分析。


<details>
  <summary>Details</summary>
Motivation: 由于现有研究主要关注有限方差的随机梯度，缺乏对重尾梯度的分析，本文旨在弥补这一空白，推动在更复杂噪声条件下OCO的理论和应用发展。

Method: 分析了传统的在线梯度下降等算法在重尾梯度噪声条件下的性能表现，推导出在无需算法修改（如梯度裁剪）的情况下达到的最优遗憾界限。

Result: 推导出在有限p-阶矩条件下经典OCO算法的最优遗憾界限，适应全参数且不依赖p值，证明其在非光滑非凸优化中的应用效果，且扩展到平滑和乐观算法。

Conclusion: 即使在重尾噪声环境中，经典算法仍能实现最优表现，且无需特殊处理，这为复杂优化问题提供了强有力的理论支持和实践指南。

Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a
finite variance, many algorithms provably work and guarantee a sublinear
regret. However, limited results are known if the gradient estimate has a heavy
tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th
central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this
work examines different old algorithms for OCO (e.g., Online Gradient Descent)
in the more challenging heavy-tailed setting. Under the standard bounded domain
assumption, we establish new regrets for these classical methods without any
algorithmic modification. Remarkably, these regret bounds are fully optimal in
all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting
that OCO with heavy tails can be solved effectively without any extra operation
(e.g., gradient clipping). Our new results have several applications. A
particularly interesting one is the first provable convergence result for
nonsmooth nonconvex optimization under heavy-tailed noise without gradient
clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and
extend our ideas to optimistic algorithms to handle different cases
simultaneously.

</details>


### [199] [N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting](https://arxiv.org/abs/2508.07490)
*Ricardo Matos,Luis Roque,Vitor Cerqueira*

Main category: cs.LG

TL;DR: 提出一种扩展的深度学习时间序列预测模型N-BEATS-MOE，利用专家混合机制提升模型适应性和可解释性，在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 提升时间序列预测模型的适应性和可解释性，以应对异质性强的实际应用场景。

Method: 在已有的N-BEATS模型基础上引入Mixture-of-Experts层，通过门控网络实现动态加权和专家识别。

Result: 在12个基准数据集上与多种方法对比，显示出在异质时间序列任务中的显著性能改善。

Conclusion: N-BEATS-MOE通过专家机制增强了模型的适应性和解释能力，适用于复杂多样的时间序列预测任务。

Abstract: Deep learning approaches are increasingly relevant for time series
forecasting tasks. Methods such as N-BEATS, which is built on stacks of
multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on
benchmark datasets and competitions. N-BEATS is also more interpretable
relative to other deep learning approaches, as it decomposes forecasts into
different time series components, such as trend and seasonality. In this work,
we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts
(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a
gating network which allows the model to better adapt to the characteristics of
each time series. We also hypothesize that the gating mechanism provides
additional interpretability by identifying which expert is most relevant for
each series. We evaluate our method across 12 benchmark datasets against
several approaches, achieving consistent improvements on several datasets,
especially those composed of heterogeneous time series.

</details>


### [200] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: 提出了一种用于非凸去中心化最小最大优化的隐私保护算法DPMixSGD，结合STORM算法并确保隐私和收敛性。


<details>
  <summary>Details</summary>
Motivation: 在去中心化最小最大优化中引入隐私保护以防模型更新被攻击，同时保持优化性能。

Method: 基于STORM的算法，加入差分隐私机制，进行理论分析与实验验证。

Result: 证明噪声不会显著影响收敛，并在多任务中验证了算法的有效性。

Conclusion: DPMixSGD在确保隐私的同时，保持了非凸去中心化最小最大优化的性能，为相关应用提供新的解决方案。

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [201] [FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction](https://arxiv.org/abs/2508.07518)
*Sichen Zhao,Wei Shao,Jeffrey Chan,Ziqi Xu,Flora Salim*

Main category: cs.LG

TL;DR: 提出一种基于解耦表示学习的公平深度时空预测框架，重点在城市出行需求预测，兼顾公平性与预测准确性。


<details>
  <summary>Details</summary>
Motivation: 随着深度时空神经网络在城市计算中的应用，其预测偏差可能加剧社会不平等，亟需解决公平性问题。

Method: 利用对抗学习与解耦表示学习，分离敏感属性，实现在无监督条件下的公平预测。

Result: 在真实城市出行数据集上，显著缩小公平性差距，并保持竞争性预测性能。

Conclusion: 该框架有效实现了无监督公平预测，有助于AI在公共基础设施中的伦理部署。

Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban
computing contexts, the deployment of such methods can have a direct impact on
users of critical urban infrastructure, such as public transport, emergency
services, and traffic management systems. While many spatio-temporal methods
focus on improving accuracy, fairness has recently gained attention due to
growing evidence that biased predictions in spatio-temporal applications can
disproportionately disadvantage certain demographic or geographic groups,
thereby reinforcing existing socioeconomic inequalities and undermining the
ethical deployment of AI in public services. In this paper, we propose a novel
framework, FairDRL-ST, based on disentangled representation learning, to
address fairness concerns in spatio-temporal prediction, with a particular
focus on mobility demand forecasting. By leveraging adversarial learning and
disentangled representation learning, our framework learns to separate
attributes that contain sensitive information. Unlike existing methods that
enforce fairness through supervised learning, which may lead to
overcompensation and degraded performance, our framework achieves fairness in
an unsupervised manner with minimal performance loss. We apply our framework to
real-world urban mobility datasets and demonstrate its ability to close
fairness gaps while delivering competitive predictive performance compared to
state-of-the-art fairness-aware methods.

</details>


### [202] [Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning](https://arxiv.org/abs/2508.07536)
*Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.LG

TL;DR: 本文提出了一种融合物理知识的多模态卷积神经网络，用于轴承故障分类，结合物理信息的损失函数显著提升模型的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 保障旋转机械的可靠性，提升轴承故障诊断的准确性和解释性，特别是在变量工况下应对域偏移问题。

Method: 提出多模态CNN与物理信息融合，设计物理损失函数，采用转移学习策略增强模型在不同工况下的表现。

Result: 模型在多个数据集上均优于基线，准确率高达98%，显示出良好的跨数据集适应性和统计学显著性。

Conclusion: 将领域知识融入数据驱动模型，提升轴承故障诊断的准确性、解释性和鲁棒性，为工业应用提供有效解决方案。

Abstract: Accurate and interpretable bearing fault classification is critical for
ensuring the reliability of rotating machinery, particularly under variable
operating conditions where domain shifts can significantly degrade model
performance. This study proposes a physics-informed multimodal convolutional
neural network (CNN) with a late fusion architecture, integrating vibration and
motor current signals alongside a dedicated physics-based feature extraction
branch. The model incorporates a novel physics-informed loss function that
penalizes physically implausible predictions based on characteristic bearing
fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency
Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive
experiments on the Paderborn University dataset demonstrate that the proposed
physics-informed approach consistently outperforms a non-physics-informed
baseline, achieving higher accuracy, reduced false classifications, and
improved robustness across multiple data splits. To address performance
degradation under unseen operating conditions, three transfer learning (TL)
strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy
(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS
yields the best generalization, with additional performance gains when combined
with physics-informed modeling. Validation on the KAIST bearing dataset
confirms the framework's cross-dataset applicability, achieving up to 98
percent accuracy. Statistical hypothesis testing further verifies significant
improvements (p < 0.01) in classification performance. The proposed framework
demonstrates the potential of integrating domain knowledge with data-driven
learning to achieve robust, interpretable, and generalizable fault diagnosis
for real-world industrial applications.

</details>


### [203] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: 提出一种基于索引的双模态调度策略，有效降低远程多模态机器学习系统中的推断误差，具有广泛适用性和高计算效率。


<details>
  <summary>Details</summary>
Motivation: 在远程多模态机器学习系统中，传感器动态变化导致获取新鲜特征的网络资源有限，亟需优化调度策略以降低推断误差。

Method: 设计索引阈值策略，根据索引函数值决定切换模态，理论证明其最优性，效率高还能适应非单调、非加性和异质传输时间的AoI函数。

Result: 策略在数值模拟中比轮询和随机分配方法分别提升推断精度最大55%，验证了其优越性和实用性。

Conclusion: 该研究为远程多模态推断系统的任务导向AoI优化提供了策略基础，有助于提升系统整体推断性能。

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [204] [Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning](https://arxiv.org/abs/2508.07556)
*Stephan Rabanser*

Main category: cs.LG

TL;DR: 本论文研究了如何利用模型训练轨迹中的不确定性信号改善机器学习的预测安全性，通过集成中间检查点实现鲁棒的选择性预测方法，并分析了差分隐私对不确定性质量的影响。同时，论文提出了误差来源的分解和对抗性操控的防御策略，推动可靠性机器学习的发展。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在高风险领域的应用，对预测的可靠性和安全性提出了更高要求，亟需有效的不确定性估计方法以增强模型的可信度。

Method: 利用训练轨迹中的不确定性信号，通过集成中间检查点实现后验弃权，从而提供一种轻量级、跨任务的选择性预测方法；分析差分隐私对不确定性质量的影响；提出有限样本的误差分解模型以识别影响预测性能的关键错误源；研究不确定性信号的对抗操控及防御手段。

Result: 该方法在多任务中表现出优越的选择性预测效果，具有与差分隐私兼容的优势；误差分解揭示了影响模型性能的多重误差源并提出相应改进策略；对抗性实验表明，可以通过校准审查和可验证推理加强系统的安全性。

Conclusion: 该研究显著提升了不确定性估计在机器学习中的应用效果，增强了模型在高风险领域的安全性和可信性，为未来构建具有自我识别能力的可靠系统提供了理论和技术基础。

Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes
domains where reliability is paramount. This thesis investigates how
uncertainty estimation can enhance the safety and trustworthiness of ML,
focusing on selective prediction -- where models abstain when confidence is
low.
  We first show that a model's training trajectory contains rich uncertainty
signals that can be exploited without altering its architecture or loss. By
ensembling predictions from intermediate checkpoints, we propose a lightweight,
post-hoc abstention method that works across tasks, avoids the cost of deep
ensembles, and achieves state-of-the-art selective prediction performance.
Crucially, this approach is fully compatible with differential privacy (DP),
allowing us to study how privacy noise affects uncertainty quality. We find
that while many methods degrade under DP, our trajectory-based approach remains
robust, and we introduce a framework for isolating the privacy-uncertainty
trade-off. Next, we then develop a finite-sample decomposition of the selective
classification gap -- the deviation from the oracle accuracy-coverage curve --
identifying five interpretable error sources and clarifying which interventions
can close the gap. This explains why calibration alone cannot fix ranking
errors, motivating methods that improve uncertainty ordering. Finally, we show
that uncertainty signals can be adversarially manipulated to hide errors or
deny service while maintaining high accuracy, and we design defenses combining
calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating,
and safeguarding uncertainty estimation, enabling models that not only make
accurate predictions -- but also know when to say "I do not know".

</details>


### [205] [Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression](https://arxiv.org/abs/2508.07571)
*Xingwu Chen,Miao Lu,Beining Wu,Difan Zou*

Main category: cs.LG

TL;DR: 增加测试时的计算量和随机采样可以改善模型推理性能


<details>
  <summary>Details</summary>
Motivation: 弥合实际语言模型推理与理论分析之间的差距，理解推理行为

Method: 通过噪声注入和二元系数采样模拟语言模型的解码过程，分析推理技术

Result: 提供了详细的推理技术分析，并通过实验证明了理论框架的有效性

Conclusion: 该方法有助于深入理解实际语言模型中的推理行为，为未来研究提供新的思路

Abstract: Using more test-time computation during language model inference, such as
generating more intermediate thoughts or sampling multiple candidate answers,
has proven effective in significantly improving model performance. This paper
takes an initial step toward bridging the gap between practical language model
inference and theoretical transformer analysis by incorporating randomness and
sampling. We focus on in-context linear regression with continuous/binary
coefficients, where our framework simulates language model decoding through
noise injection and binary coefficient sampling. Through this framework, we
provide detailed analyses of widely adopted inference techniques. Supported by
empirical results, our theoretical framework and analysis demonstrate the
potential for offering new insights into understanding inference behaviors in
real-world language models.

</details>


### [206] [When and how can inexact generative models still sample from the data manifold?](https://arxiv.org/abs/2508.07581)
*Nisha Chandramoorthy,Adriaan de Clercq*

Main category: cs.LG

TL;DR: 本文探讨了生成模型中支持鲁棒性的现象，从动力学系统角度分析学习误差对模型生成的影响，并提出支持鲁棒性的动力机制。


<details>
  <summary>Details</summary>
Motivation: 理解为什么尽管存在学习误差，生成样本仍沿数据支持分布移动并未偏离。

Method: 通过扰动分析和动力学系统理论，研究生成模型中的支持鲁棒性，特别是李雅普诺夫向量的对齐机制。

Result: 证明了 infinitesimal 学习误差只影响数据流形上的预测密度，支持支持鲁棒性的动力机制是李雅普诺夫向量与数据边界切空间的对齐。

Conclusion: 提出了检测和实现支持鲁棒性的动力学条件，扩展了生成模型的理论保障范围。

Abstract: A curious phenomenon observed in some dynamical generative models is the
following: despite learning errors in the score function or the drift vector
field, the generated samples appear to shift \emph{along} the support of the
data distribution but not \emph{away} from it. In this work, we investigate
this phenomenon of \emph{robustness of the support} by taking a dynamical
systems approach on the generating stochastic/deterministic process. Our
perturbation analysis of the probability flow reveals that infinitesimal
learning errors cause the predicted density to be different from the target
density only on the data manifold for a wide class of generative models.
Further, what is the dynamical mechanism that leads to the robustness of the
support? We show that the alignment of the top Lyapunov vectors (most sensitive
infinitesimal perturbation directions) with the tangent spaces along the
boundary of the data manifold leads to robustness and prove a sufficient
condition on the dynamics of the generating process to achieve this alignment.
Moreover, the alignment condition is efficient to compute and, in practice, for
robust generative models, automatically leads to accurate estimates of the
tangent bundle of the data manifold. Using a finite-time linear perturbation
analysis on samples paths as well as probability flows, our work complements
and extends existing works on obtaining theoretical guarantees for generative
models from a stochastic analysis, statistical learning and uncertainty
quantification points of view. Our results apply across different dynamical
generative models, such as conditional flow-matching and score-based generative
models, and for different target distributions that may or may not satisfy the
manifold hypothesis.

</details>


### [207] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: Klear-Reasoner是具有长逻辑推理能力的模型，通过细致的训练和优化策略实现了优异表现，解决了训练细节不透明和探索激励不足的问题。


<details>
  <summary>Details</summary>
Motivation: 提升推理模型的性能与可再现性，特别是在复杂任务中的表现。

Method: 提出长链式思考微调与强化学习相结合的训练流程，并优化剪枝机制以增强探索能力。

Result: 在数学和编程任务中表现出色，显著优于现有模型，在多个测评中取得高分。

Conclusion: 详细分析训练流程，有效解决探索和训练细节披露不足的问题，推动推理模型的发展。

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [208] [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo](https://arxiv.org/abs/2508.07631)
*Advait Parulekar,Litu Rout,Karthikeyan Shanmugam,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 本文提出了一种在分数生成模型中进行后验采样的理论框架，显示在一定条件下可以在多项式时间内实现与真实后验分布接近的样本生成，突破了已有的计算复杂度限制。


<details>
  <summary>Details</summary>
Motivation: 尽管后验采样在理论上受计算复杂度的限制，但实际中在图像处理等任务中的应用非常成功，激发了研究者探索其可行性。

Method: 作者提出将后验采样问题视为一种偏向测量的“倾斜”问题，并在最小假设下证明可以在KL和Fisher散度条件下高效采样，确保样本既符合测量也符合先验。

Result: 理论上证明了在多项式时间内可以获得既接近后验分布又符合实际测量的近似样本，此为首个有力的形式化理论结果。

Conclusion: 该方法为后验采样提供了理论基础，为在复杂模型中实现高效近似采样奠定了基础，有望促进相关应用的发展。

Abstract: We study the problem of posterior sampling in the context of score based
generative models. We have a trained score network for a prior $p(x)$, a
measurement model $p(y|x)$, and are tasked with sampling from the posterior
$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)
under well-accepted computational hardness assumptions. Despite this, popular
algorithms for tasks such as image super-resolution, stylization, and
reconstruction enjoy empirical success. Rather than establishing distributional
assumptions or restricted settings under which exact posterior sampling is
tractable, we view this as a more general "tilting" problem of biasing a
distribution towards a measurement. Under minimal assumptions, we show that one
can tractably sample from a distribution that is simultaneously close to the
posterior of a noised prior in KL divergence and the true posterior in Fisher
divergence. Intuitively, this combination ensures that the resulting sample is
consistent with both the measurement and the prior. To the best of our
knowledge these are the first formal results for (approximate) posterior
sampling in polynomial time.

</details>


### [209] [Attribution Explanations for Deep Neural Networks: A Theoretical Perspective](https://arxiv.org/abs/2508.07636)
*Huiqi Deng,Hongbin Pei,Quanshi Zhang,Mengnan Du*

Main category: cs.LG

TL;DR: 本文分析了神经网络归因方法的信实性问题，强调理论统一、基础和评估的重要性，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决归因方法是否真实反映输入变量贡献的关键挑战，以增强解释的信赖度。

Method: 总结领域内关于理论统一、基础和评估的最新理论研究进展。

Result: 提供了对归因方法的理论基础的深刻理解，为方法选择和新方法的启发提供了指导。

Conclusion: 未来应关注开放问题，进一步提升归因解释的科学性和实用性。

Abstract: Attribution explanation is a typical approach for explaining deep neural
networks (DNNs), inferring an importance or contribution score for each input
variable to the final output. In recent years, numerous attribution methods
have been developed to explain DNNs. However, a persistent concern remains
unresolved, i.e., whether and which attribution methods faithfully reflect the
actual contribution of input variables to the decision-making process. The
faithfulness issue undermines the reliability and practical utility of
attribution explanations. We argue that these concerns stem from three core
challenges. First, difficulties arise in comparing attribution methods due to
their unstructured heterogeneity, differences in heuristics, formulations, and
implementations that lack a unified organization. Second, most methods lack
solid theoretical underpinnings, with their rationales remaining absent,
ambiguous, or unverified. Third, empirically evaluating faithfulness is
challenging without ground truth. Recent theoretical advances provide a
promising way to tackle these challenges, attracting increasing attention. We
summarize these developments, with emphasis on three key directions: (i)
Theoretical unification, which uncovers commonalities and differences among
methods, enabling systematic comparisons; (ii) Theoretical rationale,
clarifying the foundations of existing methods; (iii) Theoretical evaluation,
rigorously proving whether methods satisfy faithfulness principles. Beyond a
comprehensive review, we provide insights into how these studies help deepen
theoretical understanding, inform method selection, and inspire new attribution
methods. We conclude with a discussion of promising open problems for further
work.

</details>


### [210] [Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs](https://arxiv.org/abs/2508.07637)
*Guanqun Ma,David Lenz,Hanqi Guo,Tom Peterka,Bei Wang*

Main category: cs.LG

TL;DR: 首次提出一种方法，从多变量函数逼近模型中直接提取复杂的拓扑特征，如等高线、雅可比集和脊谷图，无需转换为离散表示。


<details>
  <summary>Details</summary>
Motivation: 随着隐式连续模型在科学数据存储、传输和分析中的应用增加，迫切需要有效的工具提取其拓扑特征以增强分析能力。

Method: 开发了基于多变量函数逼近(MFA)的框架，能从模型直接提取拓扑特征，支持高阶导数查询，避免离散化。

Result: 实现了在连续隐式模型上进行拓扑数据分析和可视化的基础，为后续研究提供了新工具。

Conclusion: 该方法实现了从连续隐式模型中直接提取复杂拓扑特征，为隐式模型的拓扑分析奠定基础，具有广泛的适用潜力。

Abstract: Implicit continuous models, such as functional models and implicit neural
networks, are an increasingly popular method for replacing discrete data
representations with continuous, high-order, and differentiable surrogates.
These models offer new perspectives on the storage, transfer, and analysis of
scientific data. In this paper, we introduce the first framework to directly
extract complex topological features -- contours, Jacobi sets, and ridge-valley
graphs -- from a type of continuous implicit model known as multivariate
functional approximation (MFA). MFA replaces discrete data with continuous
piecewise smooth functions. Given an MFA model as the input, our approach
enables direct extraction of complex topological features from the model,
without reverting to a discrete representation of the model. Our work is easily
generalizable to any continuous implicit model that supports the queries of
function values and high-order derivatives. Our work establishes the building
blocks for performing topological data analysis and visualization on implicit
continuous models.

</details>


### [211] [Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals](https://arxiv.org/abs/2508.07638)
*Jia Zhang,Yao Liu,Chen-Xi Zhang,Yi Liu,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于偏好偏差的多偏好优化方法，用于提高大语言模型（LLMs）在复杂价值观下的对齐效率，通过数据选择策略解决偏好冲突问题，显著提升了训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法难以处理多价值观冲突，亟需有效利用细粒度偏好数据以改善模型对人类价值的符合程度。

Method: 引入偏好偏差（PD）指标，通过数据筛选策略优化多偏好对齐过程，结合理论分析和实际估计技术实现。

Result: 在UltraFeedback数据集上，该策略相较传统方法提升超过10%，提高训练效率，减少偏好标签的昂贵获取成本。

Conclusion: 利用偏好偏差作为数据选择依据，能有效缓解偏好冲突，推动更稳健的大模型对齐机制，展现出实际应用潜力。

Abstract: Aligning Large Language Models (LLMs) with diverse human values requires
moving beyond a single holistic "better-than" preference criterion. While
collecting fine-grained, aspect-specific preference data is more reliable and
scalable, existing methods like Direct Preference Optimization (DPO) struggle
with the severe noise and conflicts inherent in such aggregated datasets. In
this paper, we tackle this challenge from a data-centric perspective. We first
derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a
key Preference Divergence (PD) term that quantifies inter-aspect preference
conflicts. Instead of using this term for direct optimization, we leverage it
to formulate a novel, theoretically-grounded data selection principle. Our
principle advocates for selecting a subset of high-consensus data-identified by
the most negative PD values-for efficient DPO training. We prove the optimality
of this strategy by analyzing the loss bounds of the DMPO objective in the
selection problem. To operationalize our approach, we introduce practical
methods of PD term estimation and length bias mitigation, thereby proposing our
PD selection method. Evaluation on the UltraFeedback dataset with three varying
conflict levels shows that our simple yet effective strategy achieves over 10%
relative improvement against both the standard holistic preference and a
stronger oracle using aggregated preference signals, all while boosting
training efficiency and obviating the need for intractable holistic preference
annotating, unlocking the potential of robust LLM alignment via fine-grained
preference signals.

</details>


### [212] [Multi-Turn Jailbreaks Are Simpler Than They Seem](https://arxiv.org/abs/2508.07646)
*Xiaoxue Yang,Jaeha Lee,Anna-Katharina Dick,Jasper Timm,Fei Xie,Diogo Cruz*

Main category: cs.LG

TL;DR: 多轮越狱攻击在抗单轮攻击模型上依然有效，攻击方法与多次单轮攻击类似，成功率高且与模型相似度有关。


<details>
  <summary>Details</summary>
Motivation: 验证和分析多轮越狱攻击的实际效果及其对模型安全的影响。

Method: 对GPT-4、Claude和Gemini等模型进行实证分析，采用StrongREJECT基准测试自动多轮越狱攻击。

Result: 多轮攻击的成功率类似于反复采样单轮攻击，模型间的攻击成功存在相关性，推理模型的推理努力与攻击成功成正比。

Conclusion: 多轮越狱未显著优于单轮攻击，提升模型安全需考虑模型间的相似性和推理复杂度。

Abstract: While defenses against single-turn jailbreak attacks on Large Language Models
(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent
vulnerability, often achieving success rates exceeding 70% against models
optimized for single-turn protection. This work presents an empirical analysis
of automated multi-turn jailbreak attacks across state-of-the-art models
including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.
Our findings challenge the perceived sophistication of multi-turn attacks: when
accounting for the attacker's ability to learn from how models refuse harmful
requests, multi-turn jailbreaking approaches are approximately equivalent to
simply resampling single-turn attacks multiple times. Moreover, attack success
is correlated among similar models, making it easier to jailbreak newly
released ones. Additionally, for reasoning models, we find surprisingly that
higher reasoning effort often leads to higher attack success rates. Our results
have important implications for AI safety evaluation and the design of
jailbreak-resistant systems. We release the source code at
https://github.com/diogo-cruz/multi_turn_simpler

</details>


### [213] [Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning](https://arxiv.org/abs/2508.07659)
*Hyeon-Ju Jeon,Jeon-Ho Kang,In-Hyuk Kwon,O-Joun Lee*

Main category: cs.LG

TL;DR: 本文提出一种基于自适应边采样的时空图神经网络模型，用于捕捉气象数据中的动态空间相关性，以提升全球大气状态预估的准确性。


<details>
  <summary>Details</summary>
Motivation: 通过改进空间相关性建模方法，提高气象预测的精度，解决传统方法在动态空间关系建模上的局限。

Method: 采用具有结构学习能力的时空图神经网络（STGNN），并引入自适应节点度调控和空间距离考虑，以优化边的采样，避免信息损失和过度平滑。

Result: 在东亚实测数据中验证了该方法的有效性，尤其在高大气波动区域，展示出优于现有STGNN模型的性能。

Conclusion: 新颖的自适应边采样策略增强了模型对动态空间相关性的捕捉能力，推动大气状态预报技术的发展。

Abstract: This study aims to discover spatial correlations between Earth observations
and atmospheric states to improve the forecasting accuracy of global
atmospheric state estimation, which are usually conducted using conventional
numerical weather prediction (NWP) systems and is the beginning of weather
forecasting. NWP systems predict future atmospheric states at fixed locations,
which are called NWP grid points, by analyzing previous atmospheric states and
newly acquired Earth observations without fixed locations. Thus, surrounding
meteorological context and the changing locations of the observations make
spatial correlations between atmospheric states and observations over time. To
handle complicated spatial correlations, which change dynamically, we employ
spatiotemporal graph neural networks (STGNNs) with structure learning. However,
structure learning has an inherent limitation that this can cause structural
information loss and over-smoothing problem by generating excessive edges. To
solve this problem, we regulate edge sampling by adaptively determining node
degrees and considering the spatial distances between NWP grid points and
observations. We validated the effectiveness of the proposed method by using
real-world atmospheric state and observation data from East Asia. Even in areas
with high atmospheric variability, the proposed method outperformed existing
STGNN models with and without structure learning.

</details>


### [214] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: 提出了一种名为GLiClass的新型序列分类方法，结合了多种模型优点，兼顾效率和灵活性，特别适用于零样本和少样本场景。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模扩大和需求变化，传统分类方法在效率和适应性方面存在瓶颈，亟需新的解决方案。

Method: 结合GLiNER架构进行序列分类，采用优化的策略，包括适应多标签分类的PPO训练方法。

Result: 在效率和准确性方面优于传统方案，兼具灵活性，适应复杂场景及极端数据条件。

Conclusion: GLiClass在提升分类性能的同时增强了系统的适应能力，为大规模和动态场景下的分类提供了新途径。

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [215] [AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting](https://arxiv.org/abs/2508.07668)
*Hyobin Park,Jinwook Jung,Minseok Seo,Hyunsoo Choi,Deukjae Cho,Sekil Park,Dong-Geol Choi*

Main category: cs.LG

TL;DR: 提出了一种结合AIS时序数据与大语言模型的框架AIS-LLM，能同时完成船舶轨迹预测、异常检测和碰撞风险评估，显著优于现有单任务方法，促进智能海事交通管理。


<details>
  <summary>Details</summary>
Motivation: 随着海事交通增加和AIS系统的强制应用，复杂交通任务的整体性分析需求增加，但现有方法多针对单一任务，难以全面考虑复杂情境。

Method: 设计融合时间序列编码器、文本提示编码器、跨模态对齐模块及多任务解码器的架构，实现多任务端到端的海事交通分析。

Result: 实验验证AIS-LLM优于已有单任务方法，实现多任务平衡，提升整体性能，并能生成情境总结，增强管理效率。

Conclusion: AIS-LLM通过多模态融合，推动海事交通智能分析，有望改善现有分散分析模式，助力未来智能交通系统发展。

Abstract: With the increase in maritime traffic and the mandatory implementation of the
Automatic Identification System (AIS), the importance and diversity of maritime
traffic analysis tasks based on AIS data, such as vessel trajectory prediction,
anomaly detection, and collision risk assessment, is rapidly growing. However,
existing approaches tend to address these tasks individually, making it
difficult to holistically consider complex maritime situations. To address this
limitation, we propose a novel framework, AIS-LLM, which integrates time-series
AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series
Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a
Cross-Modality Alignment Module for semantic alignment between time-series data
and textual prompts, and an LLM-based Multi-Task Decoder. This architecture
enables the simultaneous execution of three key tasks: trajectory prediction,
anomaly detection, and risk assessment of vessel collisions within a single
end-to-end system. Experimental results demonstrate that AIS-LLM outperforms
existing methods across individual tasks, validating its effectiveness.
Furthermore, by integratively analyzing task outputs to generate situation
summaries and briefings, AIS-LLM presents the potential for more intelligent
and efficient maritime traffic management.

</details>


### [216] [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/abs/2508.07675)
*Xutong Liu,Baran Atalar,Xiangxiang Dai,Jinhang Zuo,Siwei Wang,John C. S. Lui,Wei Chen,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出一种基于学习的语义缓存策略，有效应对未知参数环境下的语义缓存置换问题，性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练语言模型带来的高推理成本及缓存置换优化问题，特别是在环境参数未知的情况下。

Method: 提出理论基础的离线和在线算法，结合学习与优化方法，处理query与成本分布未知的情境。

Result: 算法在合成数据集上表现出与或优于现有方法的性能。

Conclusion: 提供了一个完善的学习框架，用于语义缓存的置换策略，有助于提升LLMs的效率和可扩展性。

Abstract: Large Language Models (LLMs) are revolutionizing how users interact with
information systems, yet their high inference cost poses serious scalability
and sustainability challenges. Caching inference responses, allowing them to be
retrieved without another forward pass through the LLM, has emerged as one
possible solution. Traditional exact-match caching, however, overlooks the
semantic similarity between queries, leading to unnecessary recomputation.
Semantic caching addresses this by retrieving responses based on semantic
similarity, but introduces a fundamentally different cache eviction problem:
one must account for mismatch costs between incoming queries and cached
responses. Moreover, key system parameters, such as query arrival probabilities
and serving costs, are often unknown and must be learned over time. Existing
semantic caching methods are largely ad-hoc, lacking theoretical foundations
and unable to adapt to real-world uncertainty. In this paper, we present a
principled, learning-based framework for semantic cache eviction under unknown
query and cost distributions. We formulate both offline optimization and online
learning variants of the problem, and develop provably efficient algorithms
with state-of-the-art guarantees. We also evaluate our framework on a synthetic
dataset, showing that our proposed algorithms perform matching or superior
performance compared with baselines.

</details>


### [217] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: 提出一种社会感知的隐私保护联邦学习机制，利用多跳传播模型量化隐私泄露，通过Stackelberg博弈优化激励策略，实现提升用户隐私和模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决社交网络中联邦学习存在的隐私外部性问题，保障多方隐私安全同时促进合作。

Method: 构建多跳传播模型、Stackelberg博弈框架及均值场估算器，分析纳什均衡，优化激励机制。

Result: 机制显著改善客户端效用、降低服务器成本，保持模型性能，优于基线和考虑外部效应的方法。

Conclusion: 提出的机制有效应对社交网络中的隐私外部性，增强联邦学习的实用性和隐私保护能力。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [218] [MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation](https://arxiv.org/abs/2508.07681)
*Yooseok Lim,ByoungJun Jeon,Seong-A Park,Jisoo Lee,Sae Won Choi,Chang Wook Jeong,Ho-Geol Ryu,Hongyeol Lee,Hyun-Lim Yang*

Main category: cs.LG

TL;DR: 提出了一种结合大规模语言模型和多模态离线强化学习的败血症管理新框架MORE-CLEAR，有效提升患者状态表征和治疗策略。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法主要依赖结构化数据，缺乏对患者整体状况的全面理解，因此需要引入丰富的语义信息改善模型性能。

Method: 利用预训练的大规模语言模型从医嘱中提取语义，结合门控融合和跨模态注意机制动态调节不同模态数据的权重，实现多模态信息的融合。

Result: 在多个公开和私有数据集上的验证表明，MORE-CLEAR显著提高生存率估计和策略表现，优于单模态RL方法。

Conclusion: 首次将大规模语言模型引入多模态离线RL以提升医疗场景中的状态表达，有助于加快败血症的治疗与管理。

Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ
dysfunction, making early detection and optimal management critical. Previous
reinforcement learning (RL) approaches to sepsis management rely primarily on
structured data, such as lab results or vital signs, and on a dearth of a
comprehensive understanding of the patient's condition. In this work, we
propose a Multimodal Offline REinforcement learning for Clinical notes
Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis
control in intensive care units. MORE-CLEAR employs pre-trained large-scale
language models (LLMs) to facilitate the extraction of rich semantic
representations from clinical notes, preserving clinical context and improving
patient state representation. Gated fusion and cross-modal attention allow
dynamic weight adjustment in the context of time and the effective integration
of multimodal data. Extensive cross-validation using two public (MIMIC-III and
MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly
improves estimated survival rate and policy performance compared to
single-modal RL approaches. To our knowledge, this is the first to leverage LLM
capabilities within a multimodal offline RL for better state representation in
medical applications. This approach can potentially expedite the treatment and
management of sepsis by enabling reinforcement learning models to propose
enhanced actions based on a more comprehensive understanding of patient
conditions.

</details>


### [219] [Semantic-Enhanced Time-Series Forecasting via Large Language Models](https://arxiv.org/abs/2508.07697)
*Hao Liu,Chun Yang,Zhang xiaoxing,Xiaobin Zhu*

Main category: cs.LG

TL;DR: 提出一种语义增强的LLM（SE-LLM），通过融合时间序列的周期性和异常特征，增强模型对时间序列的理解能力，同时引入插件模块改善短长依赖建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 弥合语言知识结构与时间序列数据间的本质差距，提升时间序列预测的语义表达和模型性能。

Method: 融合时间序列的周期性和异常信息到语义空间，设计内嵌于自注意力机制的插件模块以同时建模长短期依赖，冻结LLM并降低序列维度。

Result: 在多个实验中优于最新的方法，验证了模型的有效性和优越性。

Conclusion: 通过引入语义信息和短长依赖建模插件，SE-LLM有效提升时间序列预测能力，拓展了LLM在序列分析中的应用潜力。

Abstract: Time series forecasting plays a significant role in finance, energy,
meteorology, and IoT applications. Recent studies have leveraged the
generalization capabilities of large language models (LLMs) to adapt to time
series forecasting, achieving promising performance. However, existing studies
focus on token-level modal alignment, instead of bridging the intrinsic
modality gap between linguistic knowledge structures and time series data
patterns, greatly limiting the semantic representation. To address this issue,
we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent
periodicity and anomalous characteristics of time series to embed into the
semantic space to enhance the token embedding. This process enhances the
interpretability of tokens for LLMs, thereby activating the potential of LLMs
for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel
at capturing long-range dependencies but are weak at modeling short-term
anomalies in time-series data. Hence, we propose a plugin module embedded
within self-attention that models long-term and short-term dependencies to
effectively adapt LLMs to time-series analysis. Our approach freezes the LLM
and reduces the sequence dimensionality of tokens, greatly reducing
computational consumption. Experiments demonstrate the superiority performance
of our SE-LLM against the state-of-the-art (SOTA) methods.

</details>


### [220] [Energy Consumption in Parallel Neural Network Training](https://arxiv.org/abs/2508.07706)
*Philipp Huber,David Li,Juan Pedro Gutiérrez Hermosillo Muriedas,Deifilia Kieckhefen,Markus Götz,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 扩展神经网络训练中的能耗与规模的关系，提供节能的训练策略建议。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练对计算资源需求增加，能耗问题日益突出，亟需研究其影响因素。

Method: 通过对ResNet50和FourCastNet的分布式训练进行扩展实验，分析GPU数量、批量大小等参数对性能、时间和能耗的影响。

Result: 能耗与GPU小时数呈线性关系，但不同模型和硬件的比例不同，受样本数和梯度更新次数影响显著。

Conclusion: 神经网络训练的能耗受多因素影响，为实现可持续人工智能发展提供理论依据。

Abstract: The increasing demand for computational resources of training neural networks
leads to a concerning growth in energy consumption. While parallelization has
enabled upscaling model and dataset sizes and accelerated training, its impact
on energy consumption is often overlooked. To close this research gap, we
conducted scaling experiments for data-parallel training of two models,
ResNet50 and FourCastNet, and evaluated the impact of parallelization
parameters, i.e., GPU count, global batch size, and local batch size, on
predictive performance, training time, and energy consumption. We show that
energy consumption scales approximately linearly with the consumed resources,
i.e., GPU hours; however, the respective scaling factor differs substantially
between distinct model trainings and hardware, and is systematically influenced
by the number of samples and gradient updates per GPU hour. Our results shed
light on the complex interplay of scaling up neural network training and can
inform future developments towards more sustainable AI research.

</details>


### [221] [Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer](https://arxiv.org/abs/2508.07710)
*Jingya Wang,Xin Deng,Wenjie Wei,Dehao Zhang,Shuai Wang,Qian Sun,Jieyuan Zhang,Hanwen Liu,Ning Xie,Malu Zhang*

Main category: cs.LG

TL;DR: 提出一种无训练、基于MBE神经元的高效ANN到SNN转换框架，有效处理Transformer中的非线性操作，兼容多任务和模型架构。


<details>
  <summary>Details</summary>
Motivation: 面对传统ANN到SNN转换方法在处理Transformer中的非线性操作和需要微调的问题，提出更高效、无训练的转换框架以实现更低延迟和更广泛应用。

Method: 引入多基指数衰减（MBE）神经元，采用指数衰减和多基编码策略，无需修改预训练模型参数，直接转换。

Result: 在CV、NLU、NLG等任务，以及ViT、RoBERTa、GPT-2等架构上实现接近无损的转换精度，显著降低延迟。

Conclusion: 该方法为高效、可扩展的Spiking Transformer部署提供了有力技术支持，推动其在实际场景中的应用。

Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a
promising approach for constructing energy-efficient Transformer architectures.
Compared to directly trained Spiking Transformers, ANN-to-SNN conversion
methods bypass the high training costs. However, existing methods still suffer
from notable limitations, failing to effectively handle nonlinear operations in
Transformer architectures and requiring additional fine-tuning processes for
pre-trained ANNs. To address these issues, we propose a high-performance and
training-free ANN-to-SNN conversion framework tailored for Transformer
architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)
neuron, which employs an exponential decay strategy and multi-basis encoding
method to efficiently approximate various nonlinear operations. It removes the
requirement for weight modifications in pre-trained ANNs. Extensive experiments
across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures
(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless
conversion accuracy with significantly lower latency. This provides a promising
pathway for the efficient and scalable deployment of Spiking Transformers in
real-world applications.

</details>


### [222] [Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information](https://arxiv.org/abs/2508.07713)
*Jinghan Yang,Jiayu Weng*

Main category: cs.LG

TL;DR: 利用互信息衡量样本对模型的重要性，有效筛除噪声样本，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易记忆噪声标签，数据质量影响模型效果，需有效筛选优质数据。

Method: 提出基于互信息的样本选择框架，通过计算样本对互信息的贡献筛除噪声样本。

Result: 在MNIST测试中，筛选高互信息样本提升分类准确率达15%，并具备鲁棒性。

Conclusion: 互信息方法有效提升模型抗噪声能力，筛选优质样本，改善性能。

Abstract: Deep neural networks can memorize corrupted labels, making data quality
critical for model performance, yet real-world datasets are frequently
compromised by both label noise and input noise. This paper proposes a mutual
information-based framework for data selection under hybrid noise scenarios
that quantifies statistical dependencies between inputs and labels. We compute
each sample's pointwise contribution to the overall mutual information and find
that lower contributions indicate noisy or mislabeled instances. Empirical
validation on MNIST with different synthetic noise settings demonstrates that
the method effectively filters low-quality samples. Under label corruption,
training on high-MI samples improves classification accuracy by up to 15\%
compared to random sampling. Furthermore, the method exhibits robustness to
benign input modifications, preserving semantically valid data while filtering
truly corrupted samples.

</details>


### [223] [Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations](https://arxiv.org/abs/2508.07722)
*Pietro Talli,Federico Mason,Federico Chiariotti,Andrea Zanella*

Main category: cs.LG

TL;DR: 提出一种在通信网络上训练RL的全面架构HR3L，无需梯度交换，提升效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决RL在存在通信延迟、丢包和容量限制的无线网络中训练困难的问题。

Method: 设计了Homomorphic Robust Remote Reinforcement Learning (HR3L)架构，包括编码解码单元，无需跨无线通道传输梯度信息。

Result: 在多种通信场景下，HR3L在采样效率和适应性方面优于现有方法，表现出更好的鲁棒性。

Conclusion: HR3L通过避免梯度交换降低通信开销，提升了RL在不理想无线环境中的训练效率和效果。

Abstract: In this work, we address the problem of training Reinforcement Learning (RL)
agents over communication networks. The RL paradigm requires the agent to
instantaneously perceive the state evolution to infer the effects of its
actions on the environment. This is impossible if the agent receives state
updates over lossy or delayed wireless systems and thus operates with partial
and intermittent information. In recent years, numerous frameworks have been
proposed to manage RL with imperfect feedback; however, they often offer
specific solutions with a substantial computational burden. To address these
limits, we propose a novel architecture, named Homomorphic Robust Remote
Reinforcement Learning (HR3L), that enables the training of remote RL agents
exchanging observations across a non-ideal wireless channel. HR3L considers two
units: the transmitter, which encodes meaningful representations of the
environment, and the receiver, which decodes these messages and performs
actions to maximize a reward signal. Importantly, HR3L does not require the
exchange of gradient information across the wireless channel, allowing for
quicker training and a lower communication overhead than state-of-the-art
solutions. Experimental results demonstrate that HR3L significantly outperforms
baseline methods in terms of sample efficiency and adapts to different
communication scenarios, including packet losses, delayed transmissions, and
capacity limitations.

</details>


### [224] [Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning](https://arxiv.org/abs/2508.07738)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Xinyu Wei,Mingyue Yang,Leqian Li,Mengzhu Wang,Chunping Qiu*

Main category: cs.LG

TL;DR: 提出TRGE方法解决多领域连续学习中的遗忘问题，通过层次路由策略和多模态模型识别任务，实现高效知识迁移和任务协作。


<details>
  <summary>Details</summary>
Motivation: 面对多领域连续学习中类别和分布的变化带来的遗忘和转移挑战，寻求有效解决方案。

Method: 动态扩展预训练模型，采用两级路由机制进行专家分配与选择，利用多模态大模型识别任务信息，实现信息融合以缓解遗忘。

Result: 在多任务连续学习任务中优于先进方法，参数效率高。

Conclusion: 通过层次路由和多模态任务识别，有效提高多领域连续学习的性能，减少遗忘，提高跨任务协作能力。

Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential
tasks with shifting class sets and distribution. Despite the
Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual
heterogeneity, they still suffer from catastrophic forgetting and forward
forgetting. To address these challenges, we propose a Two-Level Routing Grouped
Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the
pre-trained CLIP model, assigning specific expert group for each task to
mitigate catastrophic forgetting. With the number of experts continually grows
in this process, TRGE maintains the static experts count within the group and
introduces the intra-group router to alleviate routing overfitting caused by
the increasing routing complexity. Meanwhile, we design an inter-group routing
policy based on task identifiers and task prototype distance, which dynamically
selects relevant expert groups and combines their outputs to enhance inter-task
collaboration. Secondly, to get the correct task identifiers, we leverage
Multimodal Large Language Models (MLLMs) which own powerful multimodal
comprehension capabilities to generate semantic task descriptions and recognize
the correct task identifier. Finally, to mitigate forward forgetting, we
dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE
adapter based on training progress, leveraging both pre-trained and learned
knowledge. Through extensive experiments across various settings, our method
outperforms other advanced methods with fewer trainable parameters.

</details>


### [225] [A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory](https://arxiv.org/abs/2508.07746)
*Fengdi Che*

Main category: cs.LG

TL;DR: 离线强化学习的理论基础与实践挑战，重点在于数据质量、函数逼近与解决困难的条件分析。


<details>
  <summary>Details</summary>
Motivation: 理解离线RL的理论基础，有助于指导实际算法的设计。

Method: 梳理关键理论条件、反例与挑战，提出满足条件的解决方案。

Result: 分析了数据覆盖与函数逼近条件，以及不可解决的极端案例，强调条件的重要性和未来方向。

Conclusion: 理论条件指导算法设计，但实践中仍需探索满足条件的创新方法。

Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed
dataset of agent trajectories without additional interactions with the
environment. While algorithm development has progressed rapidly, significant
theoretical advances have also been made in understanding the fundamental
challenges of offline RL. However, bridging these theoretical insights with
practical algorithm design remains an ongoing challenge. In this survey, we
explore key intuitions derived from theoretical work and their implications for
offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function
representation and data coverage assumptions. Function representation
conditions tell us what to expect for generalization, and data coverage
assumptions describe the quality requirement of the data. We then examine
counterexamples, where offline RL is not solvable without an impractically
large amount of data. These cases highlight what cannot be achieved for all
algorithms and the inherent hardness of offline RL. Building on techniques to
mitigate these challenges, we discuss the conditions that are sufficient for
offline RL. These conditions are not merely assumptions for theoretical proofs,
but they also reveal the limitations of these algorithms and remind us to
search for novel solutions when the conditions cannot be satisfied.

</details>


### [226] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [227] [Sparse Probabilistic Graph Circuits](https://arxiv.org/abs/2508.07763)
*Martin Rektoris,Milan Papež,Václav Šmídl,Tomáš Pevný*

Main category: cs.LG

TL;DR: 本文提出稀疏Probabilistic Graph Circuits（SPGCs），通过直接在稀疏图上进行建模，显著降低了复杂度，增强了可扩展性，同时在药物设计中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决深度生成模型在图结构上由于非线性导致的推断不可解析性问题，同时提升大规模稀疏图的计算效率。

Method: 引入稀疏PGCs，基于稀疏图表示，降低复杂度至O(n + m)，并在药物设计中验证其实用性。

Result: SPGCs实现了精确推断，提升了存储和计算效率，性能与传统DGMs相当，适用于大规模稀疏图任务。

Conclusion: 稀疏PGCs为图结构的可扩展性提供了高效、准确的解决方案，特别适合药物设计等应用场景。

Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive
power thanks to very efficient and scalable neural networks. However, these
networks contain non-linearities that prevent analytical computation of many
standard probabilistic inference queries, i.e., these DGMs are considered
\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)
address this issue by enabling \emph{tractable} probabilistic inference, they
operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for
graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue,
we introduce Sparse PGCs, a new class of tractable generative models that
operate directly on sparse graph representation, reducing the complexity to
$\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the
context of de novo drug design, we empirically demonstrate that SPGCs retain
exact inference capabilities, improve memory efficiency and inference speed,
and match the performance of intractable DGMs in key metrics.

</details>


### [228] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 提出一种高效的多目标对齐算法PAMA，有效解决大规模语言模型多目标优化的复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 当前的单一奖励函数优化限制了大模型在多目标场景中的适应性，亟需多目标对齐方法以更好匹配人类多样偏好。

Method: 将多目标强化学习转化为凸优化问题，采用封闭式解，显著降低复杂度。

Result: 在不同规模的语言模型上验证了PAMA的有效性，展现出快速且稳健的多目标对齐能力。

Conclusion: PAMA提供了一种理论可靠、计算高效的多目标对齐解决方案，推动了大模型在实际应用中的多样化和适应性。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [229] [Topological Feature Compression for Molecular Graph Neural Networks](https://arxiv.org/abs/2508.07807)
*Rahul Khorana*

Main category: cs.LG

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Recent advances in molecular representation learning have produced highly
effective encodings of molecules for numerous cheminformatics and
bioinformatics tasks. However, extracting general chemical insight while
balancing predictive accuracy, interpretability, and computational efficiency
remains a major challenge. In this work, we introduce a novel Graph Neural
Network (GNN) architecture that combines compressed higher-order topological
signals with standard molecular features. Our approach captures global
geometric information while preserving computational tractability and
human-interpretable structure. We evaluate our model across a range of
benchmarks, from small-molecule datasets to complex material datasets, and
demonstrate superior performance using a parameter-efficient architecture. We
achieve the best performing results in both accuracy and robustness across
almost all benchmarks. We open source all code \footnote{All code and results
can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.

</details>


### [230] [EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning](https://arxiv.org/abs/2508.07809)
*Huanyu Liu,Jia Li,Chang Yu,Taozhi Chen,Yihong Dong,Lecheng Wang,Hu XiaoLong,Ge Li*

Main category: cs.LG

TL;DR: EvoCoT是一种基于自我演化课程学习的框架，通过两阶段链式思考优化，改善大型语言模型在稀疏奖励环境下的推理能力，能解决以前难以解决的问题。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在硬问题上奖励稀疏、学习效果差的问题，提升大模型推理能力。

Method: 提出EvoCoT框架，通过自我生成和验证链式思考轨迹，逐步缩短并扩展探索空间，实现从难题到易题的逐步学习。

Result: 在多个大模型上验证，EvoCoT能解决之前无法解决的问题，提高推理能力，并兼容多种RL微调方法，无需外部监督。

Conclusion: EvoCoT通过自我演化的课程学习策略，有效改善大模型在稀疏奖励环境中的推理表现，为未来研究提供新的方向。

Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (LLMs) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes sparse, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger LLMs for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
LLMs to stably learn from initially unsolved hard problems under sparse
rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables LLMs to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.

</details>


### [231] [Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: 利用物理知识神经网络（PINNs）结合模型预测控制（MPC）显著提升航天器姿态控制的性能、稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型在空间器姿态控制中的模型不完整或难以获取问题，探索利用深度学习尤其是PINNs增强控制系统的效果。

Method: 采用带自注意力机制的Real NVP神经网络，结合仿真数据进行训练，比较纯数据驱动模型与加入物理信息的PINNs模型在控制任务中的表现。

Result: 引入物理信息的模型在相对误差、控制准确性和鲁棒性方面具有显著优势，提升了模型性能及对噪声的鲁棒性。

Conclusion: 结合物理知识的深度学习模型能有效改善空间器姿态控制的性能，是未来研究的重要方向。

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error of the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, yielding
improvements of up to 42.86% in performance stability error and increased
robustness-to-noise.

</details>


### [232] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: 利用迁移学习技术实现预测性流程监控，帮助缺乏数据的组织提升决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有预测性流程监控方法对数据需求较高，限制了部分组织的应用。

Method: 采用迁移学习，将知识从一个业务流程转移到另一个类似流程，实现无须大量数据的预测。

Result: 在两个实际案例中验证，知识迁移提升了目标流程的预测效果，跨组织应用也有效。

Conclusion: 迁移学习为预测性流程监控提供新途径，支持跨组织协作与资源共享。

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [233] [Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant](https://arxiv.org/abs/2508.07887)
*Sabrina Namazova,Alessandra Brondetta,Younes Strittmatter,Matthew Nassar,Sebastian Musslick*

Main category: cs.LG

TL;DR: 本文评估了大型语言模型Centaur作为人类行为模拟器的潜力，发现其在预测准确性方面表现优秀，但在生成行为的真实性方面仍有差距。


<details>
  <summary>Details</summary>
Motivation: 推动行为科学模拟工具的发展，以实现更高效的实验设计和假设验证。

Method: 分析并评估Centaur在模拟人类行为方面的表现，比较其预测准确性和生成行为的真实性。

Result: Centaur在预测方面表现优异，但在生成行为的真实性方面存在偏差，尚不能作为完全可靠的行为模拟器。

Conclusion: 尽管Centaur代表了向人类行为模拟迈进的重要一步，但其生成行为的偏差表明仍需改进，以达到可靠模拟的标准。

Abstract: Simulators have revolutionized scientific practice across the natural
sciences. By generating data that reliably approximate real-world phenomena,
they enable scientists to accelerate hypothesis testing and optimize
experimental designs. This is perhaps best illustrated by AlphaFold, a
Nobel-prize winning simulator in chemistry that predicts protein structures
from amino acid sequences, enabling rapid prototyping of molecular
interactions, drug targets, and protein functions. In the behavioral sciences,
a reliable participant simulator - a system capable of producing human-like
behavior across cognitive tasks - would represent a similarly transformative
advance. Recently, Binz et al. introduced Centaur, a large language model (LLM)
fine-tuned on human data from 160 experiments, proposing its use not only as a
model of cognition but also as a participant simulator for "in silico
prototyping of experimental studies", e.g., to advance automated cognitive
science. Here, we review the core criteria for a participant simulator and
assess how well Centaur meets them. Although Centaur demonstrates strong
predictive accuracy, its generative behavior - a critical criterion for a
participant simulator - systematically diverges from human data. This suggests
that, while Centaur is a significant step toward predicting human behavior, it
does not yet meet the standards of a reliable participant simulator or an
accurate model of cognition.

</details>


### [234] [Score Augmentation for Diffusion Models](https://arxiv.org/abs/2508.07926)
*Liang Hou,Yuan Gao,Boyuan Jiang,Xin Tao,Qi Yan,Renjie Liao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.LG

TL;DR: 提出了一种针对扩散模型的Score Augmentation方法，有效减少过拟合并提升性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但在数据有限时易过拟合，亟需有效的数据增强策略。

Method: 设计了在噪声空间进行变换的ScoreAug框架，通过对噪声数据进行变换并训练去噪器预测原始目标扩充得分空间，实现等变学习目标。

Result: 在多项基准测试中，ScoreAug显著优于传统方法，有效缓解过拟合，且与传统增强结合效果更佳。

Conclusion: ScoreAug是一种适用于扩散模型的强大数据增强方法，可提升模型性能并确保训练稳定性。

Abstract: Diffusion models have achieved remarkable success in generative modeling.
However, this study confirms the existence of overfitting in diffusion model
training, particularly in data-limited regimes. To address this challenge, we
propose Score Augmentation (ScoreAug), a novel data augmentation framework
specifically designed for diffusion models. Unlike conventional augmentation
approaches that operate on clean data, ScoreAug applies transformations to
noisy data, aligning with the inherent denoising mechanism of diffusion.
Crucially, ScoreAug further requires the denoiser to predict the augmentation
of the original target. This design establishes an equivariant learning
objective, enabling the denoiser to learn scores across varied denoising
spaces, thereby realizing what we term score augmentation. We also
theoretically analyze the relationship between scores in different spaces under
general transformations. In experiments, we extensively validate ScoreAug on
multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with
results demonstrating significant performance improvements over baselines.
Notably, ScoreAug effectively mitigates overfitting across diverse scenarios,
such as varying data scales and model capacities, while exhibiting stable
convergence properties. Another advantage of ScoreAug over standard data
augmentation lies in its ability to circumvent data leakage issues under
certain conditions. Furthermore, we show that ScoreAug can be synergistically
combined with traditional data augmentation techniques to achieve additional
performance gains.

</details>


### [235] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文系统评估了强化学习在大型语言模型推理中的应用，分析关键机制并提出选择指南，发现简约策略能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 推动强化学习在大型语言模型推理中的规范化应用，解决现有方法碎片化和机制不明的问题。

Method: 通过严谨复现和孤立评估，在统一开源框架下分析多种RL技术的机制、适用场景及核心原理，并进行丰富多样的实验。

Result: 明确了不同RL技术的适用条件和机制，提出有效的选择策略，发现简洁技术组合能优于复杂策略。

Conclusion: 为RL在大模型中的应用提供了科学指导和实践路径，证实简单策略的潜力。

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


### [236] [Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting](https://arxiv.org/abs/2508.07927)
*Amal Saadallah,Abdulaziz Al-Ademi*

Main category: cs.LG

TL;DR: 提出一种时间序列预测新框架，通过模型适应和选择应对非平稳环境，结合聚类和概念漂移检测实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境下时间序列预测困难，提升深度神经网络的适应性和表现。

Method: 训练基础DNN，提取时间序列中的主要模式，建立多模型体系，并利用簇心匹配和概念漂移检测进行动态调整。

Result: 在不同DNN架构上均表现出显著性能提升，尤其在传统和先进架构中效果明显。

Conclusion: 该框架具有广泛适用性，有效提升非平稳环境中的时间序列预测能力。

Abstract: Time series forecasting poses significant challenges in non-stationary
environments where underlying patterns evolve over time. In this work, we
propose a novel framework that enhances deep neural network (DNN) performance
by leveraging specialized model adaptation and selection. Initially, a base DNN
is trained offline on historical time series data. A reserved validation subset
is then segmented to extract and cluster the most dominant patterns within the
series, thereby identifying distinct regimes. For each identified cluster, the
base DNN is fine-tuned to produce a specialized version that captures unique
pattern characteristics. At inference, the most recent input is matched against
the cluster centroids, and the corresponding fine-tuned version is deployed
based on the closest similarity measure. Additionally, our approach integrates
a concept drift detection mechanism to identify and adapt to emerging patterns
caused by non-stationary behavior. The proposed framework is generalizable
across various DNN architectures and has demonstrated significant performance
gains on both traditional DNNs and recent advanced architectures implemented in
the GluonTS library.

</details>


### [237] [Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters](https://arxiv.org/abs/2508.07952)
*Richard J. Fawley,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: 提出一种基于Shapley值的特征加权k-means算法SHARK，有效提升高维或噪声环境下聚类效果。


<details>
  <summary>Details</summary>
Motivation: 解决高维或噪声环境下特征贡献不均导致的聚类性能下降问题。

Method: 利用Shapley值量化特征相关性，提出迭代重加权机制，降低计算复杂度。

Result: 在多种数据集上表现优异，增强鲁棒性与准确性。

Conclusion: SHARK结合游戏论中的Shapley值，为无监督特征重要性评估提供理论基础，并提升聚类性能。

Abstract: Clustering algorithms often assume all features contribute equally to the
data structure, an assumption that usually fails in high-dimensional or noisy
settings. Feature weighting methods can address this, but most require
additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a
feature-weighted clustering algorithm motivated by the use of Shapley values
from cooperative game theory to quantify feature relevance, which requires no
additional parameters beyond those in $k$-means. We prove that the $k$-means
objective can be decomposed into a sum of per-feature Shapley values, providing
an axiomatic foundation for unsupervised feature relevance and reducing Shapley
computation from exponential to polynomial time. SHARK iteratively re-weights
features by the inverse of their Shapley contribution, emphasising informative
dimensions and down-weighting irrelevant ones. Experiments on synthetic and
real-world data sets show that SHARK consistently matches or outperforms
existing methods, achieving superior robustness and accuracy, particularly in
scenarios where noise may be present. Software:
https://github.com/rickfawley/shark.

</details>


### [238] [WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2508.07970)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Tingfeng Xian,Haoqiang Hong,Boqi Chen,Haotao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: 提出了一种名为WeChat-YATT的RLHF训练框架，解决多模态任务中的可扩展性和动态调度问题，显著提升训练效率并已应用于微信产品中。


<details>
  <summary>Details</summary>
Motivation: 当前的RLHF系统在管理大型模型和复杂工作流程时面临可扩展性和资源调度的挑战。

Method: 设计了并行控制器编程模型和动态资源调度方案，优化流程管理和资源利用。

Result: 实现了训练吞吐量的提升，并在微信产品中成功部署，验证了其实用性和有效性。

Conclusion: WeChat-YATT是一种简单、可扩展、平衡的RLHF训练框架，解决了大规模多模态训练中的关键难题，具有良好的应用前景。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent
paradigm for training large language models and multimodal systems. Despite
notable advances enabled by existing RLHF training frameworks, significant
challenges remain in scaling to complex multimodal workflows and adapting to
dynamic workloads. In particular, current systems often encounter limitations
related to controller scalability when managing large models, as well as
inefficiencies in orchestrating intricate RLHF pipelines, especially in
scenarios that require dynamic sampling and resource allocation. In this paper,
we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,
scalable, and balanced RLHF training framework specifically designed to address
these challenges. WeChat-YATT features a parallel controller programming model
that enables flexible and efficient orchestration of complex RLHF workflows,
effectively mitigating the bottlenecks associated with centralized controller
architectures and facilitating scalability in large-scale data scenarios. In
addition, we propose a dynamic placement schema that adaptively partitions
computational resources and schedules workloads, thereby significantly reducing
hardware idle time and improving GPU utilization under variable training
conditions. We evaluate WeChat-YATT across a range of experimental scenarios,
demonstrating that it achieves substantial improvements in throughput compared
to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been
successfully deployed to train models supporting WeChat product features for a
large-scale user base, underscoring its effectiveness and robustness in
real-world applications.

</details>


### [239] [A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation](https://arxiv.org/abs/2508.08002)
*Hongxin Yu,Yibing Wang,Fengyue Jin,Meng Zhang,Anni Chen*

Main category: cs.LG

TL;DR: 引入基于深度神经网络的物理信息深度操作符网络(PI-DeepONet)用于实时高速公路交通状态估算，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了提升实时高速公路交通状态估算的准确性，结合物理模型和数据驱动方法的优势，提出一种新型深度操作符网络架构。

Method: 扩展PI-DeepONet架构，支持二维数据输入，加入非线性扩展层、注意力机制和多输入多输出(MIMO)机制，并设计专用神经网络进行交通模型参数自适应识别。

Result: 在中国多个真实高速公路数据集上的评估显示，该方法在交通流和平均速度估算方面显著优于四种基线方法。

Conclusion: 基于物理信息深度操作符网络的交通状态估算方法具有高精度和实时性，展示了深度学习与交通模型融合的巨大潜力。

Abstract: Traffic state estimation (TSE) falls methodologically into three categories:
model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies
on macroscopic traffic flow models originated from hydrodynamics. Data-driven
TSE leverages historical sensing data and employs statistical models or machine
learning methods to infer traffic state. Model-data dual-driven traffic state
estimation attempts to harness the strengths of both aspects to achieve more
accurate TSE. From the perspective of mathematical operator theory, TSE can be
viewed as a type of operator that maps available measurements of inerested
traffic state into unmeasured traffic state variables in real time. For the
first time this paper proposes to study real-time freeway TSE in the idea of
physics-informed deep operator network (PI-DeepONet), which is an
operator-oriented architecture embedding traffic flow models based on deep
neural networks. The paper has developed an extended architecture from the
original PI-DeepONet. The extended architecture is featured with: (1) the
acceptance of 2-D data input so as to support CNN-based computations; (2) the
introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO
mechanism; (3) dedicated neural network design for adaptive identification of
traffic flow model parameters. A traffic state estimator built on the basis of
this extended PI-DeepONet architecture was evaluated with respect to a short
freeway stretch of NGSIM and a large-scale urban expressway in China, along
with other four baseline TSE methods. The evaluation results demonstrated that
this novel TSE method outperformed the baseline methods with high-precision
estimation results of flow and mean speed.

</details>


### [240] [Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP](https://arxiv.org/abs/2508.08005)
*Xiang Li,Shanshan Wang,Chenglong Xiao*

Main category: cs.LG

TL;DR: 提出一种结合图神经网络和传统机器学习的学习框架，用于最大团问题的算法选择，基于图结构特征训练模型，显著提升算法性能预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决不同图实例中单一最大团算法性能不稳定的问题，缺乏针对最大团问题的算法选择研究。

Method: 构建标注数据集，提取图结构与统计特征，比较传统分类器，设计结合图注意机制的双通道模型GAT-MLP。

Result: 随机森林表现稳定优异，GAT-MLP达到了较强的性能和一致性。

Conclusion: 多通道架构和图神经网络在组合优化算法选择中具有巨大潜力。

Abstract: Extensive experiments and prior studies show that no single maximum clique
algorithm consistently performs best across all instances, highlighting the
importance of selecting suitable algorithms based on instance features. Through
an extensive analysis of relevant studies, it is found that there is a lack of
research work concerning algorithm selection oriented toward the Maximum Clique
Problem (MCP). In this work, we propose a learning-based framework that
integrates both traditional machine learning and graph neural networks to
address this gap. We construct a labeled dataset by running four exact MCP
algorithms on a diverse collection of graph instances, accompanied by
structural and global statistical features extracted from each graph. We first
evaluate four conventional classifiers: Support Vector Machine (SVM), Random
Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple
dataset variants. Experimental results show that RF consistently shows strong
performance across metrics and dataset variants, making it a reliable baseline.
In addition, feature importance analysis indicates that connectivity and
topological structure are strong predictors of algorithm performance. Building
on these findings, we develop a dual-channel model named GAT-MLP, which
combines a Graph Attention Network (GAT) for local structural encoding with a
Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model
shows strong and consistent performance across all metrics. Our results
highlight the effectiveness of dual-channel architectures and the promise of
graph neural networks in combinatorial algorithm selection.

</details>


### [241] [Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks](https://arxiv.org/abs/2508.08013)
*Mohamad Assaad,Zeinab Nehme,Merouane Debbah*

Main category: cs.LG

TL;DR: 提出两种通讯高效的联邦学习方法，通过通信标量值及允许异步设备，提高效率，无需额外CSI资源，提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中大量信息交换带来的通信瓶颈，利用信道信息提升学习效率，减少资源消耗。

Method: 使用零阶优化与一阶梯度计算，结合信道信息，无需额外CSI，支持异步设备操作。

Result: 构建严谨的分析框架，证明收敛性，设定性能界限，验证方法的有效性。

Conclusion: 所提方法有效降低通信开销，利用信道信息优化联邦学习，适应异步环境，并具有理论保证。

Abstract: Federated Learning (FL) is an emerging learning framework that enables edge
devices to collaboratively train ML models without sharing their local data. FL
faces, however, a significant challenge due to the high amount of information
that must be exchanged between the devices and the aggregator in the training
phase, which can exceed the limited capacity of wireless systems. In this
paper, two communication-efficient FL methods are considered where
communication overhead is reduced by communicating scalar values instead of
long vectors and by allowing high number of users to send information
simultaneously. The first approach employs a zero-order optimization technique
with two-point gradient estimator, while the second involves a first-order
gradient computation strategy. The novelty lies in leveraging channel
information in the learning algorithms, eliminating hence the need for
additional resources to acquire channel state information (CSI) and to remove
its impact, as well as in considering asynchronous devices. We provide a
rigorous analytical framework for the two methods, deriving convergence
guarantees and establishing appropriate performance bounds.

</details>


### [242] [Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles](https://arxiv.org/abs/2508.08034)
*Roksana Yahyaabadi,Ghazal Farhani,Taufiq Rahman,Soodeh Nikan,Abdullah Jirjees,Fadi Araji*

Main category: cs.LG

TL;DR: 本文提出一种基于数据驱动的多平台电力消耗预测方法，结合机器学习和深度神经网络，取得了高精度和良好的泛化能力，特别强调了不同动力系统的复杂性和不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统电力预测方法在大规模实际应用中的局限性，提供一种高效、普适的预测工具。

Method: 利用动力学特征集结合机器学习（如Transformer和LSTM）与深度神经网络进行电力消耗预测。

Result: ICE平台预测精度极高，误差极小；EV和HEV中Transformer和LSTM表现最佳，误差均低于4.1%；不确定性分析显示EV和HEV的能量管理复杂性带来更大变异性。

Conclusion: 提出的方法具有广泛适应性，凸显了复杂动力系统预测中的挑战与需求，未来需加强对多样性数据的鲁棒性研究。

Abstract: Accurate power consumption prediction is crucial for improving efficiency and
reducing environmental impact, yet traditional methods relying on specialized
instruments or rigid physical models are impractical for large-scale,
real-world deployment. This study introduces a scalable data-driven method
using powertrain dynamic feature sets and both traditional machine learning and
deep neural networks to estimate instantaneous and cumulative power consumption
in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric
vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with
mean absolute error and root mean squared error on the order of $10^{-3}$, and
cumulative errors under 3%. Transformer and long short-term memory models
performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,
respectively. Results confirm the approach's effectiveness across vehicles and
models. Uncertainty analysis revealed greater variability in EV and HEV
datasets than ICE, due to complex power management, emphasizing the need for
robust models for advanced powertrains.

</details>


### [243] [BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models](https://arxiv.org/abs/2508.08040)
*Maozhen Zhang,Mengnan Zhao,Bo Wang*

Main category: cs.LG

TL;DR: BadPromptFL是一种对基于提示的联邦学习多模态模型的后门攻击方法，能够高效、隐蔽地植入后门，威胁模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 探讨提示基联邦学习中的安全风险，揭示其潜在的攻击面。

Method: 通过联合优化本地后门触发器和提示嵌入，将毒化提示注入全局聚合过程，实现无参数修改的后门激活。

Result: 攻击成功率高（如超过90%），具有良好的隐蔽性和泛化能力，验证了其有效性和危险性。

Conclusion: 提示基联邦学习存在严重安全隐患，需引起重视和加强防护措施。

Abstract: Prompt-based tuning has emerged as a lightweight alternative to full
fine-tuning in large vision-language models, enabling efficient adaptation via
learned contextual prompts. This paradigm has recently been extended to
federated learning settings (e.g., PromptFL), where clients collaboratively
train prompts under data privacy constraints. However, the security
implications of prompt-based aggregation in federated multimodal learning
remain largely unexplored, leaving a critical attack surface unaddressed. In
this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack
targeting prompt-based federated learning in multimodal contrastive models. In
BadPromptFL, compromised clients jointly optimize local backdoor triggers and
prompt embeddings, injecting poisoned prompts into the global aggregation
process. These prompts are then propagated to benign clients, enabling
universal backdoor activation at inference without modifying model parameters.
Leveraging the contextual learning behavior of CLIP-style architectures,
BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal
visibility and limited client participation. Extensive experiments across
multiple datasets and aggregation protocols validate the effectiveness,
stealth, and generalizability of our attack, raising critical concerns about
the robustness of prompt-based federated learning in real-world deployments.

</details>


### [244] [On Understanding of the Dynamics of Model Capacity in Continual Learning](https://arxiv.org/abs/2508.08052)
*Supriyo Chakraborty,Krishnan Raghavan*

Main category: cs.LG

TL;DR: 引入CL的有效模型容量（CLEMC）以理解神经网络在持续学习中的稳定性与塑性之间的动态平衡，发现容量非静态且随任务分布变化而变化。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中神经网络面临的稳定性与塑性平衡难题。

Method: 建立差分方程模型，分析不同架构和优化方法下容量的演变。

Result: 证明有效容量是非静态的，任务分布的变化影响网络表示新任务的能力。

Conclusion: 有效容量动态变化，影响持续学习的性能表现，适用于多种网络架构。

Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN)
capacity-its ability to represent tasks-is a fundamental challenge in continual
learning (CL). Within this context, we introduce CL's effective model capacity
(CLEMC) that characterizes the dynamic behavior of the stability-plasticity
balance point. We develop a difference equation to model the evolution of the
interplay between the NN, task data, and optimization procedure. We then
leverage CLEMC to demonstrate that the effective capacity-and, by extension,
the stability-plasticity balance point is inherently non-stationary. We show
that regardless of the NN architecture or optimization method, a NN's ability
to represent new tasks diminishes when incoming task distributions differ from
previous ones. We conduct extensive experiments to support our theoretical
findings, spanning a range of architectures-from small feedforward network and
convolutional networks to medium-sized graph neural networks and
transformer-based large language models with millions of parameters.

</details>


### [245] [C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction](https://arxiv.org/abs/2508.08071)
*Yunqing Li,Zixiang Tang,Jiaying Zhuang,Zhenyu Yang,Farhad Ameri,Jianbang Zhang*

Main category: cs.LG

TL;DR: 引入PMGraph数据集和C-MAG模型以改善供应链中的制造商与产品连接，通过多模态信息增强预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉供应链中制造商的复杂能力、认证、地理限制和多模态数据，亟需更有效的连接预测技术。

Method: 构建PMGraph基准数据集，提出两阶段C-MAG架构，结合文本和图像信息进行多模态信息融合与传播。

Result: C-MAG在链路预测中表现优越，提供了模态感知融合的实用指南，有助于应对真实世界中的噪声环境。

Conclusion: 多模态信息的集成提升了供应链连接预测的准确性，为供应链管理提供了新的技术路径。

Abstract: Connecting an ever-expanding catalogue of products with suitable
manufacturers and suppliers is critical for resilient, efficient global supply
chains, yet traditional methods struggle to capture complex capabilities,
certifications, geographic constraints, and rich multimodal data of real-world
manufacturer profiles. To address these gaps, we introduce PMGraph, a public
benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking
8,888 manufacturers, over 70k products, more than 110k manufacturer-product
edges, and over 29k product images. Building on this benchmark, we propose the
Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first
aligns and aggregates textual and visual attributes into intermediate group
embeddings, then propagates them through a manufacturer-product hetero-graph
via multiscale message passing to enhance link prediction accuracy. C-MAG also
provides practical guidelines for modality-aware fusion, preserving predictive
performance in noisy, real-world settings.

</details>


### [246] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 通过引入分类器提前识别无效剪裁，有效加快逻辑优化，平均提速3.9倍。


<details>
  <summary>Details</summary>
Motivation: 逻辑优化操作，尤其是在电路简化中，计算成本高且95%以上剪裁无效，亟需提升效率。

Method: 采用分类器预判无效剪裁，减少不必要的重复重构操作。

Result: 在EPFL基准和工业设计上实现了平均3.9倍的速度提升。

Conclusion: 该方法通过智能预筛，显著优化了逻辑优化的计算效率，比传统并行方法更为有效。

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [247] [Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles](https://arxiv.org/abs/2508.08080)
*Cas Oude Hoekstra,Floris den Hengst*

Main category: cs.LG

TL;DR: 引入符号分位数回归（SQR）扩展符号回归（SR），能更全面理解变量对目标分布不同点的影响，适用于高风险领域。


<details>
  <summary>Details</summary>
Motivation: 拓展符号回归在除了平均值外的目标分布端点的预测能力，满足高风险场景对极端值和中位数的需求。

Method: 开发符号分位数回归（SQR）方法，通过符号回归实现条件分位数预测，并进行广泛评估。

Result: SQR在性能上优于透明模型，接近黑箱模型，能解释极端和中心值的差异，展现出在不同分位点预测和特征影响理解上的优势。

Conclusion: SQR适合用于预测条件分位数，帮助理解不同分位点的特征影响，增强模型的解释性与实用性。

Abstract: Symbolic Regression (SR) is a well-established framework for generating
interpretable or white-box predictive models. Although SR has been successfully
applied to create interpretable estimates of the average of the outcome, it is
currently not well understood how it can be used to estimate the relationship
between variables at other points in the distribution of the target variable.
Such estimates of e.g. the median or an extreme value provide a fuller picture
of how predictive variables affect the outcome and are necessary in
high-stakes, safety-critical application domains. This study introduces
Symbolic Quantile Regression (SQR), an approach to predict conditional
quantiles with SR. In an extensive evaluation, we find that SQR outperforms
transparent models and performs comparably to a strong black-box baseline
without compromising transparency. We also show how SQR can be used to explain
differences in the target distribution by comparing models that predict extreme
and central outcomes in an airline fuel usage case study. We conclude that SQR
is suitable for predicting conditional quantiles and understanding interesting
feature influences at varying quantiles.

</details>


### [248] [Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation](https://arxiv.org/abs/2508.08087)
*Amir Ali Panahi,Daniel Luder,Billy Wu,Gregory Offer,Dirk Uwe Sauer,Weihan Li*

Main category: cs.LG

TL;DR: 本文比较了三种操作符学习模型在锂离子电池单粒子模型中的表现，特别是提出的参数嵌入傅里叶神经操作器PE-FNO在速度和适应性方面具有显著优势，推动数字孪生技术应用于实时电池管理。


<details>
  <summary>Details</summary>
Motivation: 提高锂离子电池数字孪生的物理精确度和计算速度，满足实时监控与优化的需求。

Method: 采用深度操作神经网络（DeepONet）、傅里叶神经操作器（FNO）及新提出的参数嵌入FNO（PE-FNO），在多种电流负载和SOC范围内训练和测试模型，比较其性能及速度，并应用于参数估计。

Result: PE-FNO在速度上比传统模拟器快约200倍，误差低且具有良好的泛化能力，在参数估计中表现优异，验证了其在实时电池管理中的潜力。

Conclusion: 神经操作器尤其是PE-FNO，具备高速度、高精度和参数灵活性，为电池数字孪生的发展提供了实用路径。

Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical
fidelity with sub-millisecond speed. In this work, we benchmark three
operator-learning surrogates for the Single Particle Model (SPM): Deep Operator
Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed
parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each
spectral layer on particle radius and solid-phase diffusivity. Models are
trained on simulated trajectories spanning four current families (constant,
triangular, pulse-train, and Gaussian-random-field) and a full range of
State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates
constant-current behaviour but struggles with more dynamic loads. The basic FNO
maintains mesh invariance and keeps concentration errors below 1 %, with
voltage mean-absolute errors under 1.7 mV across all load types. Introducing
parameter embedding marginally increases error, but enables generalisation to
varying radii and diffusivities. PE-FNO executes approximately 200 times faster
than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse
tasks are explored in a parameter estimation task with Bayesian optimisation,
recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute
percentage error, respectively, and 0.5918 percentage points higher error in
comparison with classical methods. These results pave the way for neural
operators to meet the accuracy, speed and parametric flexibility demands of
real-time battery management, design-of-experiments and large-scale inference.
PE-FNO outperforms conventional neural surrogates, offering a practical path
towards high-speed and high-fidelity electrochemical digital twins.

</details>


### [249] [Grid2Guide: A* Enabled Small Language Model for Indoor Navigation](https://arxiv.org/abs/2508.08100)
*Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: Grid2Guide结合A*算法与Small Language Model，提供高效、无基础设施的室内导航解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中缺乏外部信号和基础设施的室内导航难题。

Method: 通过建筑物占用矩阵使用A*算法寻找路径，并利用小型语言模型将路径转换为自然语言指令。

Result: 在多场景测试中，系统表现出准确、及时的导航指导，验证其有效性。

Conclusion: 该方法为实时室内导航提供了一个轻量、无基础设施的解决方案，具有良好的应用潜力。

Abstract: Reliable indoor navigation remains a significant challenge in complex
environments, particularly where external positioning signals and dedicated
infrastructures are unavailable. This research presents Grid2Guide, a hybrid
navigation framework that combines the A* search algorithm with a Small
Language Model (SLM) to generate clear, human-readable route instructions. The
framework first conducts a binary occupancy matrix from a given indoor map.
Using this matrix, the A* algorithm computes the optimal path between origin
and destination, producing concise textual navigation steps. These steps are
then transformed into natural language instructions by the SLM, enhancing
interpretability for end users. Experimental evaluations across various indoor
scenarios demonstrate the method's effectiveness in producing accurate and
timely navigation guidance. The results validate the proposed approach as a
lightweight, infrastructure-free solution for real-time indoor navigation
support.

</details>


### [250] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 利用视觉定位结合大语言模型实现室内导航，达96%的定位准确率和75%的导航指令准确率，提供了无需特殊基础设施的室内导航方案。


<details>
  <summary>Details</summary>
Motivation: 解决室内导航缺乏可靠GPS信号的问题，提供一种高效的室内定位与导航解决方案。

Method: 采用微调的ResNet-50进行视觉定位，并利用大语言模型解读楼层图生成导航指令。

Result: 在实际环境中展示出较高的定位准确性和导航效果，适用于资源有限的场景。

Conclusion: 基于现有设备和公开资源的室内导航具有巨大潜力，尤其在基础设施不足的环境中。

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


### [251] [MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing](https://arxiv.org/abs/2508.08122)
*Mingrong Lin,Ke Deng,Zhengyang Wu,Zetao Zheng,Jie Li*

Main category: cs.LG

TL;DR: memoryKT通过模拟学生记忆动态，改进知识追踪模型的表现和个性化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型多忽视记忆过程中的个性化遗忘机制，导致对学生差异的建模不足。

Method: 引入基于时间变分自编码器的三阶段记忆动态建模，包括编码、存储和检索，融合个性化遗忘模块。

Result: 在四个公开数据集上，显著优于现有最优模型。

Conclusion: 该模型有效提升知识追踪的性能和对个体差异的感知能力。

Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery
from their historical interactions. Simulating students' memory states is a
promising approach to enhance both the performance and interpretability of
knowledge tracing models. Memory consists of three fundamental processes:
encoding, storage, and retrieval. Although forgetting primarily manifests
during the storage stage, most existing studies rely on a single,
undifferentiated forgetting mechanism, overlooking other memory processes as
well as personalized forgetting patterns. To address this, this paper proposes
memoryKT, a knowledge tracing model based on a novel temporal variational
autoencoder. The model simulates memory dynamics through a three-stage process:
(i) Learning the distribution of students' knowledge memory features, (ii)
Reconstructing their exercise feedback, while (iii) Embedding a personalized
forgetting module within the temporal workflow to dynamically modulate memory
storage strength. This jointly models the complete encoding-storage-retrieval
cycle, significantly enhancing the model's perception capability for individual
differences. Extensive experiments on four public datasets demonstrate that our
proposed approach significantly outperforms state-of-the-art baselines.

</details>


### [252] [NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection](https://arxiv.org/abs/2508.08124)
*Guanghao Jin,Yuan Liang,Yihan Ma,Jingpei Wu,Guoyang Liu*

Main category: cs.LG

TL;DR: 提出为EEG疾病检测设计的NeuronDx-LM模型，结合时间频率embedding和逐步特征训练，显著提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG大模型在临床应用中因数据匮乏和性能不足的问题，推动神经疾病检测的精准化。

Method: 引入选择性时频embedding机制与渐进特征感知训练策略，分两阶段优化特征。

Result: 在CHB-MIT和精神分裂症数据集上实现了最佳检测性能，展示了临床潜力。

Conclusion: NeuroDx-LM模型提升了EEG疾病检测的准确性和实用性，具有推广价值。

Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown
promise in clinical applications such as neurological disorder detection.
However, the practical deployment of EEG-based large-scale models faces
critical challenges such as limited labeled EEG data and suboptimal performance
in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel
large-scale model specifically designed for detecting EEG-based neurological
disorders. Our key contributions include (i) a Selective Temporal-Frequency
Embedding mechanism that adaptively captures complex temporal and spectral
patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy
that refines feature representation in a two-stage process. In the first stage,
our model learns the fundamental discriminative features of EEG activities; in
the second stage, the model further extracts more specialized fine-grained
features for accurate diagnostic performance. We evaluated NeuroDx-LM on the
CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in
EEG-based seizure and schizophrenia detection, respectively. These results
demonstrate the great potential of EEG-based large-scale models to advance
clinical applicability. Our code is available at
https://github.com/LetItBe12345/NeuroDx-LM.

</details>


### [253] [OFAL: An Oracle-Free Active Learning Framework](https://arxiv.org/abs/2508.08126)
*Hadi Khorsand,Vahid Pourahmadi*

Main category: cs.LG

TL;DR: 提出了一种无需标注员的主动学习方法OFAL，利用神经网络的不确定性进行样本选择，通过生成新的不确定样本提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 期望在大量未标记数据中，减少对昂贵的标注员依赖，提高主动学习效率。

Method: 结合神经网络不确定性分析与变分自编码器，生成新的不确定样本进行主动学习。

Result: 提升模型准确率，并与其他主动学习方法进行了比较和集成。

Conclusion: 提出的OFAL方法有效降低了标注成本，并增强了主动学习的效果。

Abstract: In the active learning paradigm, using an oracle to label data has always
been a complex and expensive task, and with the emersion of large unlabeled
data pools, it would be highly beneficial If we could achieve better results
without relying on an oracle. This research introduces OFAL, an oracle-free
active learning scheme that utilizes neural network uncertainty. OFAL uses the
model's own uncertainty to transform highly confident unlabeled samples into
informative uncertain samples. First, we start with separating and quantifying
different parts of uncertainty and introduce Monte Carlo Dropouts as an
approximation of the Bayesian Neural Network model. Secondly, by adding a
variational autoencoder, we go on to generate new uncertain samples by stepping
toward the uncertain part of latent space starting from a confidence seed
sample. By generating these new informative samples, we can perform active
learning and enhance the model's accuracy. Lastly, we try to compare and
integrate our method with other widely used active learning sampling methods.

</details>


### [254] [MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08137)
*Pravallika Abbineni,Saoud Aldowaish,Colin Liechty,Soroosh Noorzad,Ali Ghazizadeh,Morteza Fayazi*

Main category: cs.LG

TL;DR: 提出MuaLLM，一种结合混合检索增强生成和多模态能力的电路设计辅助大语言模型，改善文献综述和设计决策效率。


<details>
  <summary>Details</summary>
Motivation: 电路设计文献繁杂，信息获取困难，急需高效的辅助工具以提升研究效率。

Method: 引入MuaLLM，通过ReAct工作流程融合检索与推理，结合多模态处理能力，动态更新数据库，优化大规模推理能力。

Result: 在两个定制数据集上表现优异，检索召回率达90.1%，多步推理精度86.8%，在成本和速度方面优于传统模型。

Conclusion: MuaLLM显著提升电路设计研究的文献信息整合与推理效率，为电路设计提供强大支持。

Abstract: Conducting a comprehensive literature review is crucial for advancing circuit
design methodologies. However, the rapid influx of state-of-the-art research,
inconsistent data representation, and the complexity of optimizing circuit
design objectives make this task significantly challenging. In this paper, we
propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for
circuit design assistance that integrates a hybrid Retrieval-Augmented
Generation (RAG) framework with an adaptive vector database of circuit design
research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +
Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step
information retrieval. It functions as a question-answering design assistant,
capable of interpreting complex queries and providing reasoned responses
grounded in circuit literature. Its multimodal capabilities enable processing
of both textual and visual data, facilitating more efficient and comprehensive
analysis. The system dynamically adapts using intelligent search tools,
automated document retrieval from the internet, and real-time database updates.
Unlike conventional approaches constrained by model context limits, MuaLLM
decouples retrieval from inference, enabling scalable reasoning over
arbitrarily large corpora. At the maximum context length supported by standard
LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining
the same accuracy. This allows rapid, no-human-in-the-loop database generation,
overcoming the bottleneck of simulation-based dataset creation for circuits. To
evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval
and citation performance, and Reasoning-100 (Reas-100), focused on multistep
reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%
accuracy on Reas-100.

</details>


### [255] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: 提出FairFLRep，一种自动化的公平性感知故障检测与修复技术，有效提升神经网络的公平性并保持精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在实际应用中存在偏见问题，影响公平性与决策准确性。

Method: 分析输入输出关系，定位偏差神经元，调整敏感属性相关的神经元权重，以修正偏差。

Result: 在多种数据集和模型上，FairFLRep优于现有方法，实现了更好的公平性和效率。

Conclusion: FairFLRep有效平衡了公平性与准确性，为偏差修复提供了一种自动化解决方案。

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [256] [Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets](https://arxiv.org/abs/2508.08159)
*Cem Ata Baykara,Saurav Raj Pandey,Ali Burak Ünal,Harlin Lee,Mete Akgün*

Main category: cs.LG

TL;DR: 采用联邦学习策略提升多源异构EEG数据的癫痫发作预测性能，特别通过随机子集聚合方法解决数据偏差问题，提高模型的全面性和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决不同临床站点EEG数据异质性和隐私限制带来的癫痫发作预测模型开发困难。

Method: 引入隐私保护的全局归一化和随机子集聚合策略，利用单一EEG通道数据在多数据源上训练模型。

Result: 新策略显著改善模型在边缘数据集的性能，实现了更公平和稳健的全局模型。

Conclusion: 平衡联邦学习方法在多源、多患者环境中有助于构建高效且公平的癫痫发作预测系统。

Abstract: Developing accurate and generalizable epileptic seizure prediction models
from electroencephalography (EEG) data across multiple clinical sites is
hindered by patient privacy regulations and significant data heterogeneity
(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving
framework for collaborative training, but standard aggregation methods like
Federated Averaging (FedAvg) can be biased by dominant datasets in
heterogeneous settings. This paper investigates FL for seizure prediction using
a single EEG channel across four diverse public datasets (Siena, CHB-MIT,
Helsinki, NCH), representing distinct patient populations (adult, pediatric,
neonate) and recording conditions. We implement privacy-preserving global
normalization and propose a Random Subset Aggregation strategy, where each
client trains on a fixed-size random subset of its data per round, ensuring
equal contribution during aggregation. Our results show that locally trained
models fail to generalize across sites, and standard weighted FedAvg yields
highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on
Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation
significantly improves performance on under-represented clients (accuracy
increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior
macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,
demonstrating a more robust and fair global model. This work highlights the
potential of balanced FL approaches for building effective and generalizable
seizure prediction systems in realistic, heterogeneous multi-hospital
environments while respecting data privacy.

</details>


### [257] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: 提出一种改进的神经逻辑网络模型，结合NOT操作和偏置，增强可解释性与概率建模，提升布尔网络发现能力，并在医疗数据中验证其应用潜力。


<details>
  <summary>Details</summary>
Motivation: 目前的神经网络虽分类性能优异，但缺乏可解释性和透明性，亟需具备逻辑可理解性的模型以满足实际应用需求。

Method: 在神经逻辑网络基础上扩展引入NOT操作和偏置，提出新的因式化IF-THEN规则结构及改进的学习算法，用于逻辑与概率的结合建模。

Result: 该方法在布尔网络发现方面优于当前技术，能学习出具有解释性的规则，特别在医学分类任务中展现出实际价值。

Conclusion: 改进的神经逻辑网络具备更强的可解释性和应用潜力，为实际需要透明决策的领域提供了有效工具。

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [258] [Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion](https://arxiv.org/abs/2508.08216)
*Nicole Lai-Tan,Xiao Gu,Marios G. Philiastides,Fani Deligianni*

Main category: cs.LG

TL;DR: 提出了一种新的个体切线空间对齐策略（ITSA）以增强脑机接口在跨个体应用中的表现，结合相近空间模式和几何匹配，显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 面临EEG信号个体差异大、校准时间长的问题，亟需更有效的泛化方法以支持个性化音乐干预。

Method: 开发ITSA策略，结合临近空间重心、分布匹配和监督旋转对齐，同时融合RCSP与黎曼几何，在不同配置下进行分类性能优化。

Result: ITSA在交叉验证中显著改善了跨个体和条件的分类性能，尤其是平行融合方案表现最佳，且具有数据变化和电极配置适应性。

Conclusion: ITSA方法有效提升BCI的跨个体泛化能力，为个性化干预工具的实用化提供技术基础，可公开代码促进研究推广。

Abstract: Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.

</details>


### [259] [Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)
*Tong Yang,Yu Huang,Yingbin Liang,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文通过理论分析，揭示了训练后的单层变换器在树路径查找任务中的推理机制，阐明了多头注意力如何自主协作学习解决多步推理问题的路径。


<details>
  <summary>Details</summary>
Motivation: 理解变换器获得多步推理能力的机制，特别是通过链式思考在符号树路径问题中的表现。

Method: 基于梯度下降动力学的理论分析，证明训练后的单层变换器可以解决路径查找任务，并分析多头注意力的专业化与协作机制。

Result: 证实训练好的单层变换器可以在未见过的树结构上泛化解决路径推理任务，阐明了多头注意力的自主专精和合作。

Conclusion: 变换器通过链式思考实现多步推理，即使是浅层多头结构也能在结构化任务中表现出强大的推理能力，提供了理解其推理机制的理论基础。

Abstract: Transformers have demonstrated remarkable capabilities in multi-step
reasoning tasks. However, understandings of the underlying mechanisms by which
they acquire these abilities through training remain limited, particularly from
a theoretical standpoint. This work investigates how transformers learn to
solve symbolic multi-step reasoning problems through chain-of-thought
processes, focusing on path-finding in trees. We analyze two intertwined tasks:
a backward reasoning task, where the model outputs a path from a goal node to
the root, and a more complex forward reasoning task, where the model implements
two-stage reasoning by first identifying the goal-to-root path and then
reversing it to produce the root-to-goal path. Our theoretical analysis,
grounded in the dynamics of gradient descent, shows that trained one-layer
transformers can provably solve both tasks with generalization guarantees to
unseen trees. In particular, our multi-phase training dynamics for forward
reasoning elucidate how different attention heads learn to specialize and
coordinate autonomously to solve the two subtasks in a single autoregressive
path. These results provide a mechanistic explanation of how trained
transformers can implement sequential algorithmic procedures. Moreover, they
offer insights into the emergence of reasoning abilities, suggesting that when
tasks are structured to take intermediate chain-of-thought steps, even shallow
multi-head transformers can effectively solve problems that would otherwise
require deeper architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [260] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 本文提出利用CUDA加速的计算框架模拟Pasur牌类游戏，并通过极大化利用内存和节点评估来求解近纳什均衡，为类似游戏提供了技术支撑。


<details>
  <summary>Details</summary>
Motivation: 解决Pasur游戏复杂的玩法和庞大的游戏树结构，探索利用深度学习和强化学习优化策略的方法。

Method: 采用PyTorch CUDA张量管理规则复杂的游戏状态，分解游戏树，并通过逐轮逆向训练策略，同时建模预测实用策略。

Result: 成功构建了含超10^9节点的完整游戏树，计算出近纳什均衡策略，并训练模型进行策略预测，验证了GPU加速模拟及策略优化的效果。

Conclusion: 该框架极大提升了处理复杂、不完全信息游戏的效率，可推广至多轮战术和序贯决策场景。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [261] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink是一个多智能体AI框架，旨在促进材料研究中的偶然发现，通过结合机器学习与大语言模型，实现数据分析、新颖性评估和理论模拟的自动化链接。


<details>
  <summary>Details</summary>
Motivation: 现代自动化实验室在加速假设验证方面表现出色，但可能忽视意外发现的重要性。

Method: 结合专业的机器学习模型进行定量分析与大语言模型进行高层次推理，构建一个能自动将实验数据转化为科学命题并评估新颖性的多智能体系统。

Result: 展示了其在多种材料数据分析中的应用能力、与专家实时互动的功能及提出后续实验的能力，推动自动化和开放式探索的结合。

Conclusion: SciLink成功桥接了自动实验与开放式科研，为AI驱动的材料研究提供了促进偶然发现的实用框架。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [262] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 提出了一种结合逆强化学习和自主驾驶的闭环VLA模型，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型多在开环模仿学习中，性能受限，缺乏高效闭环训练方法。

Method: 引入IRL-VLA框架，包括预训练VLA策略、构建轻量级奖励世界模型、利用PPO进行强化学习优化。

Result: 在NAVSIM v2和CVPR2025竞赛中达成了最佳表现和优异排名，验证了方法的有效性。

Conclusion: 该方法有助于推动闭环自主驾驶VLA模型的发展，提升安全性与效率。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [263] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: CountQA是一个针对多模态大语言模型在复杂场景下物体计数能力的基准测试，揭示当前模型在高密度、多遮挡环境中表现不足，有助于推动模型的数值理解能力发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在视觉理解方面表现优异，但在物体计数这一基本认知技能上存在明显缺陷，缺乏真实复杂场景下的评估。

Method: 设计了CountQA基准，包含1500多个真实图片上的问答对，评测15个主流模型的物体计数能力。

Result: 最优模型准确率仅达42.9%，且随着物体数量增加，性能明显下降。

Conclusion: CountQA揭示了模型在复杂环境中的不足，为未来提升数值理解和空间感知能力提供了测试平台。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [264] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 该论文探讨了形式概念分析（FCA）在变异性提取和分析中的应用潜力，强调了其在组织和理解复杂对象变异性中的作用。


<details>
  <summary>Details</summary>
Motivation: 尽管FCA具有潜在优势，但其在变异性任务中的具体应用尚不明确，本研究旨在阐明FCA的哪些属性有助于变异性分析。

Method: 通过汇总FCA的关键属性，分析其在变异性信息解释中的作用，桥接基础理论与应用之间的差距。

Result: 识别了FCA中对变异性分析有帮助的属性，并阐明了它们在解读多样变异信息中的具体应用方法。

Conclusion: 本文确定了FCA在变异性提取与分析中的关键属性，为未来使用其进行复杂数据变异性研究提供理论基础。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [265] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 提出一种基于像素校准的零样本细胞轨迹匹配方法，结合地理知识和空间-temporal特征提升匹配精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在未覆盖区域适应性差，依赖标识数据的问题，提升无训练样本的轨迹匹配效果。

Method: 引入像素化轨迹校准、变分自编码器中的高斯混合模型、空间-temporal意识模块及约束路径搜索算法，实现轨迹校准和路径重建。

Result: 模型在零样本条件下比现有方法高出16.8%的性能，显著提升匹配精度。

Conclusion: 该方法有效利用地理和时间特征，增强区域适应性，并改善轨迹匹配的准确性。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [266] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 提出一种基于规则上下文的概率图模型，有效减少规则数量，同时提升知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 解决规则基方法在知识图谱补全中规则集过大影响可解释性的问题。

Method: 从训练数据中发现规则上下文，利用概率电路对其概率分布进行建模，实现快速性能提升。

Result: 规则数量大幅减少（70-96%），性能超越基线，最少规则集仍保持91%的性能，并在8个数据集上验证效果优异。

Conclusion: 此方法结合概率逻辑语义，无需独立假设，提供边界和精确概率，具有广泛的推理应用潜力。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [267] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种改进的可微规则学习方法，能处理更复杂的逻辑规则结构，显著优于现有方法，并可与深度学习结合。


<details>
  <summary>Details</summary>
Motivation: 弥补现有逻辑规则学习方法在规则复杂度和表达能力方面的不足，提高知识图谱补全和节点分类的性能。

Method: 提出基于可微消息传递的推理算法，支持多样化规则结构，设计简洁的规则搜索空间并能提取逻辑规则。

Result: GLIDR在知识图谱补全任务中优于现有方法，能与深度网络联用，具有鲁棒性。

Conclusion: GLIDR增强了规则表示能力，并在推理性能和实用性方面表现优异，可推广至多模态数据。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [268] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 通过引入ParBalans，结合多层次并行策略，有效提升大规模混合整数规划问题的求解效率。


<details>
  <summary>Details</summary>
Motivation: 解决MIP问题计算资源消耗大，寻求提高求解速度和可扩展性。

Method: 扩展Balans，通过结合求解器和算法的并行策略，开发出ParBalans。

Result: ParBalans在挑战性MIP实例上表现优越，接近甚至优于商业顶尖求解器Gurobi的性能。

Conclusion: 多层次的并行探索策略显著提升了解决大规模复杂MIP问题的能力，验证了其有效性。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [269] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 提出一种结合图扩散策略优化和Stackelberg博弈的自组织无人机网络框架，用于提升连接可靠性和隐蔽通信能力。


<details>
  <summary>Details</summary>
Motivation: 随着无人机网络在敏感应用中的需求增加，如何确保其连接可靠性和隐蔽性成为关键挑战。

Method: 采用图扩散策略优化结合生成式AI动态生成拓扑结构，使用Stackelberg博弈引导无人机行为以促进合作。

Result: 通过大量实验验证，该框架在模型收敛、拓扑生成质量和隐蔽通信性能方面表现良好。

Conclusion: 提出的方法有效应对无人机网络中的动态性和安全性挑战，提升网络整体性能。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [270] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 本论文设计优化的1-bit和2-bit微核，显著提升超低比特LLM模型的推理性能，达到比现有SOTA runtime更快的速度和效率。


<details>
  <summary>Details</summary>
Motivation: 随着超低比特LLM模型的发展，其在边缘设备等资源受限环境中的应用需求日益增长，但现有推理运行时的计算效率尚未充分挖掘。

Method: 开发具备极高性能的1-bit和2-bit微核，并将其集成进PyTorch-TPP推理框架，进行端到端性能优化。

Result: 提出的微核显著提高推理速度，与现有最先进的bitnet.cpp runtime相比提升多达2.2倍，16-bit模型的推理速度也提升至7倍。

Conclusion: 本研究通过微核优化极大改善了超低比特LLM模型在AI PC和边缘设备上的推理效能，有助于未来高效部署。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [271] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 提出一种模块化提示架构，基于人类学习理论（ZPD），提升LLM在动态任务中的安全性和适应性，特别是在教育和游戏内容生成中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在动态、用户中心任务中缺乏安全性和适应性的问题。

Method: 结合边界提示、模糊脚手架逻辑和适应规则，设计模块化提示框架，模拟人类学习中的ZPD原则。

Result: 在模拟教学环境中，该框架提升了脚手架质量、适应性和教学一致性，优于传统提示方法，且在游戏内容生成等其他互动密集领域具有潜力。

Conclusion: 该框架为安全、可调节的LLM行为提供了可重用的方法，适用于不确定或变化的场景，促进模型在实际应用中的安全与效果。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [272] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 该论文提出了利用自然语言交互和强化学习自动选择最佳视点以改善3D体数据探索的方法，增强科学数据解析的效率与解释性。


<details>
  <summary>Details</summary>
Motivation: 在科学数据的三维可视化中，选择合适的观察视点具有挑战性，尤其对于非专业用户。

Method: 结合体块编码、CLIP得分机制及强化学习框架，实现语义引导的视点自动搜索。

Result: 该方法提升了体数据导航效率和复杂科学现象的可解释性。

Conclusion: 自动化视点选择技术有助于非专业用户更有效理解复杂的科学数据。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [273] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 提出了一种以语言为核心的遥感解读新范式，借鉴人类认知的全球工作空间理论，将大规模语言模型作为认知中枢，增强模态融合、推理与决策能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉中心模型在多模态推理和语义抽象上的局限，强调语言在遥感解读中的潜力。

Method: 引入全球工作空间理论，构建以语言为核心的认知框架，分析核心技术挑战并提出对应机制。

Result: 提出语言中心的遥感解读框架，识别关键技术难点，规划未来研究方向，包括多模态对齐、知识约束下的任务理解、可信推理及自主交互。

Conclusion: 为遥感解读系统提供认知驱动的理论基础，指引未来智能地理空间分析的发展路径。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [274] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 提出了一种多层次优势信贷分配（MACA）方法，用于多智能体强化学习中的复杂信用分配问题，通过多层次优势函数和注意力机制提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中不同合作层级下的信用分配难题，提升多智能体协作效果。

Method: 引入多层次优势函数，结合显式反事实推理和注意力机制，识别相关智能体关系并进行多层次信用评估。

Result: 在Starcraft任务中实验证明，MACA显著优于现有方法，有效应对复杂的信用分配场景。

Conclusion: MACA通过多层次优势建模和注意力机制，提升多智能体系统在复杂任务中的合作效率与表现。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [275] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 本文提出了MDK12-Bench，这是一个基于K-12考试的多学科大规模基准，用于综合评估多模态大型语言模型的理解和推理能力，通过动态评估方法提升评估的全面性和客观性，探讨知识点增强生成对模型的影响。


<details>
  <summary>Details</summary>
Motivation: 弥补现有多模态大模型评估不足，提供更全面、动态和真实世界的评估工具。

Method: 构建包含六学科、多样题型、难度和年份标注的MDK12-Bench，设计动态评估框架引入新颖的视觉、文本和问题形式变化，结合知识点增强生成方法。

Result: 发现当前模型在多个方面存在不足，验证了新基准和方法在提升模型鲁棒性和解释性方面的潜力，为未来模型优化和教育应用提供指导。

Conclusion: MDK12-Bench及其评估策略有助于推动多模态模型的性能提升，特别是在复杂理解和推理任务中的应用，为AI教育和智能系统发展提供有价值的工具。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [276] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 提出了一种面向极端天气预测的全新大规模多模态数据集MP-Bench和面向预测的多模态大模型MMLM，突破了现有模型在高维气象数据处理上的局限，显著提升了天气预警自动化水平。


<details>
  <summary>Details</summary>
Motivation: 当前天气预警系统依赖人工解读，存在主观性和效率问题，亟需自动化、智能化的解决方案。

Method: 构建包含多样极端天气场景的MP-Bench数据集，并开发可直接处理4D气象数据的多模态大模型MMLM，，通过引入三种适配融合模块，实现对时间、垂直压力层和空间维度的动态特征融合。

Result: 在MP-Bench上进行的广泛实验显示，MMLM在多项极端天气理解任务中表现优异，验证了其在气象灾害预测中的潜力，推动了AI驱动的天气预警自动化发展。

Conclusion: 本文提出的多模态数据集和模型为天气预警的自动化和智能化奠定了基础，为未来气象预测技术提供了重要的研究资源和技术路径。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [277] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 提出了一种基于确定性下推自动机的奖励机器（pdRM），比传统奖励机器更具表达能力，能识别和奖励确定性上下文无关语言表达的行为。


<details>
  <summary>Details</summary>
Motivation: 扩展奖励机器的表达能力，以支持更复杂的行为表示，提升强化学习的效果。

Method: 基于确定性下推自动机设计pdRM，提出两种策略变体，分析其最优性和表达能力，并进行理论和实验验证。

Result: 证明pdRMs具有更强的表达能力，分析两种策略的最优性条件，验证其在强化学习中的应用效果。

Conclusion: pdRMs比传统奖励机器更具表达力，有助于训练能够执行复杂行为的强化学习代理。

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [278] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出了一种新颖的分布式引导局部搜索（DGLS）框架，有效解决了现有GDBA在DCOP中的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为改善局部搜索在解决分布式约束优化问题中的局部最优陷阱问题，分析了GDBA的不足并提出改进方法。

Method: 引入自适应违反条件、惩罚蒸发机制和同步方案，理论证明其有界性与潜在博弈结构。

Result: 在多项标准基准测试中，DGLS优于现有最优基线算法，在结构化问题中表现出显著的性能提升（3.77%-66.3%）。

Conclusion: 通过机制创新，有效缓解GDBA的缺陷，使得分布式约束优化算法在实际应用中表现更优。

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [279] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: 提出了一种基于概念检索和增强的自动形式化方法CRAMF，有效改善了大规模语言模型在数学自动形式化中的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决自动形式化中模型幻觉和语义鸿沟问题，提升自动化程度。

Method: 构建包含大量数学定义的知识库，采用上下文查询增强和双通道检索策略，集成于大模型中实现高精度定义检索。

Result: 在多个数学验证基准中实现了显著提升，最高提高到62.1%，平均提升29.9%。

Conclusion: CRAMF有效结合概念知识，显著改善了数学自动形式化的准确性和可靠性，有助于推动互动定理证明工具的自动化发展。

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [280] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: 该研究利用Transformer模型的内在可解释性，分析多模态学习网络在作物产量预测中的表现及其解释方法，发现Transformer优于其他架构，Attention Rollout在时间特性解释中表现较优，同时结合农业知识增强理解。


<details>
  <summary>Details</summary>
Motivation: 提升多模态学习模型的可解释性，以便在农业应用中更好理解模型决策过程。

Method: 采用Transformer模型，结合Attention Rollout和Generic Attention等方法进行特征归因分析，并用Shapley值进行对比，同时提出Weighted Modality Activation评估模态贡献。

Result: Transformer模型在作物产量预测中优于卷积和递归网络，Attention Rollout在时间特性归因中表现更优，结合农业知识解读结果。

Conclusion: 基于Transformer的多模态模型具有优越的解释能力，Attention Rollout方法稳定可靠，结合农艺知识有助于理解模型。

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [281] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: LLMs不能模拟人类心理，应谨慎使用于心理学研究。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否可以替代人类参与心理学研究的可能性。

Method: 通过理论分析和实验证明，即使是经过微调的模型，也在表达微小变化时表现出显著差异。

Result: 不同模型对新颖项目反应差异大，表明其不可靠。

Conclusion: LLMs不等同于人类心理，应作为辅助工具并验于人类反应。

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [282] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 该论文提出了DatasetResearch基准，用于评估AI在自动发现和合成数据集方面的能力，结果显示目前系统只达到22%的表现，指出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 解决数据获取瓶颈，推动AI自主数据搜集与合成。

Method: 设计多维度评估框架，评测AI在208个真实需求中的表现。

Result: AI系统整体表现有限，尤其在边缘案例上表现差强人意。

Conclusion: 提出了数据集发现的基础标准和未来路径，促进AI自主数据管理的进步。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [283] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: 提出MASteer框架，通过自动生成样本和自适应策略提升大规模语言模型的可信性修复效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大模型可信性问题存在成本高、效率低、鲁棒性不足等挑战，促使开发自动化、灵活的修复方法。

Method: 集成多智能体生成高质量样本的AutoTester以及自适应策略制定的AutoRepairer，基于表示工程进行推断时自动选择策略。

Result: 在标准和定制任务中表现优越，提升模型性能指标，具有良好的鲁棒性和实际应用价值。

Conclusion: MASteer为大模型可信性修复提供了有效、自动化、可扩展的解决方案，兼顾模型能力和修复效率。

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [284] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse是一个分布式机器学习推理的模块化框架，结合战略密码验证，实现目标性部分推理验证，减少全模型电路化的成本。


<details>
  <summary>Details</summary>
Motivation: 旨在解决当前全模型电路化成本高昂和缺乏灵活性的问题，提升分布式机器学习推理的验证效率。

Method: 设计一个支持目标验证部分（切片）和多系统评估的架构，通过零知识证明对模型不同部分进行验证，并强化全局一致性。

Result: 在多证明系统下评估了性能，包括内存、运行时间和电路行为，验证了架构的可扩展性和灵活性。

Conclusion: DSperse实现了目标导向的高效验证策略，满足不同部署场景的需求，推动分布式机器学习的可信性发展。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [285] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: 本文提出基于主动推断的模型，用以模拟具備記憶和預測能力的自主代理决策过程，结合生物神经网络，旨在增強可解釋性和生物合理性。


<details>
  <summary>Details</summary>
Motivation: 探索生物学基础的智能行为模型，为自主代理提供更具解释性和生物合理性的决策机制。

Method: 利用主动推断框架和实验指导的生成模型，在模拟实验环境中研究决策过程。

Result: 展示了代理的学习能力，揭示了记忆和预测在智能决策中的作用。

Conclusion: 提出一种生物基础、可扩展的行为理解方法，推动可解释 AI 的发展。

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [286] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: 本文探讨了隐式击中集（IHS）方法的替代算法，包括伪布尔（PB）推理和随机局部搜索在击中集优化中的应用，提出了在效率和可靠性之间的权衡，强调PB推理在确保计算正确性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 为了改善IHS方法中击中集优化的效率和可靠性，探讨了不同的算法技术和推理方式。

Method: 结合伪布尔推理和随机局部搜索技术，分析其在IHS中的应用，并通过实验证明PB推理在确保结果正确性方面的优势。

Result: 发现商业IP求解器虽高效，但存在数值不稳定的问题；使用PB推理可实现与IP求解器竞争的准确性，并提供证明算法正确性的证书。

Conclusion: 伪布尔推理为IHS击中集计算提供了一种可靠且具证明能力的方法，值得在凸显准确性需求的应用中推广。

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [287] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: 提出MultiMedEdit，为临床多模态任务中的知识编辑提供评测基准，揭示现有方法在复杂场景下的局限性，并分析实际应用中的性能折衷。


<details>
  <summary>Details</summary>
Motivation: 弥补目前在医疗多模态场景中知识编辑研究的不足，推动临床应用中的安全性和可解释性。

Method: 构建多模态医学知识编辑基准，定义多维度评价指标，进行单次与终身编辑实验，并进行效率分析。

Result: 显示现有方法难以应对复杂临床流程中的泛化和长尾推理，揭示实际部署中的性能折衷。

Conclusion: MultiMedEdit为未来临床稳健的知识编辑技术提供基础，但当前方法仍有提升空间。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [288] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: 提出K-Dense Analyst，一个多智能体系统，用于实现自主生物信息分析，显著优于现有大语言模型，推动自动化科研进程。


<details>
  <summary>Details</summary>
Motivation: 弥补大语言模型在复杂生物信息分析中的不足，提升自主科研能力。

Method: 采用双环结构的分层多智能体系统，结合规划与执行，细化任务，确保验证与安全。

Result: 在BixBench基准测试中，K-Dense Analyst达成29.2%的准确率，优于GPT-5和其他强大模型，展示其超越模型本身能力的潜力。

Conclusion: 自主科学推理须依赖定制系统，而非仅依赖增强的语言模型，为未来自主生物信息学分析奠定基础。

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [289] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: 本文提出一个评估大模型在内容监管中的表现的框架，使用包含多类别的数据集和改进型模型，发现模型在伦理敏感内容检测中仍存在不足，强调增强数据多样性和人类参与的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统融入日常生活，对更安全可信的内容监管需求增加，但现有大模型在微妙的伦理判断上仍存在不足。

Method: 构建包含49类的统一基准数据集，开发SafePhi模型以改进伦理内容检测能力，并对比现有模型性能。

Result: SafePhi模型在伦理内容检测中表现优越，取得了0.89的Macro F1成绩，优于OpenAI Moderator和Llama Guard。

Conclusion: 需引入更多异质性数据及人类干预，提升模型的鲁棒性和解释性，以改善大模型的伦理内容监管能力。

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [290] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: 提出一种反馈驱动的决策支持系统，通过增量学习不断优化学生表现预测模型，提高预测精度并确保模型透明度。


<details>
  <summary>Details</summary>
Motivation: 现有教育机器学习模型多为静态，无法适应新数据，影响预测准确性与实用性。

Method: 采用LightGBM回归器结合增量重新训练，集成Flask平台和SHAP解释性工具，实现动态模型更新和透明性。

Result: 系统在重新训练后RMSE降低10.7%，预测更加准确，整体预测系统更具适应性和透明度。

Conclusion: 该系统将静态预测模型转变为自我优化的动态系统，推动教育数据分析朝着以人为中心的智能方向发展，可集成进LMS和机构仪表盘。

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [291] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型的多维企业数据总结新框架，通过多智能体流程提高总结的准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 传统表格到文本模型在处理层次结构和上下文变动方面不足，限制在商务报告中的应用。

Method: 利用多个智能体进行数据切片、变异检测、上下文构建与生成，以实现多维数据的准确总结。

Result: 新框架在数据忠实度、重要变动覆盖和相关性方面优于传统方法，特别在复杂权衡场景表现突出。

Conclusion: 该方法显著改善企业数据总结的效果，可广泛应用于实际商务分析中。

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [292] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: 提出了一种基于记忆引导的端镜分析AI代理EndoAgent，结合短期和长期记忆实现复杂决策与工具整合，辅以EndoAgentBench评测系统，展现出优异的多任务处理与推理能力。


<details>
  <summary>Details</summary>
Motivation: 弥补现有大规模预训练方法在多任务协调和复杂临床工作流程中的不足，以及探索AI在内镜分析中的潜力。

Method: 构建双重记忆设计的EndoAgent，结合迭代推理、工具选择和协作，集成专家工具以支持多样临床任务，并开发EndoAgentBench基准测试。

Result: EndoAgent在多任务、推理和语言生成等方面优于现有模型，表现出强大的灵活性和推理能力。

Conclusion: EndoAgent有效提升端镜分析中的智能决策和任务适应性，为临床AI应用提供新范例。

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [293] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [294] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 提出一种基于全面性与紧凑性原则的迭代基准框架Comp-Comp，突破规模法在特定领域构建中的局限，验证于学术领域的XUBench案例。


<details>
  <summary>Details</summary>
Motivation: 当前领域特定基准多依赖规模法，但其对语料和问答集设计的影响还未充分探究。

Method: 提出Comp-Comp框架，强调语料和问答集的全面性与紧凑性，进行迭代优化。

Result: 在知名大学案例中成功构建了XUBench，验证框架的有效性。

Conclusion: Comp-Comp框架不仅提升基准构建的针对性和精度，还具有很强的拓展潜力，超越单一学术场景。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [295] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: Pentest-R1通过两阶段强化学习提升大模型渗透测试能力，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 自动化渗透测试对 cybersecurity 关键，但现有大模型存在错误处理不足、推理效率低、复杂任务能力差的问题。

Method: 利用真实多步攻击路径构建数据集，通过离线强化学习灌输攻击逻辑，再在CTF环境进行在线强化学习，提升模型自我纠错和策略适应能力。

Result: 在Cybench和AutoPenBench上显著优于大部分模型，成功率达24.2%和15.0%，达到了开源模型的新高，与顶级私有模型看齐。

Conclusion: 两阶段训练（离线和在线）协同作用是提升模型性能的关键，验证了此框架的有效性。

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [296] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: 引入逆任务框架增强视频定位与语义理解，提升TVG性能。


<details>
  <summary>Details</summary>
Motivation: 当前TVG方法过于优化IoU指标，忽视语义理解，影响鲁棒性。

Method: 通过Verb Completion、Action Recognition、Video Description三项逆任务结合强化学习优化。

Result: 在Charades-STA数据集上提升显著，R1@0.7提升7.1%。

Conclusion: 逆任务增强方法提升了TVG的语义理解和定位精度，超越现有方法。

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [297] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 本文提出一种模块化模型，利用大语言模型和话题模型辅助制定政府战略计划，验证了BERTopic与NMF在生成符合战略愿景的主题方面的有效性，显示BERTopic表现优异，具有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能和大语言模型的突破，传统专业服务正逐步融合AI技术，尤其是在政府战略规划中的应用亟需探索与验证。

Method: 采用BERTopic和非负矩阵分解（NMF）对政府审计报告进行话题建模，并将生成的主题与现有战略计划中的愿景元素进行相似度评分，以评估模型性能。

Result: 两种模型都能生成与战略愿景元素高度相关的主题，BERTopic表现更佳，超过半数话题达中等或强相关水平，显示出其在战略规划中的潜在价值。

Conclusion: G较的能力在战略规划中的融合应用具有巨大行业影响，未来将进一步研究模型的实际操作性及其他模块的效用，以推动AI在政府策略制定中的普及与落实。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [298] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 本文综述了自我进化AI代理系统的设计框架和技术，强调其适应性与安全性，为未来自适应、自治代理系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大模型的发展，传统静态的AI代理难以应对变化的环境，发展自我进化技术成为提升代理系统适应性的重要方向。

Method: 本文提出统一的反馈循环框架，并系统性综述了不同组件的自我进化技术，涵盖泛领域和专业领域的策略，同时讨论了评估、安全与伦理问题。

Result: 明确了自我进化AI代理的理论基础和技术路径，分类整理了相关方法，提供了发展方向，为建立更智能、自治的代理系统提供指导。

Conclusion: 自我进化AI代理是未来AI发展的关键，通过系统框架与安全伦理的考虑，将推动更具适应性和自主性的人工智能系统的实现。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [299] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文提出了一种扩展大型语言模型的多智能体决策框架，强调先进的提示工程、记忆架构、多模态信息处理和微调策略，旨在提升多智能体合作与沟通能力，经过游戏环境测试验证。


<details>
  <summary>Details</summary>
Motivation: 推动多智能体系统中的语言交流与合作，提升决策与协作效率。

Method: 设计了系统框架，采用先进的提示工程、记忆架构、多模态处理和微调策略，结合经典游戏环境进行消融实验。

Result: 验证了该设计在多智能体决策中的有效性，特别是在社会困境和博弈论场景下表现良好。

Conclusion: 多智能体语言模型结合多种技术能有效增强智能体间的沟通与合作，为复杂环境中的智能体协作提供了新思路。

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [300] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出一种基于ReAct原则的通用Python编码代理，有效解决约束编程中的自然语言转正式模型问题，突破了传统流程的限制。


<details>
  <summary>Details</summary>
Motivation: 自动化构建约束模型依赖专家技能，现有方法在复杂问题上表现不足。

Method: 基于ReAct原理构建Python编码代理，通过动态测试与调试实现问题求解，强调知识注入和工具整合。

Result: 在CP-Bench基准测试中成功解决所有101个问题，验证方法的有效性和通用性。

Conclusion: 结合通用编程工具和提示编码的域知识，是解决复杂约束建模问题的关键，而无需固定流程或特殊架构。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [301] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 首次提出可直接使用即开箱大型语言模型(LLMs)进行完整Diplomacy游戏的评估方法，不需微调或特殊训练。


<details>
  <summary>Details</summary>
Motivation: 解决因Diplomacy游戏复杂性导致传统LLMs难以直接进行策略评估的问题。

Method: 通过数据驱动的迭代优化文本状态表示，开发支持多模型的评估工具，使用Critical State Analysis分析关键游戏时刻。

Result: 大模型表现优异，小模型亦能应对；提出Critical State Analysis推动策略研究；代码开源，简化策略推理评估。

Conclusion: 所提出的方法 democratizes 了LLMs在策略推理中的评估流程，展示了广泛模型自然表现出复杂策略能力的潜力。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [302] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: 本文提出了MCPToolBench++，一个用于评估大型语言模型（LLMs）在调用模型上下文协议（MCP）工具中的能力的多领域基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏全面的评估数据集和工具调用成功率的不确定性，亟需专业化的评测工具以衡量LLMs的实际使用能力。

Method: 利用包含超过4000个MCP服务器的大型数据集，结合多领域任务设计，评估最先进的LLMs在单步和多步工具调用中的表现。

Result: 评测结果显示不同模型在调用效率与成功率方面存在差异，也验证了该基准的实用性和多样性。

Conclusion: MCPToolBench++有助于推动LLM在实际应用中对MCP工具的调用能力的提升与评估，为未来优化提供参考。

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [303] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 提出了一种新颖的隐秘语义通信框架，通过“优先采样辅助的双迟滞深度确定性策略梯度”算法，优化语义信息和发射功率，显著提升隐私和传输质量。


<details>
  <summary>Details</summary>
Motivation: 在确保语义信息隐私的同时，提高传输质量，解决传统方法在隐私保护和优化上的不足。

Method: 采用优先采样辅助的双延迟深度强化学习算法，动态优化语义信息和发射功率，且不依赖服务器和干扰器之间的通信。

Result: 算法能将隐私保护和信息传输质量提升分别达77.8%和14.3%，优于传统强化学习方法。

Conclusion: 提出的联合优化算法有效增强隐私保护和传输性能，为隐秘语义通信提供了新的解决方案。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [304] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: 提出一种分层高斯混合模型框架（HGMF）以高效准确地从大规模工具库中筛选工具，提升LLMs实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 面对大规模工具库中工具选择难题，需提升筛选的准确性与效率。

Method: 采用基于高斯混合模型的层级聚类与过滤，结合语义空间映射，实现工具的层级筛选。

Result: 实验表明HGMF显著提升工具选择的准确率，减少推理延迟，验证其对大规模工具库的可扩展性和有效性。

Conclusion: HGMF提供了一种高效、准确的工具筛选方法，适用于复杂的工具调用场景，有助于增强LLMs在实际任务中的表现。

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [305] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: 提出ThinkTuning方法，通过教师指导增强学生模型的推理能力，比传统训练方式有效提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有RL方法不足，不能从根本上培养模型的思考能力的问题。

Method: 引入基于GRPO的交互式训练，在教师模型指导下，通过反馈优化学生模型。

Result: 提高模型推理能力，平均提升3.85%，在多个基准测试中表现优异。

Conclusion: 教师指导的Implicit supervision能有效促进模型思考能力的发展，是改进模型推理的重要途径。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [306] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: 多模态AI技术通过融合视觉、声音、环境和生理数据，提升蛋鸡福利监测的科学性和效率，推动智能养殖未来发展。


<details>
  <summary>Details</summary>
Motivation: 传统福利评估方式受限于人工观察和单一传感器，无法全面反映蛋鸡福利的复杂性。

Method: 提出基于中间（特征层）融合的多模态AI模型，并开发域迁移评分和数据可靠性指标，设计模块化的系统部署框架。

Result: 中间融合策略在真实养殖环境中表现最佳，系统具备较好的可扩展性和适应性，提出的工具和框架增强了模型的跨场适应能力和数据质量评估能力。

Conclusion: 多模态AI结合创新评估工具和部署框架，有助于实现以数据驱动的预防性动物福利体系，兼顾生产效益与动物伦理。

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [307] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav引入结构化技能推理，改善视觉语言导航中在未知场景的表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLN方法难以泛化到未见环境，尤其在复杂空间和时间推理方面存在挑战。

Method: 提出SkillNav框架，分解导航任务为可解释的基础技能，由专门的代理执行，并通过新颖的零-shot视觉-语言模型路由器动态选择代理。

Result: 在R2R基准测试中实现了新的最佳性能，并在包括新指令风格和未知环境的GSA-R2R基准测试中表现出强泛化能力。

Conclusion: 引入技能基础的结构化推理增强了VLN代理的可解释性和泛化能力，推动了复杂环境下的导航技术发展。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [308] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: 提出了一种基于解缠和多层次空间-时间转换图的社交增强POI推荐模型DiMuST，有效提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间和时间转换建模上存在不同步问题，导致信息冗余和模型不确定性，影响解释性。

Method: 设计了解缠变分多层空间-时间图自编码器，利用PoE机制融合共享特征，并通过对比约束去噪私有特征。

Result: 在两个数据集上实验显示，DiMuST显著优于现有方法。

Conclusion: 通过解缠表示学习，有效提升了POI推荐的效果和模型的解释能力。

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [309] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: 多智能体框架通过任务分解和信息流控制，有效减少了大语言模型在多源信息处理中的隐私泄露问题，实现了比单智能体更优的隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 在多源信息交互中保障隐私是个挑战，需设计更有效的隐私保护机制。

Method: 提出多智能体架构，将隐私推理任务细分，通过信息流设计减少泄露风险，并进行系统性消融分析验证漏泄路径。

Result: 在两个基准测试中，优化配置显著降低隐私泄露率（18%和19%），同时保持公共内容的内容完整性，优于单智能体方法。

Conclusion: 基于信息流设计的多智能体系统在保护上下文隐私方面展现出巨大潜力，有助于提升大模型处理多源信息时的隐私安全水平。

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [310] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: EMPATHIA通过多模块和多代理人框架，实现对难民融入的文化、情感和道德维度的考虑。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法只优化狭窄目标，忽视了融入的多维度复杂性，需更全面的解决方案。

Method: 引入基于Kegan理论的多模块架构，包括SEED、RISE和THRIVE，通过多代理人合作实现多价值融合。

Result: 在UN Kakuma数据集上的实验显示验证收敛率达87.4%，实现可解释、多国家适用的难民融入评估。

Conclusion: EMPATHIA提供了一种可推广的AI框架，有助于在多价值系统中平衡和支持人类决策。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [311] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: 提出了一种扩展的Anytime2vec方法，用于将代理行为映射到多变量向量空间，以评估其与人类伦理价值的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统中伦理价值对齐的难题，特别是应对不可比的价值观问题。

Method: 将自动智能体的决策策略映射到多变量向量空间，并通过向量比较评估伦理价值的对齐。

Result: 提出的Ethics2Vec方法有效实现了对自动决策系统伦理价值的量化评估，适用于二元决策和自动控制场景。

Conclusion: 该方法为AI伦理价值的形式化衡量提供了一种可行途径，有助于推动伦理安全的人工智能系统开发。

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [312] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出一种对比学习目标增强变换器在自动化规划中的表现，解决问题对称性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 变换器在自动化规划应用中表现有限，尤其在从简单到复杂问题的推展中遇到困难。

Method: 引入对比学习目标，使变换器具有对称性感知能力，并结合结构改进进行训练。

Result: 实验验证新方法有效改善变换器在多个规划领域的性能，克服了PlanGPT的局限。

Conclusion: 对称性感知的训练策略提升了变换器在自动化规划中的泛化能力和效率。

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [313] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 提出一种新的RMDP策略选择标准，称为ORBE，结合最差和期望收益，确保在最优鲁棒策略中的优越性。


<details>
  <summary>Details</summary>
Motivation: 解决RMDP中存在多个等效最优鲁棒策略导致的决策难题，提升策略的性能和选择合理性。

Method: 引入合作性和竞争性概念，定义ORBE策略，证明其存在性，分析结构，并设计算法实现。

Result: 证明了ORBE策略的存在性，描述其结构特征，并验证其优越性和实用性。

Conclusion: ORBE策略为RMDP中的鲁棒策略选择提供了合理的原则和有效的算法，有助于实现更优的决策效果。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [314] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出利用知识图谱实现应急医疗中的智能治疗建议，提升救援效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 随着救援需求的增加，急救人员需要快速、个性化的医疗支持，但现有系统难以提供实时、精准的医疗决策支持。

Method: 通过构建知识图谱，整合新采集的生命体征数据，为救援提供动态、智能的处理建议。

Result: 知识图谱实现了对急救现场信息的有效整合与分析，支持及时、个性化的医疗决策。

Conclusion: 知识图谱作为核心知识管理工具，为应急医疗提供了创新的解决方案，提升救援效率和治疗效果。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [315] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: 提出一种名为X-evolve的方法，通过演化解空间而非单一解，提高复杂优化问题的搜索效率，显著降低LLM调用成本。


<details>
  <summary>Details</summary>
Motivation: 当前结合LLMs与演化算法解决复杂优化问题时，通常演化单一解，导致高调用成本。

Method: 在X-evolve中，LLMs生成可调程序定义参数化的解空间，通过评分引导的搜索算法高效探索此空间，扩大搜索范围。

Result: 在三大优化难题中取得突破，包括Cap集的问题、图的独立集以及在线装箱问题，显著改善了性能和解决方案质量。

Conclusion: 通过演化解空间，显著提高搜索效率，能有效解决高维难题，降低计算成本。

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [316] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: 提出一种基于LSTM的短期轨迹预测和深度Q学习结合的机器人碰撞风险预估方法，有效减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在无通信环境下的碰撞风险预测与规避问题。

Method: 利用LSTM模型预测机器人未来位置，并结合DQN调整奖励以预示碰撞风险。

Result: 在有限采样频率下，显著降低碰撞次数并提升系统稳定性，适合嵌入式平台。

Conclusion: 该方法计算成本低，效果显著，为机器人自主避障提供有效方案。

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [317] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: 提出FEAT，一个多智能体AI框架，用于标准化及自动化法医死亡调查，有效验证并优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 解决中国等高产系统中法医工作人力短缺和诊断变异的问题，提升死亡鉴定的效率和准确性。

Method: 设计融合任务规划、证据分析、反思与结论合成的多智能体架构，采用工具增强推理和人机交互。

Result: FEAT在多地区、多案例验证中表现优异，与专家判断接近，提升微妙证据的识别能力，优于现有AI系统。

Conclusion: FEAT是首个专用于法医领域的LLM基础AI系统，可提供一致、可靠的死亡鉴定服务，结合AI效率与人类监管。

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [318] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: 提出了一种基于大型语言模型的、不确定性感知的Fedspeak解读框架，提升货币政策立场分类的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 以Fedspeak作为影响市场和政策的重要沟通工具，自动解析其隐含政策信号具有高应用价值和挑战性。

Method: 结合领域推理机制和动态不确定性解码模块，增强文本语义理解和预测置信度。

Result: 模型在货币政策立场分析任务中实现了最先进的性能，不确定性与模型误差显著相关。

Conclusion: 该框架有效提升Fedspeak解读的准确性和诊断能力，为金融分析和政策制定提供技术支持。

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [319] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究适合性问题在本体论中，确定满足给定正负样例的本体，涉及不同描述逻辑和查询类型，复杂度从coNP到2EXPTIME。


<details>
  <summary>Details</summary>
Motivation: 旨在探索通过本体论满足示例的拟合问题，推动本体学习和推理的理论基础。

Method: 提出特征化方法，分析在不同逻辑和查询语言下的拟合问题的复杂性。

Result: 获得了所有拟合问题的有效特征描述，确定了其在不同逻辑和查询语言中的计算复杂度，包括CONP和2EXPTIME完全。

Conclusion: 该研究揭示了本体拟合问题的复杂性边界，为未来本体学习提供了理论支持。

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [320] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: AdaptFlow通过基于自然语言的元学习实现LLM代理工作流的快速适应，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在复杂任务中的工作流适应性和扩展性。

Method: 采用层级优化的元学习框架，利用语言引导的微调机制实现任务迁移。

Result: 在问答、代码生成和数学推理等任务中表现优异，达到目前最优性能。

Conclusion: AdaptFlow证明了基于语言的元学习设计在多任务通用工作流中的有效性和潜力。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [321] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: 提出了一种基于Dempster-Shafer理论的开放世界信息融合方法，采用全否定信念变换，有效解决异构框架融合问题。


<details>
  <summary>Details</summary>
Motivation: 应对现实中多源多组织数据导致的异构框架融合难题。

Method: 引入融合任务判别、扩展框架、全否定机制实现信息融合。

Result: 在模式分类和Zadeh反例中表现优异，验证了方法的有效性。

Conclusion: FNBT方法满足理论性质，具有实际应用潜力。

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [322] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: 提出TeamMedAgents，通过整合人类合作的团队组成部分，提升大语言模型在医疗决策中的多代理协作效果，验证了心理学中的“Big Five”模型对系统设计的指导作用。


<details>
  <summary>Details</summary>
Motivation: 旨在将人类团队合作模型应用于多代理系统中，改善医疗决策的效率和准确性。

Method: 将六个核心团队合作组件作为模块实现于多代理架构中，结合不同任务需求进行配置，通过在八个医疗基准测试中评估性能表现，并通过消融实验分析各组件贡献。

Result: 在七个测试数据集上取得了显著改进，且通过消融实验发现不同任务和数据集需要不同的团队合作配置。

Conclusion: 系统性地将人类团队合作理论转化为多代理AI设计，为关键领域的合作智能系统提供理论基础和实践探索。

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [323] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: 提出一种无需标签的无监督防御方法BlindGuard，有效检测多智能体系统中的恶意行为。


<details>
  <summary>Details</summary>
Motivation: 解决现有监督防御方法在实际应用中缺乏普适性和依赖标签的问题。

Method: 建立分层智能体编码器与腐蚀导向检测器，通过学习正常行为实现恶意检测。

Result: 在多种攻击和通信模式下实现有效检测，优于监督方法，具有广泛适用性。

Conclusion: BlindGuard提供了一种无需标签的通用安全防御方案，适应性强。

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [324] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: 提出一种基于大规模语言模型的智能代理，自动将电力系统优化问题描述转化为可求解的数学模型，结合验证和修正提升解的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决传统L宽模型直接生成解存在的可行性和优化程度不足的问题。

Method: 结合领域特定提示与结构，使用LLM生成数学模型，通过验证和迭代修正确保可行性，应用在机组调度问题中。

Result: 生成了接近最优的调度方案，验证显示结合验证环节显著提升解的可靠性。

Conclusion: 融合AI与优化框架能有效实现从高层次描述到可执行模型的转化，提升能源系统决策效率。

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>
