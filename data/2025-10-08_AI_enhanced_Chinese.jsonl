{"id": "2510.05438", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05438", "abs": "https://arxiv.org/abs/2510.05438", "authors": ["Alexander James Fernandes", "Ioannis Psaromiligkos"], "title": "Model-based Deep Learning for Joint RIS Phase Shift Compression and WMMSE Beamforming", "comment": "5 pages, 4 figures, submitted to IEEE Communications Letters", "summary": "A model-based deep learning (DL) architecture is proposed for reconfigurable\nintelligent surface (RIS)-assisted multi-user communications to reduce the\noverhead of transmitting phase shift information from the access point (AP) to\nthe RIS controller. The phase shifts are computed at the AP, which has access\nto the channel state information, and then encoded into a compressed binary\ncontrol message that is sent to the RIS controller for element configuration.\nTo help reduce beamformer mismatches due to phase shift compression errors, the\nbeamformer is updated using weighted minimum mean square error (WMMSE) based on\nthe effective channel resulting from the actual (decompressed) RIS reflection\ncoefficients. By unrolling the iterative WMMSE algorithm as part of the\nwireless communication informed DL architecture, joint phase shift compression\nand WMMSE beamforming can be trained end-to-end. Simulations show that\naccounting for phase shift compression errors during beamforming significantly\nimproves the sum-rate performance, even when the number of control bits is\nlower than the number of RIS elements.", "AI": {"tldr": "\u8be5\u6a21\u578b\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u53ef\u91cd\u6784\u667a\u80fd\u9762\uff08RIS\uff09\u8f85\u52a9\u7684\u591a\u7528\u6237\u901a\u4fe1\uff0c\u4ee5\u51cf\u5c11\u4ece\u63a5\u5165\u70b9\uff08AP\uff09\u5230RIS\u63a7\u5236\u5668\u4f20\u8f93\u76f8\u4f4d\u79fb\u4fe1\u606f\u65f6\u7684\u5f00\u9500\u3002\u901a\u8fc7\u5728AP\u5904\u8ba1\u7b97\u76f8\u4f4d\u79fb\uff0c\u5e76\u5c06\u5176\u7f16\u7801\u4e3a\u538b\u7f29\u7684\u4e8c\u8fdb\u5236\u63a7\u5236\u6d88\u606f\u53d1\u9001\u5230RIS\u63a7\u5236\u5668\u8fdb\u884c\u914d\u7f6e\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u76f8\u4f4d\u79fb\u538b\u7f29\u8bef\u5dee\u5bfc\u81f4\u7684\u6ce2\u675f\u8d4b\u5f62\u5668\u5931\u914d\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08WMMSE\uff09\u7b97\u6cd5\u5c55\u5f00\u4f5c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u611f\u77e5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u4e00\u90e8\u5206\uff0c\u53ef\u4ee5\u5bf9\u76f8\u4f4d\u79fb\u538b\u7f29\u548cWMMSE\u6ce2\u675f\u8d4b\u5f62\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u63a7\u5236\u6bd4\u7279\u6570\u5c11\u4e8eRIS\u5143\u7d20\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u6ce2\u675f\u8d4b\u5f62\u4e2d\u8003\u8651\u76f8\u4f4d\u79fb\u538b\u7f29\u8bef\u5dee\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u603b\u901f\u7387\u6027\u80fd\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8eRIS\u8f85\u52a9\u901a\u4fe1\uff0c\u4ee5\u89e3\u51b3\u5728AP\u5230RIS\u63a7\u5236\u5668\u4f20\u8f93\u76f8\u4f4d\u79fb\u4fe1\u606f\u65f6\u7684\u5f00\u9500\u95ee\u9898\u3002", "method": "\u5c06WMMSE\u7b97\u6cd5\u5c55\u5f00\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u4e00\u90e8\u5206\uff0c\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4ee5\u8054\u5408\u4f18\u5316\u76f8\u4f4d\u79fb\u538b\u7f29\u548c\u6ce2\u675f\u8d4b\u5f62\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a7\u5236\u6bd4\u7279\u6570\u5c11\u4e8eRIS\u5143\u7d20\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u663e\u8457\u63d0\u9ad8\u603b\u901f\u7387\u6027\u80fd\u3002", "conclusion": "\u5728RIS\u8f85\u52a9\u901a\u4fe1\u4e2d\uff0c\u8003\u8651\u76f8\u4f4d\u79fb\u538b\u7f29\u8bef\u5dee\u8fdb\u884c\u6ce2\u675f\u8d4b\u5f62\u662f\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u3002"}}
{"id": "2510.05559", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05559", "abs": "https://arxiv.org/abs/2510.05559", "authors": ["Md Rakibul Mowla", "Sukhbinder Kumar", "Ariane E. Rhone", "Brian J. Dlouhy", "Christopher K. Kovach"], "title": "Efficient Coherence Inference Using the Demodulated Band Transform and a Generalized Linear Model", "comment": "6 pages, 6 figures", "summary": "Statistical significance testing of neural coherence is essential for\ndistinguishing genuine cross-signal coupling from spurious correlations. A\nwidely accepted approach uses surrogate-based inference, where null\ndistributions are generated via time-shift or phase-randomization procedures.\nWhile effective, these methods are computationally expensive and yield discrete\np-values that can be unstable near decision thresholds, limiting scalability to\nlarge EEG/iEEG datasets. We introduce and validate a parametric alternative\nbased on a generalized linear model (GLM) applied to complex-valued\ntime--frequency coefficients (e.g., from DBT or STFT), using a likelihood-ratio\ntest. Using real respiration belt traces as a driver and simulated neural\nsignals contaminated with broadband Gaussian noise, we perform dense sweeps of\nground-truth coherence and compare GLM-based inference against\ntime-shift/phase-randomized surrogate testing under matched conditions. GLM\nachieved comparable or superior sensitivity while producing continuous, stable\np-values and a substantial computational advantage. At 80% detection power, GLM\ndetects at C=0.25, whereas surrogate testing requires C=0.49, corresponding to\nan approximately 6--7 dB SNR improvement. Runtime benchmarking showed GLM to be\nnearly 200x faster than surrogate approaches. These results establish GLM-based\ninference on complex time--frequency coefficients as a robust, scalable\nalternative to surrogate testing, enabling efficient analysis of large EEG/iEEG\ndatasets across channels, frequencies, and participants.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\uff08GLM\uff09\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u66ff\u4ee3\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14p\u503c\u4e0d\u7a33\u5b9a\u7684\u4f20\u7edf\u66ff\u4ee3\u68c0\u9a8c\u65b9\u6cd5\uff0c\u7528\u4e8e\u795e\u7ecf\u4fe1\u53f7\u7684\u76f8\u5e72\u6027\u663e\u8457\u6027\u68c0\u9a8c\uff0c\u5728\u4fdd\u6301\u76f8\u4f3c\u6216\u66f4\u4f18\u7684\u654f\u611f\u6027\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548cp\u503c\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u4fe1\u53f7\u76f8\u5e72\u6027\u663e\u8457\u6027\u68c0\u9a8c\u65b9\u6cd5\uff08\u5982\u65f6\u95f4-\u79fb\u4f4d\u6216\u76f8\u4f4d\u968f\u673a\u5316\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14p\u503c\u79bb\u6563\u4e0d\u7a33\u5b9a\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21EEG/iEEG\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\uff08GLM\uff09\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5c06GLM\u5e94\u7528\u4e8e\u590d\u6570\u65f6\u95f4-\u9891\u7387\u7cfb\u6570\uff0c\u5e76\u4f7f\u7528\u4f3c\u7136\u6bd4\u68c0\u9a8c\u3002\u901a\u8fc7\u6a21\u62df\u6570\u636e\u548c\u771f\u5b9e\u547c\u5438\u5e26\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5c06GLM\u65b9\u6cd5\u4e0e\u4f20\u7edf\u66ff\u4ee3\u68c0\u9a8c\u65b9\u6cd5\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GLM\u65b9\u6cd5\u5728\u654f\u611f\u6027\u65b9\u9762\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u751f\u6210\u8fde\u7eed\u3001\u7a33\u5b9a\u7684p\u503c\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684\u8ba1\u7b97\u4f18\u52bf\u3002\u572880%\u7684\u68c0\u6d4b\u6548\u529b\u4e0b\uff0cGLM\u68c0\u6d4b\u9608\u503c\u4e3aC=0.25\uff0c\u800c\u66ff\u4ee3\u68c0\u9a8c\u9700\u8981C=0.49\uff08\u4fe1\u566a\u6bd4\u63d0\u9ad8\u7ea66-7 dB\uff09\u3002GLM\u7684\u8fd0\u884c\u65f6\u95f4\u6bd4\u66ff\u4ee3\u65b9\u6cd5\u5feb\u8fd1200\u500d\u3002", "conclusion": "\u57fa\u4e8eGLM\u7684\u65f6\u95f4-\u9891\u7387\u7cfb\u6570\u63a8\u65ad\u65b9\u6cd5\u662f\u4e00\u79cd\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u4f20\u7edf\u66ff\u4ee3\u68c0\u9a8c\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0EEG/iEEG\u6570\u636e\u96c6\u8de8\u901a\u9053\u3001\u8de8\u9891\u7387\u548c\u8de8\u53c2\u4e0e\u8005\u7684\u6709\u6548\u5206\u6790\u3002"}}
{"id": "2510.05826", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05826", "abs": "https://arxiv.org/abs/2510.05826", "authors": ["Pubudu L. Indrasiri", "Bipasha Kashyap", "Pubudu N. Pathirana"], "title": "Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals", "comment": "14pages, 2 figures", "summary": "Biomedical signals provide insights into various conditions affecting the\nhuman body. Beyond diagnostic capabilities, these signals offer a deeper\nunderstanding of how specific organs respond to an individual's emotions and\nfeelings. For instance, ECG data can reveal changes in heart rate variability\nlinked to emotional arousal, stress levels, and autonomic nervous system\nactivity. This data offers a window into the physiological basis of our\nemotional states. Recent advancements in the field diverge from conventional\napproaches by leveraging the power of advanced transformer architectures, which\nsurpass traditional machine learning and deep learning methods. We begin by\nassessing the effectiveness of the Vision Transformer (ViT), a forefront model\nin image classification, for identifying emotions in imaged ECGs. Following\nthis, we present and evaluate an improved version of ViT, integrating both CNN\nand SE blocks, aiming to bolster performance on imaged ECGs associated with\nemotion detection. Our method unfolds in two critical phases: first, we apply\nadvanced preprocessing techniques for signal purification and converting\nsignals into interpretable images using continuous wavelet transform and power\nspectral density analysis; second, we unveil a performance-boosted vision\ntransformer architecture, cleverly enhanced with convolutional neural network\ncomponents, to adeptly tackle the challenges of emotion recognition. Our\nmethodology's robustness and innovation were thoroughly tested using ECG data\nfrom the YAAD and DREAMER datasets, leading to remarkable outcomes. For the\nYAAD dataset, our approach outperformed existing state-of-the-art methods in\nclassifying seven unique emotional states, as well as in valence and arousal\nclassification. Similarly, in the DREAMER dataset, our method excelled in\ndistinguishing between valence, arousal and dominance, surpassing current\nleading techniques.", "AI": {"tldr": "\u5229\u7528\u6539\u8fdb\u7684Vision Transformer (ViT)\u6a21\u578b\uff0c\u7ed3\u5408CNN\u548cSE\u5757\uff0c\u901a\u8fc7\u5904\u7406ECG\u4fe1\u53f7\u56fe\u50cf\uff0c\u5728YAAD\u548cDREAMER\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u60c5\u611f\u8bc6\u522b\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u4fe1\u53f7\uff0c\u7279\u522b\u662fECG\uff0c\u4e0d\u4ec5\u80fd\u8bca\u65ad\u75c5\u60c5\uff0c\u8fd8\u80fd\u63ed\u793a\u60c5\u7eea\u72b6\u6001\u5bf9\u751f\u7406\u7684\u5f71\u54cd\uff1b\u73b0\u6709\u65b9\u6cd5\u5728\u60c5\u611f\u8bc6\u522b\u65b9\u9762\u6709\u5f85\u63d0\u5347\uff0c\u7279\u522b\u662f\u9700\u8981\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u6765\u5904\u7406ECG\u6570\u636e\u3002", "method": "1. \u9884\u5904\u7406\uff1a\u5bf9ECG\u4fe1\u53f7\u8fdb\u884c\u964d\u566a\uff0c\u5e76\u4f7f\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u548c\u529f\u7387\u8c31\u5bc6\u5ea6\u5206\u6790\u5c06\u5176\u8f6c\u6362\u4e3a\u56fe\u50cf\u3002 2. \u6a21\u578b\uff1a\u8bc4\u4f30\u4e86Vision Transformer (ViT)\u5728\u56fe\u50cfECG\u60c5\u611f\u8bc6\u522b\u4e0a\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ViT\u6a21\u578b\uff0c\u96c6\u6210\u4e86CNN\u548cSE\u5757\uff0c\u4ee5\u589e\u5f3a\u6027\u80fd\u3002", "result": "\u5728YAAD\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e03\u79cd\u60c5\u611f\u72b6\u6001\u7684\u5206\u7c7b\u4ee5\u53ca\u6548\u4ef7\uff08valence\uff09\u548c\u5524\u9192\u5ea6\uff08arousal\uff09\u7684\u5206\u7c7b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5728DREAMER\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u4ef7\u3001\u5524\u9192\u5ea6\u548c\u4f18\u52bf\u5ea6\uff08dominance\uff09\u7684\u533a\u5206\u4e0a\u4e5f\u8d85\u8fc7\u4e86\u5f53\u524d\u9886\u5148\u6280\u672f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408CNN\u548cSE\u5757\u7684\u6539\u8fdbViT\u6a21\u578b\u5728\u5904\u7406\u56fe\u50cfECG\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u548c\u5206\u7c7b\u591a\u79cd\u60c5\u611f\u72b6\u6001\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u5728\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.05834", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05834", "abs": "https://arxiv.org/abs/2510.05834", "authors": ["Tony Lindeberg"], "title": "Time-causal and time-recursive wavelets", "comment": "23 pages, 8 figures", "summary": "When to apply wavelet analysis to real-time temporal signals, where the\nfuture cannot be accessed, it is essential to base all the steps in the signal\nprocessing pipeline on computational mechanisms that are truly time-causal.\n  This paper describes how a time-causal wavelet analysis can be performed\nbased on concepts developed in the area of temporal scale-space theory,\noriginating from a complete classification of temporal smoothing kernels that\nguarantee non-creation of new structures from finer to coarser temporal scale\nlevels. By necessity, convolution with truncated exponential kernels in cascade\nconstitutes the only permissable class of kernels, as well as their temporal\nderivatives as a natural complement to fulfil the admissibility conditions of\nwavelet representations. For a particular way of choosing the time constants in\nthe resulting infinite convolution of truncated exponential kernels, to ensure\ntemporal scale covariance and thus self-similarity over temporal scales, we\ndescribe how mother wavelets can be chosen as temporal derivatives of the\nresulting time-causal limit kernel.\n  By developing connections between wavelet theory and scale-space theory, we\ncharacterize and quantify how the continuous scaling properties transfer to the\ndiscrete implementation, demonstrating how the proposed time-causal wavelet\nrepresentation can reflect the duration of locally dominant temporal structures\nin the input signals.\n  We propose that this notion of time-causal wavelet analysis could be a\nvaluable tool for signal processing tasks, where streams of signals are to be\nprocessed in real time, specifically for signals that may contain local\nvariations over a rich span of temporal scales, or more generally for analysing\nphysical or biophysical temporal phenomena, where a fully time-causal analysis\nis called for to be physically realistic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u5c3a\u5ea6\u7a7a\u95f4\u7406\u8bba\u7684\u65f6\u95f4\u56e0\u679c\u5c0f\u6ce2\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5b9e\u65f6\u65f6\u95f4\u4fe1\u53f7\u3002", "motivation": "\u5728\u65e0\u6cd5\u8bbf\u95ee\u672a\u6765\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u57fa\u4e8e\u771f\u6b63\u65f6\u95f4\u56e0\u679c\u7684\u8ba1\u7b97\u673a\u5236\u6765\u8fdb\u884c\u5b9e\u65f6\u65f6\u95f4\u4fe1\u53f7\u7684\u5c0f\u6ce2\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u56e0\u679c\u5c0f\u6ce2\u5206\u6790\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u65f6\u95f4\u5c3a\u5ea6\u7a7a\u95f4\u7406\u8bba\uff0c\u4f7f\u7528\u622a\u65ad\u6307\u6570\u6838\u53ca\u5176\u65f6\u95f4\u5bfc\u6570\u4f5c\u4e3a\u5377\u79ef\u6838\uff0c\u5e76\u786e\u4fdd\u65f6\u95f4\u5c3a\u5ea6\u534f\u65b9\u5dee\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u53cd\u6620\u8f93\u5165\u4fe1\u53f7\u4e2d\u5c40\u90e8\u4e3b\u5bfc\u7684\u65f6\u95f4\u7ed3\u6784\u6301\u7eed\u65f6\u95f4\uff0c\u5e76\u5c06\u8fde\u7eed\u5c3a\u5ea6\u7279\u6027\u4f20\u9012\u5230\u79bb\u6563\u5b9e\u73b0\u4e2d\u3002", "conclusion": "\u8fd9\u79cd\u65f6\u95f4\u56e0\u679c\u5c0f\u6ce2\u5206\u6790\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u7684\u6709\u7528\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5b8c\u5168\u65f6\u95f4\u56e0\u679c\u5206\u6790\u7684\u5b9e\u65f6\u4fe1\u53f7\u5904\u7406\u3001\u5305\u542b\u5404\u79cd\u65f6\u95f4\u5c3a\u5ea6\u5c40\u90e8\u53d8\u5316\u7684\u4fe1\u53f7\u5206\u6790\uff0c\u4ee5\u53ca\u7269\u7406\u6216\u751f\u7269\u7269\u7406\u65f6\u95f4\u73b0\u8c61\u7684\u5206\u6790\u3002"}}
{"id": "2510.06173", "categories": ["eess.SP", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.06173", "abs": "https://arxiv.org/abs/2510.06173", "authors": ["Shuixin Li", "Jiecheng Chen", "Qingtang Jiang", "Lin Li"], "title": "Time-reassigned synchrosqueezing frequency-domain chirplet transform for multicomponent signals with intersecting group delay curves", "comment": null, "summary": "To analyze signals with rapid frequency variations or transient components,\nthe time-reassigned synchrosqueezing transform (TSST) and its variants have\nbeen recently proposed. Unlike the traditional synchrosqueezing transform, TSST\nsqueezes the time-frequency (TF) coefficients along the group delay (GD)\ntrajectories rather than the instantaneous frequency trajectories. Although\nTSST methods perform well in analyzing transient signals, they are\nfundamentally limited in processing multicomponent signals with intersecting GD\ncurves. This limitation compromises the accuracy of both feature extraction and\nsignal component recovery, thereby significantly reducing the interpretability\nof time-frequency representations (TFRs). This is particularly problematic in\nbroadband signal processing systems, where the linearity of the phase response\nis critical and precise measurement of group delay dispersion (GDD) is\nessential.\n  Motivated by the superior capability of frequency-domain signal modeling in\ncharacterizing rapidly frequency-varying signals, this paper proposes a novel\nthree-dimensional time-frequency-group delay dispersion (TF-GDD) representation\nbased on the frequency-domain chirplet transform. A subsequent time-reassigned\nsynchrosqueezing frequency-domain chirplet transform (TSFCT) is introduced to\nachieve a sharper TF-GDD distribution and more accurate GD estimation. For mode\nretrieval, a novel frequency-domain group signal separation operation (FGSSO)\nis proposed.The theoretical contributions include a derivation of the\napproximation error for the GD and GDD reference functions and an establishment\nof the error bounds for FGSSO-based mode retrieval. Experimental results\ndemonstrate that the proposed TSFCT and FGSSO effectively estimate GDs and\nretrieve modes--even for modes with intersecting GD trajectories.", "AI": {"tldr": "TSST\u53ca\u5176\u53d8\u4f53\u5728\u5206\u6790\u77ac\u6001\u4fe1\u53f7\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u4ea4\u53c9\u7fa4\u5ef6\u8fdf\u66f2\u7ebf\u7684\u591a\u5206\u91cf\u4fe1\u53f7\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u7ef4\u65f6\u9891-\u7fa4\u5ef6\u8fdf\u8272\u6563\uff08TF-GDD\uff09\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e86\u65f6\u95f4\u91cd\u5206\u914d\u540c\u6b65\u538b\u7f29\u9891\u7387\u57df\u091a\u093f\u0930\u092a\u094d\u0932\u0947\u091f\u53d8\u6362\uff08TSFCT\uff09\u548c\u9891\u7387\u57df\u7fa4\u4fe1\u53f7\u5206\u79bb\u64cd\u4f5c\uff08FGSSO\uff09\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6e05\u6670\u7684TF-GDD\u5206\u5e03\u548c\u66f4\u51c6\u786e\u7684GD\u4f30\u8ba1\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5177\u6709\u4ea4\u53c9GD\u8f68\u8ff9\u7684\u6a21\u5f0f\u4e5f\u80fd\u6709\u6548\u4f30\u8ba1GD\u548c\u68c0\u7d22\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u7684TSST\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u4ea4\u53c9\u7fa4\u5ef6\u8fdf\u66f2\u7ebf\u7684\u591a\u5206\u91cf\u4fe1\u53f7\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u7279\u5f81\u63d0\u53d6\u548c\u4fe1\u53f7\u5206\u91cf\u6062\u590d\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5bbd\u5e26\u4fe1\u53f7\u5904\u7406\u7cfb\u7edf\u4e2d\uff0c\u7cbe\u786e\u6d4b\u91cf\u7fa4\u5ef6\u8fdf\u8272\u6563\uff08GDD\uff09\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u57df\u091a\u093f\u0930\u092a\u094d\u0932\u0947\u091f\u53d8\u6362\u7684\u65b0\u9896\u7684\u4e09\u7ef4\u65f6\u9891-\u7fa4\u5ef6\u8fdf\u8272\u6563\uff08TF-GDD\uff09\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u65f6\u95f4\u91cd\u5206\u914d\u540c\u6b65\u538b\u7f29\u9891\u7387\u57df\u091a\u093f\u0930\u092a\u094d\u0932\u0947\u091f\u53d8\u6362\uff08TSFCT\uff09\u4ee5\u83b7\u5f97\u66f4\u6e05\u6670\u7684TF-GDD\u5206\u5e03\u548c\u66f4\u51c6\u786e\u7684GD\u4f30\u8ba1\u3002\u5bf9\u4e8e\u6a21\u5f0f\u68c0\u7d22\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9891\u7387\u57df\u7fa4\u4fe1\u53f7\u5206\u79bb\u64cd\u4f5c\uff08FGSSO\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684TSFCT\u548cFGSSO\u80fd\u591f\u6709\u6548\u5730\u4f30\u8ba1GD\u548c\u68c0\u7d22\u6a21\u5f0f\uff0c\u5373\u4f7f\u5bf9\u4e8eGD\u8f68\u8ff9\u76f8\u4ea4\u7684\u6a21\u5f0f\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684TF-GDD\u8868\u793a\u3001TSFCT\u548cFGSSO\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5177\u6709\u4ea4\u53c9\u7fa4\u5ef6\u8fdf\u8f68\u8ff9\u7684\u591a\u5206\u91cf\u4fe1\u53f7\uff0c\u5728\u63d0\u9ad8\u65f6\u9891\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u53f7\u5206\u6790\u7684\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
