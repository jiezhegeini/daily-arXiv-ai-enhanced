<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 59]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder通过层次化设计和借助大语言模型，实现算法到硬件的高效、可靠转换，解决传统方法中的复杂性和错误率问题。


<details>
  <summary>Details</summary>
Motivation: 解决算法设计与硬件实现之间的差距，提高无线通信系统中的算法部署效率。

Method: 提出层次化框架，结合大模型、模块化分解和多工具链协作，逐步细粒度实现算法到硬件的转换。

Result: 在5G通信应用中验证了其实用性和可靠性，提升了部署效率。

Conclusion: A2HCoder有效缩小了算法和硬件之间的差距，为无线通信系统的算法快速部署提供了新途径。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [2] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 提出了一种多层提示调节框架PersonaTwin，用于构建整合人口统计、行为和心理测量数据的数字孪生，旨在提升用户模拟的准确性和公平性，适用于个性化用户建模和行为分析。


<details>
  <summary>Details</summary>
Motivation: 弥补大语言模型在捕捉用户多维细节方面的不足，增强用户模拟的真实感和公平性。

Method: 设计多层提示调节框架PersonaTwin，结合大量医疗数据进行系统性评估，并采用文本相似度和人口统计平衡指标验证其性能。

Result: 框架实现了与理想设定一致的模拟效果，训练得到的后续模型在预测和公平性方面表现优异，展现了面向个性化和情感细腻模拟的潜力。

Conclusion: PersonaTwin在提升大语言模型数字孪生的真实性和公平性方面表现出巨大潜力，可用于改进个性化用户模型和行为分析工具。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [3] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 两款开源推理模型gpt-oss-120b和gpt-oss-20b：基于专家混合变换器架构，结合蒸馏与强化学习，具备强大推理与工具应用能力，开源促进研究。


<details>
  <summary>Details</summary>
Motivation: 推动推理模型在准确性和推理成本方面的前沿发展，满足复杂任务需求，促进研究与应用。

Method: 采用高效的专家混合变换器架构，通过大规模蒸馏与强化学习训练，并优化模型以增强代理能力，使用聊天格式实现指令遵循和角色区分。

Result: 模型在数学、编码、安全等多个基准测试中表现优异，开源模型权重、推理实现、工具环境和分词器，支持广泛应用与深入研究。

Conclusion: 本文展示了高效且功能强大的开源推理模型，推动了推理与工具融合的研究，促进AI能力的扩展与落地。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [4] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 本研究提出一种自动从新闻中提取公司风险因素的计算框架，涵盖七个方面，并通过实验证明微调模型优于零样本提示，利用该方法分析了超过27万篇新闻，揭示了风险信息的价值。


<details>
  <summary>Details</summary>
Motivation: 识别公司风险对于投资者和金融市场稳定至关重要，但自动提取风险因素具有挑战性，因此需要构建高效的自动化工具。

Method: 构建包含七个方面的风险因素架构，采样并标注744篇新闻，评估多种机器学习模型，包括微调预训练模型和零样本提示的语言模型，最后在大量新闻上应用验证。

Result: 微调模型在大多数风险因素的识别中表现优越，能有效从新闻中提取风险信息，分析了大量新闻资料得出深入见解。

Conclusion: 利用结合多方面信息的结构化方法和微调模型，可以显著提升新闻中风险因素的识别效率和准确性，对金融分析具有重要应用价值。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [5] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: 提出Rule2Text框架，利用大语言模型为知识图中的逻辑规则生成自然语言解释，提升可理解性和可用性。


<details>
  <summary>Details</summary>
Motivation: 提升知识图规则的解释可读性，解决复杂规则难以理解的问题。

Method: 利用大型语言模型，通过不同提示策略生成解释，开展系统评估、人类评价和模型微调。

Result: 显著改善了解释质量，特别是在领域特定数据集，结合类型推断模块增强模型适应性。

Conclusion: 本方法有效提升知识图规则的解释效果，为KG的普及和应用提供支持。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [6] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出了一种基于验证器的推理时间扩展方法，提升了掩码扩散语言模型（MDMs）在文本生成中的表现，并在文本风格迁移任务中优于自回归模型。


<details>
  <summary>Details</summary>
Motivation: 探索提升散扩散语言模型（MDMs）生成质量的方法，特别是在文本风格迁移任务中。

Method: 在推理过程中引入验证器辅助扩展，通过软值评估与预训练嵌入模型结合，以增强生成候选的质量。

Result: 实现了在标准文本风格迁移任务中的优异性能，显著优于传统的自回归模型，并且验证简单的软值验证器能在已有指导方法基础上带来明显改善。

Conclusion: 验证器驱动的推理扩展能有效提升MDMs的生成质量，展示了其作为非自回归文本生成模型的重要潜力。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [7] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: 提出了一种名为SGSimEval的多维评价基准，用于评估自动调查生成系统的多个方面，结合人工偏好和定量指标，提高评价的全面性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动调查生成评价方法偏颇、缺乏人类偏好考虑的问题，推动该领域的发展。

Method: 设计了融合内容、提纲和参考文献评估的多方面指标，结合LLM评分和人工偏好指标，构建了SGSimEval基准，进行广泛实验验证。

Result: 验证表明当前系统在提纲生成方面表现优异，但在内容和参考文献方面仍有提升空间，评价指标与人类评估高度一致。

Conclusion: SGSimEval为自动调查生成的评估提供了一个全面、有效的方法，有助于推动技术的进一步完善与发展。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [8] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了SproutBench，一个用于评估儿童和青少年使用的语言模型安全性的新工具，揭示了现有模型在应对年龄特定风险方面的不足，并提供了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在儿童和青少年中的广泛应用，亟需评估其安全性，特别关注其对不同年龄段的特点和风险的应对能力。

Method: 建立SproutBench评估套件，设计283个针对不同发展阶段的对抗性提示，对47个不同的语言模型进行实证测试，分析其安全性表现。

Result: 发现当前模型在情感依赖、隐私侵犯及危险行为模仿等方面存在显著安全隐患，且安全性与风险预防高度相关，交互性与年龄适宜性呈负相关。

Conclusion: 研究为儿童青少年的AI安全设计提供了实用指导，强调需关注年龄特征以增强模型安全性。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [9] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 研究了多语言模型中的跨语言知识转移，通过控制训练数据和Tokenization，影响模型在不同语言之间的知识整合，从而改善迁移效果。


<details>
  <summary>Details</summary>
Motivation: 理解大规模语言模型在跨语言知识转移中的表现机制，解决其在不同语言间表述事实时的偏差和错误。

Method: 通过从零开始在合成多语料库上训练小型Transformer模型，研究其在不同训练阶段的语义表示，调控数据分布和Tokenization以影响知识的统一。

Result: 发现知识的统一程度依赖于事实与训练语言的信息互信息和语言的提取难易，提供了调节跨语言迁移的策略。

Conclusion: 控制训练数据及Tokenization，可以有效影响模型的知识整合，促进跨语言知识转移，为优化预训练策略提供新方向。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [10] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 语言模型在复杂计划中面对外部失败时，表现出适应能力不足的问题，缺乏有效的备选方案制定能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型应用到越来越复杂的实际问题中，研究其在面对外部环境变化时的应对能力变得尤为重要。

Method: 设计了一个专项的代理计划基准，通过调用函数解决问题，模拟外部失败，检测模型的应变能力。

Result: 现有模型难以根据环境反馈选取备选方案，表现出应变能力不足，且模型规模的扩大并未显著改善问题。

Conclusion: 当前生成模型在应对外部失败时存在明显短板，未来需要探索更有效的应变策略和优化方法。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [11] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 提出一种通用、多样化、话题无关的偏见评估框架，结合极化敏感情感指标和合成平衡数据，针对俄乌战争等话题评估多个大模型偏见。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在敏感话题中偏见检测不足的问题，提升偏见评估的细粒度和适用性。

Method: 设计结合极化敏感情感指标与合成平衡数据的评估框架，针对特定话题（如俄乌战争）进行偏见检测，评估多个模型的行为差异。

Result: 框架能实现偏见的自动化生成与细粒度评估，揭示模型在不同语义类别上的差异，模型偏见受提示操作影响。

Conclusion: 该框架具有高度适应性、细粒度、话题无关的优势，优于多种偏见评估策略，可广泛应用于多极化场景。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [12] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 本文提出将数字词典转化为AMR图，并通过无损简化方法分析其性质，以解决符号基础问题。


<details>
  <summary>Details</summary>
Motivation: 旨在利用AMR和预训练模型，将数字词典转换为图结构并进行简化，从而探讨符号的基础和表达能力。

Method: 将数字词典嵌入到AMR图中，采用无损变换进行简化，并分析其性质。

Result: 成功将词典转化为AMR图，简化后保持原有信息，探讨其与符号基础的问题关系。

Conclusion: 提出的无损简化方法有效，促进对符号基础问题的理解，展示了AMR在知识表示中的潜力。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [13] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 提出RAMP框架，通过多次验证和记忆增强，提高大语言模型在市场推广任务中的表现和可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索在真实应用中，基于大模型的AI代理的可靠性及提升方法。

Method: 设计多代理框架RAMP，结合工具调用、输出验证和反思机制，加入客户知识库，实现持续改进。

Result: 提升88个测试查询的准确率28个百分点，验证和反思机制使在模糊任务中的召回率提升约20个百分点，用户满意度增加。

Conclusion: 多轮验证和记忆增强的策略有效提升LLM在实际行业任务中的表现，为行业应用提供了实用指导。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [14] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个针对复杂、信息性强且耗时的问题的基准测试，旨在推动推理模型的发展。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM基准缺乏真实时间消耗和复杂性的问题，难以全面评估模型能力。

Method: 构建含有1315个自然复杂问题的基准，采用分解式标注流程手动答案，测试最先进的LLMs。

Result: 最强LLM在MoNaCo上的F1值最高为61.2%，表明模型在复杂推理中仍有明显不足，尤其在召回和幻觉方面。

Conclusion: 需要发展更擅长处理复杂、多步推理的模型，MoNaCo为此提供了重要评估资源，同时促进模型进步。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [15] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: MobQA是用于评估大规模语言模型对人类出行数据理解能力的基准数据集，含丰富的问答形式，揭示了模型在事实检索方面表现优异，但在语义推理和解释方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 当前模型在预测人类运动模式方面表现优异，但对其理解运动背后的语义和原因仍缺乏清楚评估工具，MobQA旨在填补这一空白。

Method: 构建包含5,800个高质量问答对的MobQA数据集，涵盖事实检索、多项选择推理和自由说明三类问题，涉及空间、时间和语义推理。

Result: 大模型在事实检索上表现出色，但在语义推理和解释任务中存在明显不足，且轨迹长度影响模型性能。

Conclusion: MobQA有效揭示了当前大模型在语义Mobility理解方面的成就与不足，为未来模型提升提供了评估标准。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [16] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 首次提出泰卢语混合代码的攻击性言论识别基准数据集，评估不同深度学习模型，发现Transformer模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 泰卢语作为低资源语言，缺乏有效的攻击性言论检测资源，限制了其数字传播发展。

Method: 采集YouTube评论，构建标注数据集，使用多种深度学习模型进行评估。

Result: BiGRU+自注意力模型表现最佳，准确率82%，F1值0.81。Transformer模型表现较差。

Conclusion: 为泰卢语及类似低资源、代码混合语言的NLP研究奠定基础，指出多语预训练在此类任务中的局限性。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [17] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 提出一种基于学生个体误解的个性化干扰项生成框架，通过两阶段方法有效改善干扰项的个性化和多样性。


<details>
  <summary>Details</summary>
Motivation: 改进多项选择题的干扰项，使其更好地诊断学生的个性化认知误区。

Method: 利用Monte Carlo树搜索构建学生特定的误解原型，然后模拟学生的推理过程生成个性化干扰项，无需训练数据。

Result: 在140名学生的实验中取得了优异表现，并成功扩展至群体层面的应用，显示出方法的鲁棒性和适应性。

Conclusion: 该方法有效提升了干扰项的个性化和诊断能力，为智能教育评估提供新的解决途径。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [18] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 提出了一种新颖的双尺度寄生方法，结合模型压缩和知识蒸馏，有效提升多语种语音翻译模型在推理效率和性能上的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决多语种语音翻译模型参数大、效率低的问题，特别是在本地部署条件下的应用需求。

Method: 引入寄生双尺度方法，融合先进的猜测采样、模型压缩和知识蒸馏技术，在Whisper模型基础上进行增强，开发出多语种语音翻译模型whisperM2M。

Result: 实现六种主要语言的SOTA性能，推理速度提升40%，同时不降低BLEU分数，结合蒸馏技术整体提升速度达2.6倍，优于原始模型。

Conclusion: 寄生双尺度方法有效平衡了模型规模、速度与性能，为多语种语音翻译应用提供了新的解决方案。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [19] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: 提出了一种名为E-CaTCH的多模态假信息检测框架，结合事件级分析和多模态特征，提升检出效果。


<details>
  <summary>Details</summary>
Motivation: 多模态信息的不一致性、时间变化和类别不平衡使假信息检测具有挑战性，现有方法多忽略事件级结构。

Method: 通过文本相似和时间接近对帖子聚类为伪事件，利用预训练编码器提取特征，采用自注意力和跨模态注意力进行融合，利用趋势感知LSTM建模时间动态，并引入多种策略处理类别不平衡。

Result: 在Fakeddit、IND和COVID-19 MISINFOGRAPH等数据集上均优于最新方法，跨数据集验证展现出良好的鲁棒性和泛化性。

Conclusion: E-CaTCH通过事件级建模和多模态融合，有效提升假信息检测性能，具有实用性和扩展性。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [20] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 提出了一种基于超图的多跳问答新方法HGRAG，通过跨粒度信息整合提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法在多跳问答中未充分结合结构和语义信息，影响效果。

Method: 构建实体超图结合细粒度实体和粗粒度句子，并通过超图扩散实现多层次信息融合，添加检索增强模块优化结果。

Result: 在基准数据集上优于最新方法，且检索速度提升六倍。

Conclusion: 该方法有效融合结构和语义信息，提高多跳问答的准确性和效率。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [21] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: LLMs在低资源语言的语言谜题中表现较弱，尤其在形态复杂的任务中，分解词素的预处理有助改善表现，强调需要更适合的语言特定分词器。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在低资源语言分析中的能力及其在语言谜题中的表现，以揭示其在语言推理方面的局限性。

Method: 分析629个低资源语言中的语言谜题，结合语言特征标注，评估LLMs的表现，并测试词素分解预处理的影响。

Result: LLMs在高形态复杂度的谜题中表现较差，基于英语的特征表现较好，分词预处理改善了谜题的解决能力。

Conclusion: LLMs在低资源语言的语言推理方面存在挑战，需更语言特异的分词技术以提升表现。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [22] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: 提出一种基于专家推理结构的无标注旅游领域大型语言模型评价方法，避免昂贵的标注成本。


<details>
  <summary>Details</summary>
Motivation: 解决旅游领域LLMs评价困难，减少标注成本和幻觉问题。

Method: 利用专家推理树结构，反复优化和验证，以评估不同规模的模型性能。

Result: 实现4.99-14.15%的质量提升，揭示模型规模与推理性能关系，特别在小模型中表现显著。

Conclusion: 构建了可扩展的领域无标注评价范式，提供了替代传统标注基准的方案。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [23] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 提出了TOXIFRENCH数据集，发现小型模型在毒性检测中表现优于大型模型，并通过新颖的链式思考微调策略显著提升性能，最终实现了最新的性能水平。


<details>
  <summary>Details</summary>
Motivation: 弥补法语毒性检测数据缺乏，提升模型在多语言环境中的性能，为安全内容筛查提供更有效的方法。

Method: 构建TOXIFRENCH数据集，采用高置信度预标注和人工验证，比较多种模型，提出链式思考微调策略，利用动态加权损失增强决策可信度。

Result: 小型模型表现优于大型模型，4B微调模型在毒性检测中达到最佳性能，跨语言任务中表现出强大的多语言能力，优于GPT-40和Gemini-2.5。

Conclusion: 小型模型在毒性检测具有潜力，创新的微调策略有效提升模型性能，方法可扩展到其他语言和安全任务中。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [24] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 研究八种大语言模型在应对精神健康问题不同用户画像时的情感反应差异，强调模型选择对用户体验的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着精神健康问题日益严重，大语言模型在提供心理咨询中的潜力受到关注，但不同模型在表达情感上的差异尚不明确。

Method: 对八个LLMs针对六个用户画像的20个问题产生的2880个回答进行情感和情绪分析。

Result: 不同模型表现出不同的情感偏向，且模型的选择显著影响情感表现，而用户画像影响则较小。

Conclusion: 模型的情感特征差异强调了在心理健康应用中进行模型选择的重要性，以改善用户体验与效果。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [25] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）表现出过度拒绝行为，安全机制导致其拒绝看似有害但实际上无害的指令。提出SafeConstellations方法，调节模型表示轨迹，显著减少过度拒绝，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs中过度拒绝有害指令的问题，以改善模型在实际应用中的实用性。

Method: 分析LLMs的嵌入空间轨迹，识别拒绝与非拒绝的模式，提出轨迹调整技术SafeConstellations，指导模型避开拒绝路径。

Result: 减少过度拒绝率最多73%，对模型的整体性能影响 minimal，提供了一种有理论依据的缓解策略。

Conclusion: 通过轨迹调节技术，有效缓解LLMs的过度拒绝行为，增强其在实际任务中的实用性和可靠性。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [26] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文研究了4-bit Group Scaling Quantization（GSQ）和GPTQ两种低比特量化方法在不同大模型上的效果，评估了其在多个任务中的性能折衷，为实际部署提供参考。


<details>
  <summary>Details</summary>
Motivation: 为了降低大型语言模型（LLMs）的存储和计算成本，同时保持性能，本文探索了低比特量化技术的应用潜力。

Method: 将4-bit GSQ和GPTQ应用于LLaMA、Qwen和PHI模型，并在信息检索、问答和数学推理任务中进行性能评估，比较准确率、推理延迟和吞吐量，分析不同量化方法对模型的影响。

Result: 低比特量化在保持较好性能的同时显著减小模型大小，提高推理速度，为模型部署提供了有效方案。不同模型和任务中，GSQ和GPTQ各有优劣，提供了详细的性能折衷分析。

Conclusion: 低比特量化技术具有广泛应用前景，但需结合不同模型和任务特性选择合适的方法，以实现性能和效率的最佳平衡。

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [27] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 通过频域信号处理分析文本的频谱能量差异，有效检测由大型语言模型生成的文本。


<details>
  <summary>Details</summary>
Motivation: 应对高质量文本普及带来的检测需求，现有方法不足以捕捉文本生成过程的根本信号特性。

Method: 将检测问题转化为信号处理，通过傅里叶变换分析文本序列的频谱特性，并基于频谱能量构建检测器。

Result: 提出的SpecDetect系列方法在检测准确性和效率方面优于现有方法，且具备良好的鲁棒性和解释性。

Conclusion: 经典信号处理技术在现代文本检测中展现出强大潜力，为快速、可靠的检测提供新思路。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [28] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 本研究探讨利用Llama 3.1大型语言模型从学生作业中提取反馈指标的效果，结果显示指标与人类评价高度相关，为自动生成可解释的反馈提供基础。


<details>
  <summary>Details</summary>
Motivation: 旨在提升自动化生成高质量、信息丰富的学习反馈的能力，同时减轻教师负担，提高教学效率。

Method: 利用Llama 3.1模型，从学生语言学习任务提交中提取反馈指标，并评估这些指标与人类评分之间的相关性。

Result: 指标与人类评分之间存在统计学上显著的强相关，即使在指标与评估标准组合意外的情况下也如此。

Conclusion: 该方法为利用大模型自动提取教育反馈指标提供了有力基础，有望未来实现可解释、透明的形成性反馈自动生成。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [29] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文系统评估了五种提升大规模语言模型提示稳健性的方法，在不同模型和任务上测试其效果，提供实用的优化建议。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在实际应用中的普及，其对提示语微小变化的敏感性成为性能稳定的关键问题。

Method: 设计统一实验框架，比较五种方法在多种模型和任务上的表现，并扩展到最新的模型以测试其鲁棒性。

Result: 不同方法在不同模型和任务中表现差异明显，一些技术显著提升了模型对格式扰动的稳健性，为实践提供了参考。

Conclusion: 多样的鲁棒性提升方法均有一定效果，选择合适方案能显著提高大模型的稳定性和可靠性。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [30] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 提出一种结合推理与检索增强生成的轻量级语言模型架构，适用于资源有限环境，显著提升问答准确性。


<details>
  <summary>Details</summary>
Motivation: 应对在资源受限或安全环境中对高性能且隐私保护的问答系统的需求。

Method: 结合dense检索与微调模型，通过合成数据、文档压缩和推理微调提升性能。

Result: 模型在特定领域问答中表现出接近前沿模型的准确性和一致性，适合本地部署。

Conclusion: 该方法提高了轻量模型的问答质量，支持在不同领域的应用与复用。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [31] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出一种基于梯度优化和正则化的神经网络模型解释方法，适用于文本和图像输入，强调简洁性与充分性。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型在各领域的应用不断扩大，需要有效的模型解释以提高透明度和信任度。

Method: 通过掩码部分输入并利用梯度优化进行调整，同时引入正则化以确保解释的充足性、简便性和紧凑性，兼顾模型解释和合理性。

Result: 方案在文本和图像分类任务中表现出高质量的解释能力，验证了自然语言处理中的所提条件在不同输入类型中具有广泛适用性。

Conclusion: 该方法无需训练专门模型即可从已训练模型中提取解释，促进模型可解释性的发展。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [32] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出一种端到端可微的稳定训练范式，优化有理化变换器模型的性能与可解释性，获得最优的模型与人类标注的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的有理化模型训练存在不稳定性和效率低下的问题。

Method: 通过单一模型实现分类与输入标记重要性评分的整合，简化训练流程，并引入类特定理由机制与正则化技术。

Result: 新模型训练稳定，且在没有明确监督的情况下，显著改善了与人类标注的一致性，达到最新的性能水平。

Conclusion: 提出的端到端模型强化了有理化模型的训练稳定性与解释性，推动了该领域的发展。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [33] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 通过微调大型语言模型应答价值调研问卷，成功调整模型的价值体系，实现价值对齐。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型隐含价值偏好难以控制的问题，探索简单有效的价值引导方法。

Method: 构建模型的价值轮廓，利用微调调整模型回答，评估在标化问卷和下游任务中的表现变化。

Result: 微调后模型在问卷回答和实际任务中的价值偏好显著变化，实现了价值调控和行为调整。

Conclusion: 通过针对价值问卷的微调是一种有效的模型价值引导策略，具有潜在应用前景。

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [34] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: HumorPlanSearch通过多策略、多步骤的流水线，增强AI幽默生成的文化适应性和多样性，显著提升幽默质量。


<details>
  <summary>Details</summary>
Motivation: 解决自动幽默生成中缺乏文化敏感性和多样性的问题。

Method: 引入多策略搜索、文化语境模仿、知识图谱、过滤机制与评估反馈。

Result: 提高幽默评分15.4%，在多主题和人类反馈中表现优异，推动AI幽默向更自然、更文化敏感方向发展。

Conclusion: 通过系统整合多技术，HumorPlanSearch显著改善了AI幽默的效果，推动其在文化和情境中的应用潜力。

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [35] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 自动内容审核系统在识别反性别歧视言论时常误判为有害信息，尤其在政治敏感事件中，这可能压制反抗性别歧视的声音。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨基于大规模语言模型的自动内容审核在辨别反性别歧视言论中的表现及其潜在问题。

Method: 分析五种大型语言模型在英国2022年涉及女性议员的政治推文中的分类表现，结合事件背景进行评估。

Result: 模型常将反性别歧视的反抗性言论误判为有害，特别在政治敏感事件中误差增加，有可能压制边缘群体的声音。

Conclusion: 建议超越二元有害/无害框架，加入人类审核环节，并在训练中明确包含反性别歧视反言，提高模型的公平性和敏感性。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [36] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: 提出CoDiEmb框架，有效解决信息检索与语义文本相似性任务的冲突问题，通过任务专项目标、模型融合策略和单阶段训练，实现多任务性能提升。


<details>
  <summary>Details</summary>
Motivation: 实现统一文本嵌入在多任务中的表现，但遇到任务间负迁移问题，特别是在IR和STS的联合训练中表现不佳。

Method: 引入任务专项目标与动态采样、delta-guided模型融合策略，以及简洁高效的单阶段训练流程，协调不同任务需求。

Result: 在多个标准基准测试中验证，显著缓解任务冲突，提高嵌入空间的几何特性，增强多任务性能。

Conclusion: 通过系统解耦与创新策略，CoDiEmb高效实现多任务文本嵌入优化，克服负迁移，促进多任务协同学习。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [37] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 结构化提示增强低参数模型的情感分析表现，实用性强。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用结构化提示改善小模型的情感分析效果，响应营销理论对参考点的关注。

Method: 比较自然语言和JSON格式提示，实验两类Yelp评论，评估模型性能。

Result: JSON提示显著提升模型性能，Macro-F1提高1.6%-4%，RMSE降低9.1%-16%，验证结构化提示的有效性。

Conclusion: 结构化提示能使小模型实现竞争性表现，是低资源环境下的可行方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [38] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在识别种族偏见上表现良好，但在道德评判中常表现为物种偏见或合理化伤害非人类动物，反映文化偏见，建议将非人类道德成员纳入AI公平性框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛应用，研究其是否存在物种偏见具有重要意义，以避免文化偏见被强化。

Method: 通过SpeciesismBench基准测试、心理学对比分析和文本生成任务，系统评估LLMs在物种偏见上的表现。

Result: LLMs能识别种族偏见但少批判，表现出偏向人类优先，在文本中倾向合理化对动物的伤害，反映文化偏见。

Conclusion: 建议将非人类道德患者纳入AI公平与对齐框架，以减少偏见并防止文化偏见的固化。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [39] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [40] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 提出一种多智能体模拟医生-患者对话的心理健康评估框架，结合动态交互和树状记忆提升信息提取和评估性能。


<details>
  <summary>Details</summary>
Motivation: 传统的心理健康评估受专家短缺限制，亟需自动化及更深入的交互式评估方法。

Method: 利用多智能体系统模拟对话，采用自适应提问机制和树状记忆结构进行动态信息管理。

Result: 在DAIC-WOZ数据集上表现优于现有方法，有效提升评估性能。

Conclusion: 多智能体互动模型结合动态记忆技术能显著改善自动化心理健康评估效果。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [41] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 引入DR. SAF框架，通过动态调整推理深度，显著提高LLMs的效率，减少响应tokens，提升训练速度，且在极端训练条件下超越传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在复杂推理任务中产生冗余、效率低下的问题，优化推理过程。

Method: 设计了DR. SAF框架，融合边界自我感知对齐、适应性奖励管理和边界保持机制，使模型能根据任务复杂度动态调整推理深度。

Result: 显著减少响应token数量，提升token效率，缩短训练时间，在极端训练中取得超过16%的准确率提升。

Conclusion: DR. SAF框架有效改善LLMs的推理效率，适合资源有限环境，并在性能上优于传统模型。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [42] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream是一个模仿人类听觉机制的双阶段语音编码模型，通过模拟耳蜗处理过程和自回归序列模型，实现了有效的语音表征和多任务性能。


<details>
  <summary>Details</summary>
Motivation: 旨在借鉴人类听觉系统，提升语音表示的生物学可信度和多任务适应性。

Method: 由耳蜗基础的时频特征转换和自回归模型组成，结合生物启发的两个阶段进行语音编码。

Result: 在多个下游任务中表现优异，能生成可视化的音频续写，展现了丰富的语义和编码能力。

Conclusion: 提出的双阶段框架推动了人类类似的语音识别模型发展，提高了多任务处理的效率与效果。

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [43] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 提出了一种基于生成模型的合成数据集，用于训练视觉蕴涵模型，有助于解决数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 解决视觉蕴涵数据集小且稀疏的问题，降低数据集构建的劳动力成本。

Method: 利用SNLI文本数据作为提示，通过Stable Diffusion生成相应图片，构建合成数据集，并采用CLIP特征进行性能评估。

Result: 合成数据在SNLI-VE和SICK-VTE两个数据集上表现优异，仅略低于真实数据，验证了其有效性和潜力。

Conclusion: 合成数据为视觉蕴涵任务提供了一种有前景的解决方案，特别是在数据匮乏的情况下。

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [44] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: TinyTim 是一系列微调自詹姆斯·乔伊斯《芬尼根的守灵夜》的大型语言模型，展现出高度词汇多样性但语义连贯性较低的特殊生成特征，适合作为创新和探索的知识源。


<details>
  <summary>Details</summary>
Motivation: 提升AI在复杂文学创作及创新机制中的应用潜力，探索专业化大模型的创造性表现。

Method: 以《芬尼根的守灵夜》为训练语料，微调大型语言模型，进行定量评估并对比基线模型。

Result: TinyTim V1 展现出高词汇多样性与低语义连贯性的生成特性，被视为创新和探索任务中的潜在知识源。

Conclusion: 专业化的大模型可以作为创造性知识源，推动自动发现机制在多种场景中的应用。

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [45] [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
*Zhuoqun Li,Xuanang Chen,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun*

Main category: cs.IR

TL;DR: 提出PaperRegister，通过层次化索引实现灵活粒度的论文检索，尤其在细粒度场景表现优越。


<details>
  <summary>Details</summary>
Motivation: 满足研究人员对细粒度论文检索的需求，弥补现有系统在细粒度检索方面的不足。

Method: 构建层次化索引树，并结合在线自适应检索机制，实现论文按不同粒度的检索。

Result: 在多粒度检索任务中达到最先进的性能，特别是在细粒度场景中表现突出。

Conclusion: PaperRegister为实际应用中的灵活粒度论文检索提供了有效解决方案。

Abstract: Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

</details>


### [46] [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: +VeriRel通过整合验证成功信息，提高科学事实核查中的证据检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖传统信息检索，不能有效评估证据对支持或反驳声明的相关性。

Method: 引入包含验证成功信息的证据评估模型+VeriRel，并在多个科学事实核查数据集上进行实验。

Result: +VeriRel在证据检索和验证任务中表现优越，显示出潜在的提升空间。

Conclusion: 整合验证反馈对于提升科学事实核查系统的证据相关性评估具有显著潜力，未来可探索更细粒度的相关性分析以应对复杂文献。

Abstract: Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

</details>


### [47] [Role-Augmented Intent-Driven Generative Search Engine Optimization](https://arxiv.org/abs/2508.11158)
*Xiaolu Chen,Haojie Wu,Jie Bao,Zhen Chen,Yong Liao,Hu Huang*

Main category: cs.IR

TL;DR: 提出了一种针对生成搜索引擎的优化策略G-SEO，利用角色反思和意图引导提升内容可见性，并通过扩展数据集和引入G-Eval 2.0进行评估，取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 随着生成搜索引擎兴起，传统SEO策略面临挑战，需要新方法以提升内容在此环境中的表现。

Method: 通过建模搜索意图，结合多角色反思进行内容优化，扩展数据集并引入细粒度评价体系G-Eval 2.0以验证效果。

Result: 该方法显著提升了内容在生成搜索引擎中的曝光和满足用户意图的能力，优于单一指标的基线方法。

Conclusion: 角色驱动的意图导向优化为GSE内容优化提供了有效路径，提升了搜索内容的适应性和表现力。

Abstract: Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG), are reshaping information retrieval.
While commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive
semantic synthesis capabilities, their black-box nature fundamentally
undermines established Search Engine Optimization (SEO) practices. Content
creators face a critical challenge: their optimization strategies, effective in
traditional search engines, are misaligned with generative retrieval contexts,
resulting in diminished visibility. To bridge this gap, we propose a
Role-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)
method, providing a structured optimization pathway tailored for GSE scenarios.
Our method models search intent through reflective refinement across diverse
informational roles, enabling targeted content enhancement. To better evaluate
the method under realistic settings, we address the benchmarking limitations of
prior work by: (1) extending the GEO dataset with diversified query variations
reflecting real-world search scenarios and (2) introducing G-Eval 2.0, a
6-level LLM-augmented evaluation rubric for fine-grained human-aligned
assessment. Experimental results demonstrate that search intent serves as an
effective signal for guiding content optimization, yielding significant
improvements over single-aspect baseline approaches in both subjective
impressions and objective content visibility within GSE responses.

</details>


### [48] [Representation Quantization for Collaborative Filtering Augmentation](https://arxiv.org/abs/2508.11194)
*Yunze Luo,Yinjie Jiang,Gaode Chen,Jingchi Wang,Shicheng Wang,Ruina Sun,Jiang Yuezihan,Jun Zhang,Jian Liang,Han Li,Kun Gai,Kaigui Bian*

Main category: cs.IR

TL;DR: 提出一种新颖的两阶段协同推荐算法DQRec，通过解构式量化变分自编码器（DQ-VAE）提取行为特征，增强用户和商品的相似性连接，改善数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中协同过滤算法面临的数据稀疏问题，改进行为特征提取方法，提升推荐性能。

Method: 引入解构式量化变分自编码器（DQ-VAE）将预训练表示嵌入解构为不同维度并量化成语义ID，结合序列行为和属性信息增强特征和连接。

Result: 在多个数据集上实验表现优异，优于多种基线方法，验证了其提升推荐效果的能力。

Conclusion: 通过解构式量化实现更细粒度的行为特征提取，增强了推荐系统的连接与表达能力，有效缓解稀疏性问题，显示出较强的应用潜力。

Abstract: As the core algorithm in recommendation systems, collaborative filtering (CF)
algorithms inevitably face the problem of data sparsity. Since CF captures
similar users and items for recommendations, it is effective to augment the
lacking user-user and item-item homogeneous linkages. However, existing methods
are typically limited to connecting through overlapping interacted neighbors or
through similar attributes and contents. These approaches are constrained by
coarse-grained, sparse attributes and fail to effectively extract behavioral
characteristics jointly from interaction sequences and attributes. To address
these challenges, we propose a novel two-stage collaborative recommendation
algorithm, DQRec: Decomposition-based Quantized Variational AutoEncoder
(DQ-VAE) for Recommendation. DQRec augments features and homogeneous linkages
by extracting the behavior characteristics jointly from interaction sequences
and attributes, namely patterns, such as user multi-aspect interests. Inspired
by vector quantization (VQ) technology, we propose a new VQ algorithm, DQ-VAE,
which decomposes the pre-trained representation embeddings into distinct
dimensions, and quantize them to generates semantic IDs. We utilize the
generated semantic IDs as the extracted patterns mentioned above. By
integrating these semantic ID patterns into the recommendation process through
feature and linkage augmentation, the system enriches both latent and explicit
user and item features, identifies pattern-similar neighbors, and thereby
improves the efficiency of information diffusion. Experimental comparisons with
baselines across multiple datasets demonstrate the superior performance of the
proposed DQRec method.

</details>


### [49] [Mitigating Filter Bubble from the Perspective of Community Detection: A Universal Framework](https://arxiv.org/abs/2508.11239)
*Ming Tang,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 提出了一种基于社区检测的推荐系统框架CD-CGCN，有效缓解滤泡效应，提升推荐多样性。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中过度关注准确率导致的多样性不足和过滤泡泡问题。

Method: 引入社区检测分析用户-物品交互，结合条件判别器和社区加权图卷积网络，通过对抗学习和推断策略减轻滤泡影响。

Result: 在多个真实数据集上验证了其在缓解过滤泡泡和保持推荐质量方面的优越性，并通过抗偏差处理提升跨社区偏好捕捉能力。

Conclusion: CD-CGCN作为一种通用框架，有效改善推荐系统的多样性和用户体验，具有实用价值。

Abstract: In recent years, recommender systems have primarily focused on improving
accuracy at the expense of diversity, which exacerbates the well-known filter
bubble effect. This paper proposes a universal framework called CD-CGCN to
address the filter bubble issue in recommender systems from a community
detection perspective. By analyzing user-item interaction histories with a
community detection algorithm, we reveal that state-of-the-art recommendations
often focus on intra-community items, worsening the filter bubble effect.
CD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and
a Community-reweighted Graph Convolutional Network which can be plugged into
most recommender models. Using adversarial learning based on community labels,
it counteracts the extracted community attributes and incorporates an inference
strategy tailored to the user's specific filter bubble state. Extensive
experiments on real-world datasets with multiple base models validate its
effectiveness in mitigating filter bubbles while preserving recommendation
quality. Additionally, by applying community debiasing to the original test set
to construct an unbiased test set, we observe that CD-CGCN demonstrates
superior performance in capturing users' inter-community preferences.

</details>


### [50] [INFNet: A Task-aware Information Flow Network for Large-Scale Recommendation Systems](https://arxiv.org/abs/2508.11565)
*Kaiyuan Li,Dongdong Mao,Yongxiang Tang,Yanhua Cheng,Yanxiang Zeng,Chao Wang,Xialong Liu,Peng Jiang*

Main category: cs.IR

TL;DR: INFNet通过任务感知架构和双流设计，优化大规模推荐系统中的特征交互问题，有效解决计算复杂度和多任务融合限制，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前特征交互策略在工业应用中面临计算成本高和多任务融合不足的问题。

Method: 引入三类特征Token，采用异质和同质信息流的双流设计，利用交叉注意力机制和类型特定门控单元。

Result: 在多个离线基准测试中达到了最优性能，并在商业广告系统部署后带来明显的收入和点击率提升。

Conclusion: INFNet通过任务感知和高效的信息流设计，有效提升大规模推荐系统中的特征交互能力，具有良好的实用性和推广价值。

Abstract: Feature interaction has long been a cornerstone of ranking models in
large-scale recommender systems due to its proven effectiveness in capturing
complex dependencies among features. However, existing feature interaction
strategies face two critical challenges in industrial applications: (1) The
vast number of categorical and sequential features makes exhaustive interaction
computationally prohibitive, often resulting in optimization difficulties. (2)
Real-world recommender systems typically involve multiple prediction
objectives, yet most current approaches apply feature interaction modules prior
to the multi-task learning layers. This late-fusion design overlooks
task-specific feature dependencies and inherently limits the capacity of
multi-task modeling. To address these limitations, we propose the Information
Flow Network (INFNet), a task-aware architecture designed for large-scale
recommendation scenarios. INFNet distinguishes features into three token types,
categorical tokens, sequence tokens, and task tokens, and introduces a novel
dual-flow design comprising heterogeneous and homogeneous alternating
information blocks. For heterogeneous information flow, we employ a
cross-attention mechanism with proxy that facilitates efficient cross-modal
token interaction with balanced computational cost. For homogeneous flow, we
design type-specific Proxy Gated Units (PGUs) to enable fine-grained intra-type
feature processing. Extensive experiments on multiple offline benchmarks
confirm that INFNet achieves state-of-the-art performance. Moreover, INFNet has
been successfully deployed in a commercial online advertising system, yielding
significant gains of +1.587% in Revenue (REV) and +1.155% in Click-Through Rate
(CTR).

</details>


### [51] [The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers](https://arxiv.org/abs/2506.20844)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 提出科学事实核查中面临的多项挑战，包括证据检索、文献解析和信誉评估，并探讨潜在的解决方案，以推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 随着科学知识的不断增长，有效的科学事实核查变得尤为重要，但现有方法存在多方面的不足，亟需改进以应对复杂的科学文献。

Method: 分析现有系统的局限性，提出多项关键研究挑战，结合初步实验验证可行性。

Result: 明确了科学事实核查中的主要问题和研究方向，为未来发展提供指导方针。

Conclusion: 本文旨在通过针对性的检索系统设计，推动科学事实核查技术的进步，适应实际应用需求。

Abstract: Scientific fact-checking aims to determine the veracity of scientific claims
by retrieving and analysing evidence from research literature. The problem is
inherently more complex than general fact-checking since it must accommodate
the evolving nature of scientific knowledge, the structural complexity of
academic literature and the challenges posed by long-form, multimodal
scientific expression. However, existing approaches focus on simplified
versions of the problem based on small-scale datasets consisting of abstracts
rather than full papers, thereby avoiding the distinct challenges associated
with processing complete documents. This paper examines the limitations of
current scientific fact-checking systems and reveals the many potential
features and resources that could be exploited to advance their performance. It
identifies key research challenges within evidence retrieval, including (1)
evidence-driven retrieval that addresses semantic limitations and topic
imbalance (2) time-aware evidence retrieval with citation tracking to mitigate
outdated information, (3) structured document parsing to leverage long-range
context, (4) handling complex scientific expressions, including tables,
figures, and domain-specific terminology and (5) assessing the credibility of
scientific literature. Preliminary experiments were conducted to substantiate
these challenges and identify potential solutions. This perspective paper aims
to advance scientific fact-checking with a specialised IR system tailored for
real-world applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [52] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: 提出一种基于Datalog的智能打地基方法，有效管理ASPIC+第一阶规则的复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决ASPIC+中第一阶规则推理的计算复杂性问题。

Method: 将第一阶ASPIC+实例转化为Datalog程序，并采用特定简化策略以优化推理。

Result: 实验证明方法具有良好的可扩展性，能有效控制打地基的规模。

Conclusion: 提出的方法改善了基于规则的论证推理的效率，为ASPIC+的实际应用提供支持。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [53] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 引入多代理机制的算法追索框架，优化多利益相关者中的社会福利和行动性。


<details>
  <summary>Details</summary>
Motivation: 现有算法追索多为单一个体和模型，难以应对多利益相关者的复杂场景。

Method: 将多对多交互模型化为有容量限制的加权二分匹配问题，设计三层优化框架。

Result: 实验证明新框架实现了近最优福利同时最小化系统调整。

Conclusion: 从个体推荐扩展到系统设计，有助提升社会整体福利并保持行动性。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [54] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 本文提出了一种结合深度学习的自动放疗计划方法，通过引入Transformer-based L2O逆优化器和PPO虚拟规划器，显著提升了头颈癌质子治疗计划的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 减少手工调节和计算密集的逆优化过程，实现自动化高质量治疗计划，提升临床效率。

Method: 开发基于Transformer的L2O逆优化器，结合PPO策略网络作为虚拟规划器，通过学习任务特定数据，实现自动目标参数调整和剂量预测。

Result: 所提方法在97例临床数据上优于传统L-BFGSB方法，效率提升36.41%，效果提升22.97%。生成的治疗计划质量优异或可比，且生成时间显著缩短。

Conclusion: 该方法实现了高效、自动化、定制化的放疗计划，为临床治疗提供有力支持，具有推广应用前景。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [55] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文扩展了假设基础论证中的可采性概念，提出了强、弱可采性的语义，并分析了它们的属性和局限性。


<details>
  <summary>Details</summary>
Motivation: 为了丰富假设基础论证中的可采性研究，特别是在非平坦框架中的应用。

Method: 引入抽象两极集合论证框架，定义强、弱可采性及对应的语义，并分析其性质。

Result: 提出了强可采性在ABA中的定义，扩展了弱可采性在非平坦ABA中的研究，验证了模块化性质，分析了其局限性。

Conclusion: 强、弱可采性在非平坦ABA中具有一定的表达能力，但也存在类似标准可采性的问题，需进一步研究改进措施。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [56] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 提出一个新的含有不完整信息的问题数据集，用于评估大型推理模型在主动获取信息方面的能力，发现现有模型存在过度思考和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 推动大型推理模型超越简单问题解决，具备主动提问获取信息的能力，以实现更具智能的表现。

Method: 构建包含多样背景的不完整问题数据集，系统评估LRMs的表现，分析模型的行为特征，探讨监督微调的潜力。

Result: 发现LRMs在主动提问方面表现不足，存在过度思考和幻觉行为，揭示微调的潜在和挑战。

Conclusion: 强调开发具有主动信息获取能力的LRMs的重要性，推动向更具真正智能的方向发展。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [57] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: 提出了一种适应动态知识图谱的逐步演化框架SAGE，能根据更新规模调整嵌入维度，并利用动态蒸馏机制平衡保持已学知识与整合新事实，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱随时间演化带来的嵌入更新问题，特别是不同更新规模带来的挑战。

Method: 根据更新规模调整嵌入维度，采用动态蒸馏机制平衡知识保持与新知识整合。

Result: 在七个基准数据集上，SAGE持续优于现有方法，性能提升明确，特别是在不同快照中表现优异。

Conclusion: 自适应嵌入维度和动态蒸馏机制对动态知识图谱嵌入具有重要意义，能有效提升更新效率和精度。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [58] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: 提出了一种结合课程学习和组相对策略优化的GUI交互强化学习框架CRAFT-GUI，有效提升任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有RL方法在GUI任务中难以适应多样化难度和奖励稀疏的问题。

Method: 设计了基于分组相对策略优化的课程学习框架和细粒度的奖励函数，结合rule-based和模型评判信号。

Result: 在Android Control和内部基准测试中超越前沿方法，分别提升5.6%和10.3%。

Conclusion: 融合强化学习与课程学习显著改善GUI任务中的智能代理表现。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [59] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 引入AIM-Bench基准测试LLM在供应链决策中的偏差表现及缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在不确定环境下的库存决策能力及潜在偏差，满足实际应用需求。

Method: 设计多样化库存补货实验，通过AIM-Bench评估LLM的决策偏差与缓解策略。

Result: 不同LLM表现出类似人类的偏差，且通过认知反思和信息共享可减缓偏差影响。

Conclusion: 强调在供应链库存决策中考虑偏差问题，推动开发人性化决策支持系统。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [60] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: 提出Inclusion Arena，一个基于人类反馈的实时模型排名平台，通过创新的排序机制提升模型评价的真实反映和稳定性，促进实用导向的AI发展。


<details>
  <summary>Details</summary>
Motivation: 弥补现有基准和排行榜在反映实际应用性能方面的不足。

Method: 结合自然用户交互和对比学习，应用Bradley-Terry模型配合新颖技术实现模型排名。

Result: 实现可靠稳定的模型排名，增强真实性和抗操控性，加速应用导向的AI模型发展。

Conclusion: 通过开放平台促进基础模型与实际应用的结合，推动AI生态的优化。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [61] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 引入概率地标概念，改进UCT算法以提升在线随机规划的表现，显示合理的地标选择能显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 探索如何在随机域中有效利用地标以提升规划效率。

Method:  formalize probabilistic landmarks，改进UCT算法，作为子目标辅助分解MDPs，平衡贪婪与长远目标。

Result: 在基准域中，合理选择的地标显著改善UCT在在线概率规划中的性能，且最佳平衡依赖问题。

Conclusion: 地标可为MDP的随时算法提供有价值的引导，有助于提升规划效率和效果。

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [62] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 提出一种结合领域知识的LLM辅助规划器，通过问题分解和两种不同的利用LLM的方法，有效缩小搜索空间解决大规模规划问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模规划问题中状态空间过大的挑战。

Method: 将大问题分解为子任务，利用两种LLM辅助方法（LLM4Inspire和LLM4Predict）进行启发式引导和中间条件推断。

Result: 验证表明该方法在多个领域中有效缩小搜索空间，特别是结合领域知识的LLM4Predict表现优越。

Conclusion: 结合领域知识的LLM可以显著提升大规模规划问题的解决效率，显示出潜力。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 提出了一种基于合作博弈的多准则加权方法，用于优化集成学习中的投票集成，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有投票集成方法中只考虑单一评价标准的问题，全面利用多类信息以改进模型性能。

Method: 利用合作博弈理论在多准则情境中对分类器的预先信息进行加权，结合Open-ML-CC18数据集进行实验验证。

Result: 所提方法在实验中优于传统加权方法，表现出更好的性能。

Conclusion: 基于合作博弈的多准则加权策略能够有效整合多类信息，改善集成模型的性能，具有广泛应用前景。

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [64] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker在保持较低内存和计算成本的同时，表现出与更大模型相当甚至优越的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型因内存和计算成本高而难以在实际企业场景中应用的问题。

Method: 采用四阶段训练流程，包括模型放大、持续预训练、监督微调和基于GRPO的强化学习。

Result: 模型在多个基准测试中表现优异，达到或超过32亿参数模型的性能，且模型尺寸更小。

Conclusion: 该模型在效率和性能之间实现了良好平衡，有助于大规模语言模型的企业应用推广。

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [65] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 提出一种基于提示的持续学习方法，有效应对医疗领域中的数据分布偏移和知识遗忘问题，在多项数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域中数据隐私限制导致的分布偏移和持续学习中的知识遗忘问题。

Method: 引入提示池和有限扩展策略，通过扩展和冻结部分提示实现模型更新，并设计正则化项平衡记忆与适应。

Result: 在三个糖尿病视网膜病变数据集上，比现有最优方法提高了至少10%的分类准确率和9个百分点的F1-score，且降低了推理成本。

Conclusion: 提示式持续学习方法有望推动医疗AI的持续进步，实现实时诊断和远程医疗应用。

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [66] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 本文提出一种相对优势去偏框架，用于校正视频观看时间，提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 由于观看时间受到视频时长、受欢迎程度等因素的影响，可能导致偏差，影响推荐效果。

Method: 采用基于引用分布的分位数偏差校正方法，结合两阶段架构和分布嵌入技术。

Result: 该方法在离线和在线实验中明显优于现有基线方法，提升了推荐的准确性和鲁棒性。

Conclusion: 通过相对优势校正，改善了观看时间作为用户偏好指标的偏差，提高了推荐系统的性能。

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [67] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert通过结合大语言模型和专业模型，利用强化学习实现可解释的逆合成预测，改善黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖静态匹配，限制逻辑决策能力，需提升可解释性与决策合理性。

Method: 融合大语言模型与专业模型的合作推理，使用强化学习优化决策策略，生成化学逻辑的自然语言解释。

Result: Retro-Expert在多项指标上优于单一模型，提供专家级解释，增强预测可靠性与可解释性。

Conclusion: 该框架有效弥合AI预测与化学理解之间的鸿沟，推动化学合成的智能化发展。

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [68] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: 提出了一种基于层次图神经网络和注意机制的时尚推荐框架FGAT，有效提升了个性化推荐和搭配兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着时尚行业的快速发展，用户难以在电商平台上找到适配的商品，提升推荐系统的性能成为迫切需求。

Method: 构建涉及用户、搭配和商品的三层层次图，结合视觉和文本特征，通过图注意机制动态加权节点，提高模型对关键交互的捕捉能力。

Result: 在POG数据集上，FGAT优于HFGN等基线模型，在准确率、召回率和NDCG等指标上表现优异。

Conclusion: 结合多模态特征、层次图结构与注意机制显著提升了个性化时尚推荐系统的效果和效率。

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [69] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb是一种高质量合成数据生成框架，通过优化多个因素显著提升了预训练数据质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 弥补大规模语言模型预训练中数据量增长带来的收益递减问题，探索合成数据的潜力。

Method: 引入BeyondWeb框架，结合多因素优化和实践经验，提升合成数据质量，同时提升模型训练效率。

Result: 在多项基准测试中优于现有的合成预训练数据集，显著提升模型性能和训练速度，单模型效果超越规模更大的模型。

Conclusion: 高质量合成数据的生成依赖于多方面因素的共同优化，科学与实践结合是关键，提供了未来预训练数据构建的重要启示。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [70] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 提出了一种用于Text-to-image模型选择的框架M&C，通过匹配图预测最佳微调模型，有效提升模型选择效率。


<details>
  <summary>Details</summary>
Motivation: 在众多预训练的T2I模型中，如何快速准确选择最适合目标数据域的模型成为一大挑战，当前缺乏有效的评估工具。

Method: 构建包含模型、数据和性能信息的匹配图，利用图嵌入特征预测微调后最佳模型。

Result: 在多个模型和数据集上验证，M&C在预测最佳模型方面成功率达61.3%，优于三种基线方法。

Conclusion: 提出的M&C框架有效帮助用户在无需全部微调的情况下，快速找到最适合特定任务的预训练T2I模型，促进模型的高效应用。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [71] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: 提出CURE框架，有效防止entropy崩溃，提高LLMs在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR中静态初始状态采样导致的模型行为过于确定性和entropy崩溃问题，增强模型探索能力。

Method: 引入两阶段训练策略，第一阶段通过重生成高entropy关键token促进新颖探索，第二阶段利用静态采样进行稳步强化利用。

Result: 在Qwen-2.5-Math-7B模型上实现了6个数学基准上的性能提升5%，显著优于其他RLVR方法，达到state-of-the-art。

Conclusion: CURE有效平衡探索与利用，提升数学推理中的模型性能，验证了其优越性。

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [72] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 该论文扩展了强彩票假设（SLTH）到量化神经网络，利用数论基础结果证明了在有限精度下网络的可表达性和所需表示规模的最优界限。


<details>
  <summary>Details</summary>
Motivation: 理解量化对神经网络的理论基础，提供更有效的网络剪枝和量化策略。

Method: 基于博尔克斯等人在数区划问题上的基础结果，拓展到随机子集和问题，并将其应用到SLTH框架。

Result: 证明在量化设置中，目标离散神经网络可以被精确表示，且给出了最优的参数过参数化边界。

Conclusion: 将SLTH扩展到有限精度，揭示量化神经网络的理论潜力，促进实际应用中的网络压缩与效率提升。

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [73] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: 引入zono-符合预测，提供有效的多维输出依赖建模与计算效率提升方法。


<details>
  <summary>Details</summary>
Motivation: 当前符合预测方法计算成本高、数据需求大，受限于预测集为区间，难以捕捉多维输出中的依赖关系。

Method: 采用zonotope构建预测集合，通过线性规划实现模型挑选，兼容非线性基础预测器，涵盖分类和回归任务。

Result: zono-符合预测在测试中比传统方法更少保守，保持类似的覆盖率，并能有效检测异常点。

Conclusion: zono-符合预测为竞争性且高效的多维不确定性量化工具，拓展了符合预测的应用范围。

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [74] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 本文提出了一种关于信心的概念，用于学习和更新信念，强调其与概率的区别，并给出正式定义和测量方式。


<details>
  <summary>Details</summary>
Motivation: 理解学习中信心的作用，以及其与传统概率的不同，为提升学习算法的表现提供新思路。

Method: 通过形式公理化信心的定义，提出两种连续测量方法，并推导了其在向量场和损失函数中的表示，并将贝叶斯规则作为特殊优化情况。

Result: 成功将信心建模为一种独立的概念，并实现了其在不同数学框架下的表征，拓展了多观测融合的理论工具。

Conclusion: 信心是一个独立于概率的关键概念，可以在各种学习框架中被形式化，并通过优化实现其最大化，从而丰富了学习理论的表达力。

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [75] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 本文提出一种用于非高斯分布的条件独立结构推断的新方法，适用于广义非paranormal分布，并提供高效的算法实现。


<details>
  <summary>Details</summary>
Motivation: 探索如何在非高斯分布中利用精度矩阵反映变量独立性，填补高斯模型之外的研究空白。

Method: 定义广义非paranormal及其变换，通过理论证明精度矩阵在特定条件下反映条件独立结构，并提出高效算法加以恢复。

Result: 算法在合成和真实数据中效果良好，验证了其在非高斯情形下的适用性和有效性。

Conclusion: 该方法扩展了高斯模型的条件独立性推断框架，为分析非高斯数据提供了有力工具。

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [76] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: 本文研究了LIME和SHAP等事后解释方法在模型偏见检测中的脆弱性，并提出了改进策略以增强其鲁棒性，从而提高高风险场景下的模型透明度。


<details>
  <summary>Details</summary>
Motivation: 随着黑箱模型被广泛应用，解释方法的鲁棒性成为确保模型公正性的重要问题，现有方法易受对抗操控影响。

Method: 复制COMPAS实验验证偏见检测现状，构建模块化测试框架评估不同增强和集成的解释方法在各种模型上的表现，特别是对偏见隐藏的抵抗能力。

Result: 识别出几种显著提升偏见检测能力的配置，为高风险应用中的模型透明性提供技术支持。

Conclusion: 改进的解释方法配置有效增强了模型偏见的检测能力，有助于在实际部署中提升模型的公正性和可信度。

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [77] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 提出了一种考虑丰度的Set Transformer变体，用于生成微生物组样本的固定大小表征，优于传统平均池化方法，提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 改善微生物组样本表征，确保其在下游任务中的生物学相关性和表现。

Method: 在不修改模型架构的情况下，通过仿真丰度与序列嵌入的乘积，并采用基于自注意的聚合方式，构建丰度感知的样本嵌入。

Result: 该方法在微生物组分类任务中优于平均池化和未加权的Set Transformer，在某些情况下实现了完美的性能。

Conclusion: 引入丰度信息的聚合策略不仅改善了微生物组样本的表示，也为Transformer模型在生物信息学中的应用开辟了新途径。

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [78] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: 该论文提出了一种基于数据管理流程和逻辑回归的预测编码方法，用于分析即时消息，以解决其非正式和小样本带来的挑战，并实现成本节约。


<details>
  <summary>Details</summary>
Motivation: 研究在法律行业中采用机器学习进行文件分类时，面对非正式即时消息数据的特殊挑战。

Method: 通过将消息分组为日间聊天、特征筛选、逻辑回归分类和维度缩减，构建经济高效的预测编码模型。

Result: 在富含定量信息的Instant Bloomberg数据集上验证了方法的有效性，并展示了显著的成本节约。

Conclusion: 提出的方法有效应对即时消息数据的独特挑战，并具有实际应用价值。

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [79] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 提出一种基于神经网络的压缩元学习框架，提升压缩学习的编码和解码效率与效果，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 随着大数据集规模的快速增长，传统参数学习方法效率不足，亟需更快速、更高效的压缩学习技术。

Method: 使用神经网络对压缩编码和解码两个阶段进行元学习，优化过程以提升速度和准确性。

Result: 在压缩PCA、岭回归、k-means和自编码器等应用中验证了该框架的优势，表现优于现有方法。

Conclusion: 通过神经网络的元学习，有效改善压缩学习的性能，促进其在大规模数据处理中的应用。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [80] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 提出一种多模态系统用于早期预测ICD编码，通过融合临床笔记和电子健康记录事件，结合预训练编码器和交叉模态注意力机制，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 早期预测医疗编码有助于风险识别和资源优化，但现有研究多关注出院后分类，缺乏有效的早期预测模型。

Method: 利用预训练编码器、特征池化和交叉模态注意力机制融合临床笔记与表格事件，设计加权时序损失，提升模型在早期预测中的表现。

Result: 该方法在实验中优于现有最优系统，有效改善早期预测性能。

Conclusion: 多模态融合与加权时序损失能显著增强早期预测模型，有助于临床早期干预和管理。

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [81] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: 本文研究了在监督学习中分段线性正则化（PAR）在优化和统计方面的基础，揭示了其在过参数化和统计性能上的优势，以及解决算法和建模的具体方法。


<details>
  <summary>Details</summary>
Motivation: 解决离散或量化变量的优化难题，探索PAR的理论基础与应用潜力。

Method: 通过分析关键点性质、推导封闭形式的邻近映射及其在不同优化方法中的应用，结合统计分析验证其效果。

Result: 证明了在过参数化条件下，PAR的关键点具有高度量化特性；提出了多种优化算法的解决方案；验证了PAR在统计学上的等效性和保障。

Conclusion: PAR为离散优化问题提供了灵活的建模工具，其理论和算法基础为未来量化学习任务提供重要支撑，有望促进在复杂模型中的广泛应用。

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [82] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 提出了一种新颖的元学习方法CTRL，有效结合跨域残差学习和自适应池化/聚类，优化多源数据中的整体性能与源间差异保留。


<details>
  <summary>Details</summary>
Motivation: 多源数据带来的预测任务中，需兼顾整体准确性与源内异质性，现有方法难以兼顾两者。

Method: 结合跨域残差学习与自适应聚类技术，提出CTRL，实现跨源残差调整和源间异质性保持。

Result: 在五个大规模数据集上，CTRL优于现有方法，特别是在瑞士庇护项目数据集上表现出色，增强了预测的可靠性与差异化。

Conclusion: CTRL有效解决多源数据中数据量与质量的权衡，提升多源预测任务中的性能与可靠性。

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [83] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 提出高阶贝叶斯网络分类器NeuralKDB，通过学习特征值的分布表示，显著提升复杂数据的概率推断能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统贝叶斯网络分类器在高阶特征依赖建模中的局限性。

Method: 引入分布表示学习，扩展K-dependence贝叶斯分类器为神经网络版本，结合随机梯度下降训练。

Result: 在60个UCI数据集上表现优异，优于传统贝叶斯分类器和其他神经网络分类器。

Conclusion: NeuralKDB有效捕捉高阶特征依赖，提升分类性能，具有较大应用潜力。

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [84] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种针对多模态联邦学习中模态数量和质量不平衡问题的Р算法，通过理论分析和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的智能化和多模态数据的增长，现有的联邦学习面临模态不平衡带来的性能挑战。

Method: 提出模态数量和质量平衡（QQR）算法，并在真实多模态数据集上进行实验验证。

Result: QQR算法在面对模态不平衡时表现优越，有效提升了联邦学习的性能。

Conclusion: 该研究为解决多模态联邦学习中的模态不平衡问题提供了有效方案，具有实际应用价值。

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [85] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出一种半监督多视图学习模型，结合变分方法和跨视图互信息最大化，有效应对缺失视图和标注不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决多视图学习中视图缺失和标注不足的挑战，提升模型性能。

Method: 引入半监督生成模型，最大化未标记样本的似然，同时在潜在空间中进行跨视图互信息最大化。

Result: 在图像和多组学数据上实现了优于现有方法的预测和插补性能，效果明显。

Conclusion: 提出的方法有效结合有标记和无标记数据，增强多视图模型的鲁棒性和性能。

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [86] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 利用量子Boltzmann机与变分自编码器结合，突破深度学习中高斯先验的局限性，有效分析复杂生物数据。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中对高斯先验的依赖，提升模型对复杂、非高斯自然数据的描述能力。

Method: 引入量子-经典混合架构QBM-VAE，利用量子处理器高效采样Boltzmann分布，作为潜在空间的先验。

Result: 在百万级单细胞数据集上，该模型优于传统高斯模型，提升了复杂生物结构的保持能力，增强了数据整合、细胞类型分类和轨迹推断等任务表现。

Conclusion: 该研究展示了量子技术在大规模科学问题中的应用潜力，为深度学习引入物理先验提供了新路径，并为发展混合量子AI模型提供了蓝图。

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [87] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 本文提出了一种调制式元学习框架，用于参数变化的结构保持动力学模型，能在少样本条件下实现准确预测且保持物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统结构保持模型在新参数条件下需再次训练，效率低且不便推广。

Method: 引入基于调制的元学习策略，利用潜在表示条件化模型，无需系统参数知识或优化，仅通过调制策略实现参数适应性。

Result: 在标准基准测试中表现优异，实现少样本学习且确保物理稳定性与广泛泛化能力。

Conclusion: 该方法通过调制机制增强结构保持模型的适应能力，有助于动态系统的高效泛化与应用推广。

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [88] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: 通过对比多模态框架BFF，提升早期儿童疾病风险预测的性能。


<details>
  <summary>Details</summary>
Motivation: 儿童风险评估通常跨多个阶段进行，早期预测准确性低，希望增强早期风险预测的性能。

Method: 提出BFF框架，将每个时间窗口作为不同模态，通过对比学习借由后期数据信息优化早期预测。

Result: 在两个真实数据集上验证，BFF在早期风险评估中表现出持续改善。

Conclusion: BFF能有效利用未来信息，提升儿童早期风险预测的准确性，具有潜在临床应用价值。

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [89] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 本文探讨了因果抽象在认知行为计算实现中的作用，强调其在表示和泛化中的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解系统中实现特定计算的条件，特别是在神经网络中的应用。

Method: 利用因果抽象理论，结合深度学习中的实例，分析计算与表现的关系。

Result: 提出了基于因果抽象的计算实现框架，强调表示在泛化与预测中的作用。

Conclusion: 因果抽象为理解认知和机器学习中的计算提供了有益的视角，有助于推动泛化和预测能力的研究。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [90] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 提出了一种结合CNN和LSTM的空气质量PM2.5指数预测模型，有效提升了预测准确性，但存在计算资源消耗大的问题。


<details>
  <summary>Details</summary>
Motivation: 应对全球气候变化对空气质量监测的需求，特别是PM2.5浓度的准确预测，以支持环境保护和公共健康决策。

Method: 利用卷积神经网络提取空间特征和长短期记忆网络建模时间依赖性，基于北京工业区多变量数据进行训练。

Result: 模型在预测准确性方面优于传统模型，RMSE达5.236，表现出良好的泛化能力。

Conclusion: 该模型在实际应用中具有巨大潜力，但需优化以降低计算资源需求并增强对复杂气象变量的支持。

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [91] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: 提出了一种增强版交互式投票地图匹配算法，有效处理不同采样率的轨迹，实现高精度轨迹重建，广泛适用于任何OpenStreetMap覆盖地区。


<details>
  <summary>Details</summary>
Motivation: 解决GPS轨迹数据质量参差不齐导致的匹配精度问题，提升轨迹重建的准确性和算法的适应性。

Method: 在原有算法基础上引入轨迹插补、距离受限的交互投票策略和缺失数据修正，结合OpenStreetMap资源进行跨地区应用。

Result: 算法性能显著提升，能在多种数据质量条件下实现精准轨迹重建，适用范围扩展到全球不同地区。

Conclusion: 增强版算法保持原有优势的同时，显著提升实用性和普遍适用性，有助于复杂环境中的GPS轨迹分析。

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [92] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: GODNF通过统一不同的观点动力学模型，实现了一个可训练、具有异质扩散和时间动态的深层图神经网络框架，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 为克服现有扩散基图神经网络在适应性、深度限制和理论理解方面的不足，提出一个更通用且可解释的模型。

Method: 将多种观点动力学模型整合到一个可训练的扩散机制中，利用节点特定行为和动态邻域影响捕捉异质扩散和时间动态，同时确保深层传播的效率与可解释性。

Result: 理论分析验证了GODNF的模型能力，实证测试在节点分类和影响估计任务中优于现有最先进的GNN方法。

Conclusion: GODNF为深层图神经网络提供了一个统一、灵活且理论坚实的框架，显著提升性能和理解能力。

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [93] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 提出一种基于提示的公平分类框架，能在闭源大模型中实现公平目标，优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在多领域应用，确保其公平性尤为重要，但现有方法难以在闭源模型中实现公平。

Method: 将LLM作为特征提取器，通过设计提示采集满足公平标准的统计特征，再应用公平算法训练轻量级分类器。

Result: 在五个数据集上验证了框架的有效性，展现了较优的准确性与公平性折衷，同时更数据高效。

Conclusion: 基于提示的公平分类框架适用于闭源LLMs，具有广泛应用潜力和优越性能。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [94] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 提出一种名为RTE的训练框架以提升脉冲神经网络的抗扰能力，通过多时间步的自我集成减少扰动的传递与脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探究脉冲神经网络在面对对抗性扰动时的脆弱性，因其在能效和类脑计算中的潜力未被充分理解。

Method: 将网络视为在不同时间步演变的子网络，提出RTE框架以增强各子网络的鲁棒性并降低扰动的时间传递，通过联合损失和随机采样优化。

Result: RTE在多个基准测试中优于现有方法，改善鲁棒性与准确性的权衡，内在鲁棒性显著提升，决策边界多样化。

Conclusion: 时间结构在对抗学习中关键，RTE为构建更抗扰的脉冲神经网络提供理论基础和实践方案。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [95] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 提出一种新的图预训练和提示调优方法HS-GPPT，通过确保谱对齐，有效适应不同同质性特征的图，增强知识转移效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多样谱分布图上效果受限，难以在不同同质性图中实现有效知识转移。

Method: 利用混合谱滤波器和局部-global对比学习，设计提示图实现谱分布对齐，结合谱特性原则优化模型性能。

Result: 实验验证了方法在转导和归纳学习两种场景下的有效性，提升了谱知识传递能力。

Conclusion: 谱对齐是实现多样谱分布图有效知识转移的关键，HS-GPPT框架通过谱滤波和提示图实现这一目标。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [96] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS是一种结合市场状态感知的深度学习架构搜索方法，通过引入贝叶斯搜索空间、特殊动态神经模块和多目标损失函数，有效提升加密货币交易的预测准确性和模型适应性，在实证中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在动态金融市场中性能有限，迫切需要考虑市场状态变化以提高预测效果。

Method: 引入贝叶斯搜索空间、设计分市场状态的动态激活神经模块、构建结合市场特定惩罚的多目标损失，并利用多头注意力进行市场状态识别。

Result: 在真实比特币数据集上，显著优于现有模型，减少80.3%的MAE，训练速度快5倍以上，各组件的贡献得到验证。

Conclusion: 结合市场状态的深度架构搜索是提升金融模型鲁棒性和适应性的关键，应将领域知识整合入NAS流程。

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [97] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 提出TACP和sTACP方法以改善长尾数据中的置信预测的类间覆盖不平衡问题，理论证明其优势并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 需要解决长尾分布下Conformal Prediction在尾类别覆盖不足的问题，以提升模型在少数类的预测可靠性。

Method: 利用长尾结构设计TACP，缩小头部与尾部之间的覆盖差异，通过重加权机制扩展为sTACP，以优化整体覆盖平衡。

Result: TACP与sTACP在多个长尾基准数据集上有效改善了类间覆盖平衡，具有理论保证和实际验证。

Conclusion: 提出的方法成功缓解了长尾场景中的覆盖不平衡，增强了置信预测的公平性和可靠性。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [98] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: NeMo方法实现了面向各种DNN结构的模块化训练，通过对神经元级别进行对比学习，有效提升模型的可扩展性和通用性，显著减少模型大小并提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络在软件系统中的广泛应用，模型构建成本高成为挑战，模块化重用能降低成本，但现有方法多局限于小规模模型或特定架构。

Method: 提出NeMo，通过神经元级别的对比学习实现可扩展的模块化训练，适用于多种架构，包括Transformer和CNN，设计了复合损失函数以增强训练效果。

Result: 在多个Transformer和CNN模型上实验结果显示，NeMo在模块识别准确率和模型规模缩减方面优于现有方法，平均提升1.72%的准确率和减少58.10%的模型体积。

Conclusion: NeMo在实现大规模、多架构的深度神经网络模块化训练方面具有显著优势，为实际应用提供了可行的解决方案。

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [99] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 本文通过构建包含全球有机植树造林项目的庞大数据集，并引入位置数据完整性指标，提高碳减排项目的透明度和可信度，同时为遥感验证与机器学习应用提供基础数据。


<details>
  <summary>Details</summary>
Motivation: 为应对碳市场中项目数据的真实性和完整性问题，提升环境项目的透明度和可信性。

Method: 整合一手和二手数据，利用卫星影像和地理位置评估提出地点数据完整性指数（LDIS）。

Result: 涵盖33年、来自45,628个项目的1289,068个植树点，发现79%地点在位置完整性上存在问题，15%项目缺乏可用的地理数据。

Conclusion: 该数据集增强了碳市场的责任追究，提高了遥感验证的基础，也为机器学习等技术应用提供了宝贵资源。

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [100] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: 介绍了一种针对数据流中的类别不平衡问题的训练方法——和谐梯度下降（HGD），通过平衡梯度范数改善偏少类别的学习效果，简洁高效，无需额外参数或数据缓冲，理论和实验均验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决数据流中类别不平衡带来的学习困难，改进现有策略的不足。

Method: 提出和谐梯度下降（HGD）算法，通过平衡不同类别的梯度范数，实现模型的平衡学习。

Result: HGD在多种不平衡数据流场景下表现优越，具有理论保证（子线性遗憾界）和实践有效性。

Conclusion: HGD是一种简洁高效的优化改进方法，有望广泛应用于不平衡数据流学习场景中。

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [101] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 通过引入基于熵的机制提升测试时强化学习的探索与利用平衡，有效改善大模型在无监督推理中的性能表现，同时控制推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在无标注数据下表现有限和依赖依赖标注的问题。

Method: 提出熵分叉树多数投射(ETMR)和基于熵的优势重塑(EAR)，以增强TTRL中的探索与利用平衡。

Result: 在AIME 2024基准测试中，Llama3.1-8B模型的Pass@1指标提升68%，同时只使用了60%的推理令牌预算。

Conclusion: 该方法有效优化推理效率、输出多样性和估值鲁棒性，推动了无监督强化学习在开放域推理中的应用。

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [102] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: PTS模型通过双分支掩码机制和信息论约束，有效实现跨被试EEG解码的个性化与共享特征提取，显著提升零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试EEG解码中的巨大个体差异和缺乏不变特征的问题。

Method: 引入双分支掩码机制与信息论约束，分离任务相关与被试相关的潜在表示，结合多目标训练优化模型性能。

Result: 在多项运动想象数据集上表现优异，优于现有方法，成功实现无需被试特定校准的泛化性能。

Conclusion:  disentangled特征表达有助于实现个性化且具有良好迁移性的脑-机接口解码模型。

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [103] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 该论文提出了通过最小化代理损失而非直接最小化后悔的方法，有效改善了线性规划中决策导向学习的梯度问题，提高了效率和结果表现。


<details>
  <summary>Details</summary>
Motivation: 解决线性规划中梯度几乎为零的问题，提升决策导向学习的效果。

Method: 提出在使用可微优化层的情况下，仍然通过最小化代理损失来优化模型，而非依赖平滑或子梯度方法。

Result: 实验结果显示，此方法在后悔值方面优于或等于现有方法，同时降低了训练时间。

Conclusion: 通过最小化代理损失，可以在保证或提升决策质量的同时，提高训练效率，适用于包括DYS-Net在内的多种可微优化技术。

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [104] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: DFA融合个体奖励与偏好，提升强化学习的效果和稳定性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中偏好和奖励融合的问题，以实现更稳定和高效的训练。

Method: 引入Dual-Feedback Actor算法，结合策略对偏好的直接建模，利用偏好信息和Q值优化策略。

Result: 在多个控制任务中表现优异，尤其在偏好信息有限时优于传统RLHF，接近理想奖励的性能。

Conclusion: DFA有效融合偏好信息，提升训练稳定性和性能，有望在复杂环境中广泛应用。

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [105] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 提出一种基于Forman-Ricci曲率的结构提升策略，有助于缓解图神经网络中的信息压缩问题。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在处理复杂、长距离信息传递中出现的信息扭曲和过度压缩问题。

Method: 利用Forman-Ricci曲率定义的边特征对数据进行提升，将基础图转换为更具表达力的拓扑结构，特别是引入超边以连接远距离的社区。

Result: 该方法改善了长距离信息传递中的信息流失现象，增强了图结构的表达能力。

Conclusion: 基于Forman-Ricci曲率的结构提升策略有效缓解了消息传递中的信息扭曲，提升了图神经网络的性能。

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [106] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合SFT和RL的CHORD框架，通过动态加权实现离线与在线学习的有效融合，改善模型性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决将SFT与RL结合时可能造成模型行为破坏和过拟合的问题。

Method: 将SFT融入在on-policy RL过程中，设计双控机制以调节离线和在场学习的影响。

Result: 在多个基准测试中表现优异，验证了CHORD的稳定性和效率。

Conclusion: CHORD通过动态调节离场与在线学习的关系，成功提升了大模型的训练效果与稳定性。

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [107] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 本文提出一个基于学习理论的框架，用于评估局部后验解释算法的有效性，发现许多流行算法对复杂模型缺乏信息性，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 目前关于复杂模型的局部解释缺乏理论保证，亟需评估解释算法的有效性。

Method: 引入数学框架，定义解释的“信息性”，分析不同算法在复杂模型中的表现，并提出改进条件。

Result: 许多流行解释算法在复杂模型中不具信息性，如梯度、SHAP等，提出改进措施。

Conclusion: 解释算法在实务中应用需考虑其信息性，尤其在审计与高风险场景中。这一理论分析指导解释算法的改进。

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [108] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: 提出LEAD方法，通过在潜在空间优化抗体的序列和结构，提升设计效率和性能，特别适用于非可微评估场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有抗体设计方法在优化过程中效率低、受限于非同步模态设计的问题。

Method: 利用联合模态的潜在空间进行序列和结构的共设计，并引入黑箱引导策略提高优化效率。

Result: LEAD在优化性能方面优于现有方法，能显著降低查询成本，同时实现多目标优化。

Conclusion: LEAD通过共享潜在空间实现抗体序列和结构的高效同步优化，适应实际非可微评价环境，具有广泛应用潜力。

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [109] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 提出通过收缩理论增强卷积神经常微分方程网络的鲁棒性，实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络易受噪声和对抗攻击影响，需提升其鲁棒性。

Method: 引入收缩理论，通过正则化诱导网络收缩性质，采用不同技术减轻计算负担，在MNIST和FashionMNIST上验证。

Result: 所提出的正则化策略增强了NODEs在噪声和攻击下的鲁棒性，表现优异。

Conclusion: 利用收缩理论为连续深度网络提供鲁棒性解决方案，有效提升性能。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [110] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: GraphOracle 提出了一种新颖的自解释图神经网络框架，通过联合学习分类器和判别性子图，有效生成和评估类别级解释，克服了现有方法的不足，具有较高的保真度、可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 增强图神经网络的可解释性，确保其安全和公平的部署。

Method: 引入GraphOracle框架，联合学习分类器和结构化稀疏子图，采用新颖的训练和评估策略。

Result:  GraphOracle在图分类任务中表现出优越的保真度、可解释性和扩展性，优于现有方法，并避免了计算瓶颈。

Conclusion: GraphOracle是一种实用且原则性的类别级自解释解决方案，提升了GNN的可信度和应用价值。

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [111] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: 提出了一种基于Reservoir Computing的多感官认知连接模板框架mCOCO，能够有效捕捉功能连接和认知特性，优于传统的GNN方法。


<details>
  <summary>Details</summary>
Motivation: 弥补现有连接模板方法在可解释性、计算效率和认知能力方面的不足。

Method: 利用Reservoir Computing结合多感官输入，构建个体和群体水平的功能连接模板，并强化认知特征建模。

Result: mCOCO显著优于GNN，在中心性、判别性、拓扑结构和多感官记忆方面表现更优。

Conclusion: mCOCO是一种高效、可解释且具认知特性的连接模板生成方法，有潜力推动脑连接组学研究的发展。

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [112] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 提出一种整合数据分析流程的全面可解释AI框架（HXAI），强调用户中心和多学科融合，以增强AI系统的透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 解决现有可解释AI方法只关注单个预测而忽略整个数据分析流程中的决策和质量控制的问题，提升AI系统的整体可解释性和信任度。

Method: 构建包含六个组成部分的全面框架（数据、分析设置、学习过程、模型输出、模型质量、沟通渠道），设计112项问题库，结合人类解释理论、HCI原则和用户研究，开发AI代理协同多种解释技术，提供定制化叙事。

Result: 提出HXAI框架及其操作性工具，验证其在不同场景中的有效性，改善用户对AI模型的理解和信任，明确不同用户需求的解释特征，填补现有工具的空白。

Conclusion: HXAI实现了端到端的AI透明度提升，为责任AI部署提供理论基础和实践路径，融合多学科知识促进AI的公平、可信和有效应用。

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [113] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: 该研究通过比较六种概率机器学习算法在不确定性估计中的表现，发现深度学习方法在识别出分布外数据时缺乏一致性校准。


<details>
  <summary>Details</summary>
Motivation: 强调统计方法在科学发现中的重要性，以及在复杂模型中不确定性估计的挑战。

Method: 采用近似贝叶斯推断框架，通过合成数据集测试六种算法的类概率和不确定性估计性能。

Result: 所有算法在校准方面表现良好，但深度学习方法在识别分布外数据的不确定性方面不足。

Conclusion: 深度学习算法需改进以更好地反映出外数据的不确定性，研究为未来不确定性估计方法提供参考。

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [114] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 通过结合AutoML和解释性AI，分析俄亥俄州交通事故数据，识别关键风险因素，为交通安全政策提供科学依据。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故仍是全球主要的伤亡原因，需数据驱动的方法理解和减缓事故严重性。

Method: 利用AutoML平台进行模型构建，应用SHAP解释模型特征，筛选关键指标，最终采用岭逻辑回归模型进行预测。

Result: 模型达到85.6%的AUC-ROC，确定17个主要预测因子，显著突出环境和情境变量的重要性，挑战传统关注的酒精等因素。

Conclusion: 该方案注重方法透明性和解释性，提供了支持零事故目标的可扩展数据框架，推动交通安全措施的优化。

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [115] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 提出一种融合参数空间与行为空间的双空间引导测试框架，有效提升关键场景的生成效果和多样性。


<details>
  <summary>Details</summary>
Motivation: 随着决策代理在动态环境中的应用增加，安全验证需求激增，现有方法难以在高维空间中平衡多样性与关键性，容易陷入局部最优。

Method: 通过层次化的参数空间表征结合多维子空间评估，实现多空间协同探索，并利用代理行为数据进行反馈调节，形成闭环。

Result: 该框架提升关键场景生成效率56.23%，在多代理测试中展现出更高的多样性，并优于现有最优方法。

Conclusion: 该方法有效缓解高维探索中的局部最优问题，提供了一种高效、多样且具有指导性的测试场景生成策略，有助于提升决策系统的安全验证能力。

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [116] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 该论文提出了一种用费米子图方法计算有限宽度神经网络的NTK修正的新框架，简化了繁琐的代数操作并拓展了深层网络稳定性分析。


<details>
  <summary>Details</summary>
Motivation: 弥补无限宽度极限下训练性质的不足，考虑有限宽度对NTK和特征学习的影响。

Method: 引入费米子图，建立递归关系，计算预激活、NTK及高阶导数张量的有限宽度修正。

Result: 成功扩展了深网络的稳定性分析，证明了某些非线性激活函数的有限宽度不存在修正，并通过数值验证。

Conclusion: 该框架有效简化了有限宽度修正的计算，并为深度学习的训练动力学提供了更全面的理解。

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [117] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 提出一种基于物理信息的扩散模型，用于多变量时间序列的无监督异常检测，结合物理知识优化模型训练，提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 利用扩散模型在时间序列异常检测中的潜力，结合物理知识提升模型准确性和泛化能力。

Method: 通过引入加权物理信息损失，并采用静态权重调度，训练扩散模型以学习时间序列的物理依赖的分布。

Result: 在合成和真实数据集上显示，该方法增强了F1分数，生成更具多样性的数据，优于多个基线和先前方法，特别是在合成及实际数据集上表现优异。

Conclusion: 引入物理信息增强的扩散模型有效提升了无监督异常检测的性能，展示了结合物理知识的潜力。

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [118] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: 提出了一种适应性通信的去中心化联邦图学习框架DFed-SST，有效利用每个客户端的子图结构特性，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有DFL方法未充分利用图结构信息的问题，以及将FGL的优势应用到去中心化环境中。

Method: 设计了双拓扑自适应通信机制，动态构建和优化客户端间通信拓扑，结合局部子图的拓扑特性进行模型聚合。

Result: 在八个真实数据集上的实验显示，DFed-SST比基线方法平均提升3.26%的准确率。

Conclusion: 该方法有效提升去中心化联邦图学习的性能，为处理异质环境中的图数据提供了新思路。

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [119] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 本文提出一种基于数据的嵌套Operator Inference方法，用于从高维系统的快照数据中学习物理信息约束的模型，尤其在控制误差和提升计算效率方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 为解决高维动力系统在模型简化中的高精度和高效性问题，提出一种创新的嵌套结构学习方法，以提升模型学习的准确性和泛化能力。

Method: 采用分层策略，逐步构建模型近似，通过先验排序捕捉主要模态的相互作用，允许模型在不同维度下递归优化，并结合先前学习的模型实现热启动。

Result: 在热传导及格陵兰冰盖模型中，所提方法显著降低了重构误差（如4倍减少）并实现了高达1万9千倍的加速，验证了其优越性和适应性。

Conclusion: 嵌套OpInf提供了一种有效的、具有高度灵活性的模型学习框架，能够在保持物理一致性和提升效率方面表现优异，具有广泛应用潜力。

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [120] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow 提出了一种基于服务器的强化学习框架，通过解耦训练和执行流程，以及动态资源调度，有效提升大规模工业环境下的RL性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决工业规模强化学习中的训练与执行耦合问题及GPU资源利用率低的问题。

Method: 引入数据平面和轨迹管理器实现训练与执行解耦，采用标签驱动调度和时空复用技术优化硬件资源利用。

Result: 实现稳定性和高性能，适用于多智能体和长时域等复杂RL任务。

Conclusion: SeamlessFlow通过创新架构设计满足工业级RL系统的性能和稳定性需求，具有广泛应用潜力。

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [121] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 本文提出了一种基于马尔可夫博弈的多利益相关者CO2储存管理方法，通过强化学习优化多站点、多目标的碳捕获与封存策略，验证了合作结构的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决碳捕获与封存项目中多利益相关者因目标和职责不同而导致的合作优化难题，实现多利益相关者在共享资源上的协同管理。

Method: 将多利益相关者多站点问题建模为带有安全约束的多智能体强化学习问题，利用Embed-to-Control框架实现高效模拟，并分析不同联盟结构的影响。

Result: 验证了该框架在多利益相关者环境下优化CO2储存管理的有效性，展示合作结构对目标实现的重要作用。

Conclusion: 基于马尔可夫博弈的多智能体强化学习为复杂合作环境中的碳封存提供了一种有效的决策工具，促进多方协作共赢。

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>
