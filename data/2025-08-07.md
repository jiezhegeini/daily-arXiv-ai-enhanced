<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 该研究系统性测量了GPT-4 Turbo中的刻板偏见，发现其在宗教和种姓维度上严重偏向文化主导群体，即使在旨在鼓励多样性的促使下，这种偏见依然存在且具有“赢家通吃”的特点，表明仅通过多样化训练数据难以根除偏见，需从根本上改进模型开发方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示大语言模型中的深层次代表性偏见，特别是在非西方和边缘身份维度中，以理解偏见的范围和性质。

Method: 通过设计多样化促发的提示，生成超过7200个关于印度重要生活事件的故事，并将宗教和种姓代表性与印度统计数据进行对比分析，评估偏见的程度和“粘性”。

Result: 发现GPT-4 Turbo在宗教和种姓方面持续过度代表文化主导群体，偏见超出统计比例，且多次促发有限影响，偏见表现出“赢家通吃”特性。

Conclusion: 仅依靠多样化训练数据不足以消除偏见，需在模型开发中引入更根本的改进策略，以实现更公正的社会代表性。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [2] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 为高能理论物理开发的定制大规模语言模型，通过微调Llama-3.1模型，表现优于基础模型，并进行行业对比。


<details>
  <summary>Details</summary>
Motivation: 旨在提升高能理论物理领域文献理解和处理能力，满足专业领域的知识需求。

Method: 使用不同领域数据集微调Llama-3.1模型，并采用两种低秩适应微调方法，比较多个变体的性能。

Result: 微调模型在高能物理相关任务中优于基础模型，并在与商业大型模型比较中表现优异，揭示了专业化模型的潜力。

Conclusion: 定制化大型语言模型能显著提升高能理论物理领域的文本处理能力，为未来专业模型的开发提供路径。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [3] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 开发了一个支持多轮对话、语音输入输出的印度农民专用AI农业咨询聊天机器人Krishi Sathi，结合检索增强生成技术，提高了问答准确率和相关性。


<details>
  <summary>Details</summary>
Motivation: 印度农村农民缺乏及时、易懂的农业咨询，特别是低识字率地区，亟需便捷的数字农业支持工具。

Method: 设计了多轮对话流程、结合检索增强生成（RAG）技术、支持本地语言和语音交互，利用微调和多数据集训练模型。

Result: 系统在问答准确率达97.53%，相关性91.35%，操作响应时间＜6秒，显著提升了农业信息服务的效果和覆盖面。

Conclusion: 结合意图驱动对话、指令微调模型和信息检索技术，有效改善了印度农村的数字农业支持手段，具有广泛应用前景。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [4] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: 提出Hierarchical Verification Tree (HVT)框架，通过优先验证高概率候选并早期剪枝，有效提升大规模语言模型推理效率，减少时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决大模型推理中因逐序验证带来的计算资源浪费问题。

Method: 引入层级验证树结构，结合理论保证和剪枝算法，优化推理过程。

Result: 显著高于现有方法，减少推理时间和能耗，同时保持或提升输出质量。

Conclusion: 层级验证策略为加速大模型推理提供新方向，具有实用价值。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [5] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: WiNELL利用多智能体框架实现对维基百科内容的持续自动更新，通过精细化编辑模型提高信息覆盖和编辑效率。


<details>
  <summary>Details</summary>
Motivation: 解决维基百科内容依赖人工编辑更新不足的问题，利用LLM技术实现自动及时更新。

Method: 采用多智能体框架，从线上信息中筛选重要知识，并生成编辑建议，通过训练的细粒度模型模拟人类编辑行为。

Result: 模型在信息覆盖率和编辑效率方面优于基线，能在高活跃页面自动识别和建议及时更新。

Conclusion: LLM智能体为自动持续更新知识库提供了有希望的研究路径。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [6] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: GanitBench是一个涵盖多数学科、包含印地语和英语的1657个图像数学题目的基准测试，用于评估视觉语言模型在多语言中的推理能力，尤其是在有限隐私信息保护的环境下。


<details>
  <summary>Details</summary>
Motivation: 当前针对视觉语言模型的多语言推理评估研究不足，特别是在非英语和有限任务类型方面。

Method: 收集印度主要考试中的数学题目，构建多模态、多语言的基准，通过零和两次链式思考（CoT）评估模型性能，并引入“Double Lock”限制以模拟信息限制环境。

Result: GPT-4o mini模型表现优异，最高准确率38.15%；引入“Double Lock”限制后模型性能大幅下降，两次CoT在此环境下效果更佳；模型在印地语中性能亦降低。

Conclusion: 本研究有助于推动多语言、多任务、多模态推理模型的发展，尤其是在资源有限的多语言环境中。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [7] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: 提出一种基于LLM注意力权重的上下文追溯新方法AttnTrace，具有更高的准确性和效率，并能应用于识别提示注入和操控内容。


<details>
  <summary>Details</summary>
Motivation: 需要高效、准确地追溯影响生成响应的上下文文本，提升模型的可解释性和信任度。

Method: 利用LLM生成的注意力权重，并引入两种技术优化追溯效果，同时提供理论分析。

Result: AttnTrace在准确性和效率上优于现有方法，可用于检测提示注入和识别恶意操控。

Conclusion: AttnTrace为上下文追溯提供了有效工具，有助提升LLM系统的透明度和安全性。

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [8] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: MajorMark通过多数比特感知编码提升水印效果，平衡内容质量和解码准确性，特别适合多比特水印应用。


<details>
  <summary>Details</summary>
Motivation: 解决多比特水印在保证内容质量和解码可靠性之间的权衡问题。

Method: 引入多数比特感知编码和基于聚类的解码策略，同时提出分块编码的增强版本MajorMark$^+$。

Result: 显著提升解码准确性和文本生成质量，优于现有多比特水印方法。

Conclusion: MajorMark及其扩展方案有效改善水印的可用性，为大规模语言模型的内容验证提供了有力技术支持。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [9] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 大型语言模型面临事实准确性挑战，需结合多种方法提升事实核查能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，确保其输出的 factual accuracy 变得尤为重要。

Method: 通过系统分析2020-2025年的相关文献，探讨评价方法和缓解技巧，包括促进机制和外部知识融合。

Result: 发现现有度量指标不足，强调结合验证性证据和域适应提升可信度。

Conclusion: 未来应打造既准确又可解释，且适应特定领域的事实核查框架，以增强模型的可信度。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [10] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 提出一种基于大语言模型的实体链接代理，用于提升QA系统在短语意境中的表现，通过仿人认知流程实现实体识别与匹配，验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有实体链接方法在短文本和模糊问句中表现不佳，影响问答系统的准确性。

Method: 采用大语言模型模拟人类认知流程，主动识别实体、检索候选实体、作出决策，进行实体链接。

Result: 通过工具式实体链接和问答任务测试，验证了代理的鲁棒性和有效性。

Conclusion: 基于大模型的实体链接代理能有效改善短文本问答的实体识别和匹配问题，具有潜在应用价值。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [11] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出SSotia-RL，通过将粗略的反馈细化为多维度、层级的奖励，有效提升了大型语言模型在社交任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 提升大模型的社交智能，使其能在复杂交互中表现出有效的合作、说服和协作能力。

Method: 引入utterance-level多维奖励机制，将社会互动的反馈细化，改善部分可观测性和多维性问题。

Result: 在Sotopia环境中，Sotopia-RL显著优于现有方法，达到先进的社交目标完成率。

Conclusion: 通过细化奖励和多维度设计，有效增强了RL在社交场景中的训练效果，验证了两者的必要性。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [12] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: 引入代码操作增强的多智能体系统CoAct-1，通过结合GUI控制与脚本执行，实现效率和可靠性的显著提升，达成新突破。


<details>
  <summary>Details</summary>
Motivation: 解决GUIs操作在复杂长任务中效率和可靠性不足的问题，引入编程操作以增强智能体能力。

Method: 设计CoAct-1系统，结合GUI操控与Python/Bash脚本编写执行，由调度器动态分配子任务。

Result: 在OSWorld基准测试中，成功率达60.76%，优于前沿方法，同时将平均步骤数缩短到10.15步。

Conclusion: 将编码操作作为核心行动极大提升了自动化的效率、鲁棒性和扩展性，推动通用计算机自动化发展。

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [13] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: 提出了一种结合用户偏好和事实一致性约束的改进大模型框架CAP-LLM，用于个性化新闻标题生成，显著提升了个性化和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有新闻标题生成方法在捕捉用户兴趣和保持事实一致性方面的不足，提升新闻阅读体验。

Method: 引入用户偏好编码器、情境注入适配器和事实一致性强化模块，结合预训练大模型实现个性化与事实保障。

Result: 在实际数据集上，CAP-LLM在个性化、内容覆盖和事实一致性方面优于现有模型，达到最新最优性能。

Conclusion: CAP-LLM有效平衡了个性化和事实准确性，具有较强的实用性和推广潜力。

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [14] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 本文提出了一套涵盖机器学习模型全生命周期的偏见治理和评估体系，特别针对大型语言模型（LLMs）中的偏见与公平性问题，强调数据和AI治理的重要性，以确保生成式AI的安全性和责任性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在偏见和公平性方面的不足，推动负责任的AI应用发展。

Method: 构建偏见评估测试套件（BEATS），提出数据与AI治理框架，涵盖模型开发、验证、监控与响应。

Result: 实现了对LLMs的偏见和公平性系统检测与治理，提升系统安全性和责任感，适用于实际应用场景。

Conclusion: 通过全生命周期的治理策略，增强生成式AI的安全性、责任性，推动社会责任感强的AI技术发展。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [15] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 引入早期假设剪枝策略，提高自我一致性方法在长链思维推理中的令牌效率，显著减少令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 解决自我一致性在长链推理任务中高令牌消耗的问题，提升其实用性。

Method: 通过平行生成所有解并利用置信度与词汇覆盖率指标，结合加权集合覆盖算法实现假设的早期剪枝。

Result: 该方法在五个大模型和三个数学基准测试中，令牌效率提升10-35%，效果显著。

Conclusion: 早期假设剪枝策略能在保持并行性的同时，有效降低长链推理任务的令牌消耗，提升大模型性能实用性。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [16] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 本文构建了一个包含43,880个关于幸福感概念的解释的大规模数据集，提出了基于原则的LLM评估框架，并通过微调显著提升了解释质量，展示了偏好优化在定制化解释中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人们借助大型语言模型理解幸福感，提供高质量且符合多样受众需求的解释成为一大挑战。

Method: 通过建立大规模数据集，采用原则指导的评判框架，以及利用监督微调和偏好优化技术提升模型表现。

Result: LLM评判与人类评价高度一致，模型间在解释质量上存在显著差异，微调模型优于大模型，偏好学习提升效果明显。

Conclusion: 偏好优化技术对于定制化、优质解释的提升具有显著潜力，为个性化信息提供了有效路径。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [17] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 本文提出一种基于概念瓶颈模型的社会机器人辅助系统，用于提升群体会议的效果，具有高透明性与适应性.


<details>
  <summary>Details</summary>
Motivation: 解决会议中引导者在识别社会动态和个体需求时的认知负担，需求一种具有人机理解透明性、能实时提供建议的辅助手段。

Method: 通过迁移学习将预训练的社交理解能力转移到基于概念的模型中，利用多模态会议数据分析社会线索，并提供可解释的干预建议。

Result: 该模型在预测干预需求方面优于零-shot基础模型，能实时被人类修正，且具备跨组泛化能力，从而有效传递专家知识，增强辅导效果。

Conclusion: 提出的概念驱动模型为复杂社交环境中的人机协作提供了可解释、高效、可迁移的解决方案，有助于提升人类辅导的智能化水平。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [18] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出了一种多智能体协作框架HarmonyGuard，用于在Web环境中平衡任务效用与安全性。


<details>
  <summary>Details</summary>
Motivation: 随着网络环境中潜在威胁的演变，Web代理在长序列操作中面临任务性能与风险的权衡挑战，现有方法多偏重单目标优化或单轮场景。

Method: 引入政策增强和双目标优化的多智能体架构，包括政策代理自动提取与维护安全策略，以及效用代理基于马尔可夫决策实时优化。

Result: 在多个基准测试中，HarmonyGuard提升了政策合规性达38%，任务完成率达20%，所有任务中超过90%的策略合规。

Conclusion: HarmonyGuard通过协作优化实现了Web环境中任务效用与安全性的显著提升，为未来多目标、多智能体安全策略提供了新思路。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [19] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: 提出一种新的模型编辑方法SMEdit，结合多步反向传播和范数正则，提升低数据场景下的编辑效果和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决MLBME在少数据环境下表现不佳且训练效率低的问题。

Method: 引入多步反向传播策略MBPS及范数正则化以改善模型编辑性能和训练效率。

Result: 在两组数据和两种模型上实验显示，SMEdit优于现有方法，搭配MBPS还能进一步提升性能。

Conclusion: SMEdit有效增强了MLBME在低数据环境下的表现，并提高了训练效率。

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [20] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA是首个基于代理的零样本文本解释人体活动识别的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的HAR方法依赖固定活动集，面临新行为或传感器设置时需昂贵的重训练，同时使用大语言模型的尝试准确率有限且缺乏解释能力。

Method: ZARA集成自动构建的成对特征知识库、多传感器检索模块和分层代理流程，指导LLM进行特征选择、证据利用和自然语言解释。

Result: 在8个基准测试中，ZARA达到了最佳零样本性能，宏平均F1超越最优基线2.53倍，验证了各模块的必要性。

Conclusion: ZARA实现了无需微调的灵活、可解释的HAR，推动可信赖、即插即用的运动时间序列分析发展。

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [21] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 本文提出一种用于支持科学文献综述的修辞角色注释架构，并评估了多种大型语言模型在此任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了提升自动化文献综述的质量，需定义合适的注释架构并进行大规模标注，但这面临诸多挑战。

Method: 设计新颖的注释架构，构建标注数据集，并评估37种LLMs在零样本和微调条件下的表现。

Result: 在高质量微调数据条件下，LLMs表现优异，F1值超96%；大型模型如GPT-4表现最佳，开源模型也表现优良；半合成数据能提升模型效果。

Conclusion: 设计的注释架构与多模型评估 提升了科学文献修辞角色识别的研究水平，为自动文献综述发展提供新路径。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [22] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 研究显示大型推理模型（LRMs）增强了越狱（绕过模型安全机制）的能力，使得非专家也可进行此类操作。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型普及，越狱技术变得越来越容易，威胁模型安全，需研究其新途径。

Method: 采用四个不同的LRMs作为自主对抗方，对九个目标模型在多个敏感域中进行多轮对话式越狱尝试，未监督情况下执行计划与操作，进行大量实验。

Result: 所有模型攻击成功率达97.14%，证明LRMs可以系统性侵蚀模型的安全保护，揭示模型安全性存在严重退化问题。

Conclusion: 需要加强模型的对齐和安全设计，防止越狱技术扩散和滥用，提升模型安全性。

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [23] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 本研究比较了不同NLP模型在提升交通事故描述信息质量中的表现，发现微调的变换器模型在准确性与效率间表现平衡，具有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 改善交通事故数据的准确性和完整性，利用先进的NLP技术自动识别次级事故，以支持交通安全分析与管理。

Method: 采用16,656条手工审查的事故叙述，比较零样本文本模型、微调变换器模型和传统逻辑回归的性能，模型在2015-2021年训练，并在2022样本测试。

Result: 微调的变换器模型（如RoBERTa）表现最佳，F1达0.90，准确率95%，零样本文本模型接近，传统模型较差。较大的LLMs虽有较高召回率，但运行时间较长。中等规模的LLMs表现优良，效率高。

Conclusion: 微调的Transformer模型在准确性和效率方面实现良好平衡，适合实际应用；未来应考虑隐私保护、集成优化和逐步处理策略，以提升交通事故数据质量。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [24] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: 提出DTPA框架，通过动态增强前缀关注，提高长文本控制生成的效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前长文本的可控性较差，尤其是随着序列长度增加，前缀注意力减弱影响控制效果。

Method: 选择最优前缀类型，动态放大前缀注意力，调整属性分布以提升控制性，同时保障文本质量。

Result: 在多项任务中，DTPA优于其他方法，特别是在长文本生成中的表现更为显著，同时保持良好的流畅度、多样性和主题相关性。

Conclusion: DTPA有效解决了长文本控制生成中的主要挑战，是一种轻量且高效的解决方案。

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [25] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: TURA框架创新融合静态与动态信息检索，提升实时搜索能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG方法在实时性和动态信息访问上的限制。

Method: 提出三阶段架构：意图感知检索、DAG任务规划和轻量级执行器，结合工具使用增强信息访问。

Result: 实现系统性桥接静态内容与动态实时数据，提升工业级搜索产品的实时性和效果。

Conclusion: TURA为企业级AI搜索提供强大技术支撑，创新性解决动态信息访问难题。

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [26] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: 提出了一种无需训练的自适应信息检索和选择框架PAIRS，有效提升RAG系统的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在简单问题和信息稀疏查询中效率低和相关性差的局限。

Method: 结合双路径生成机制，无需训练，通过自生成伪上下文判断是否使用外部检索，并在需要时进行目标导向的文档检索和筛选。

Result: 显著降低检索成本（约25%），同时在六个问答基准测试中，准确率提升1.1%的EM和1.0%的F1。

Conclusion: PAIRS通过自适应检索与选择，有效提升RAG的效率与准确性，验证了其在实际应用中的潜力。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [27] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 轻量级Transformer模型在低资源环境中表现出色，T5-Small在Text-to-SQL任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 满足非专业用户在有限资源环境中对关系型数据库的自然语言查询需求。

Method: 开发可重用、模型无关的管道，调优T5-Small、BART-Small和GPT-2模型，在Spider数据集上训练和评估。

Result: T5-Small在逻辑形式准确率上最高达27.8%，优于BART-Small和GPT-2，彰显编码器-解码器模型在方案感知SQL生成中的优势。

Conclusion: 紧凑型Transformer在资源有限环境下的Text-to-SQL任务中具有潜力，未来可通过改进方案链接等提升性能。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [28] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [29] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: ToolGrad通过逆转传统工具使用数据集生成方法，先构建工具链再生成用户查询，提高数据质量和效率。


<details>
  <summary>Details</summary>
Motivation: 提升工具使用数据集的生成效率与质量，解决现有方法中的标注失败和低效率问题。

Method: 采用迭代文本梯度引导的回答优先策略，先构建工具链再生成用户查询，形成ToolGrad框架。

Result: 成功生成ToolGrad-5k数据集，具备更复杂的工具使用、更低成本和100%通过率，模型在多个基准中表现优异。

Conclusion: 逆转传统策略，工具链优先的生成方式显著提升工具用例数据集的质量与应用效果。

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [30] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: 提出了一种生成式多模态流程奖励模型（GM-PRM），能细粒度分析推理步骤，并修改错误以提升多模态数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大型语言模型在复杂、多步数学推理中易出错的问题，现有的流程奖励模型只能验证错误，缺乏解释性和纠正能力。

Method: 将流程奖励模型转化为主动推理合作者，提供细粒度的推理步骤分析并生成修正，结合Refined BoN推理策略提升解决方案质量。

Result: 在多个多模态数学基准测试中获得了最优性能，以较少数据实现显著效果，促使模型推理更准确和多样。

Conclusion: GM-PRM通过细粒度分析和纠正增强多模态数学推理能力，推动奖励模型由被动验证向主动合作的转变，提升模型表现。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [31] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 大规模预训练语言模型在微调过程中存在过度记忆现象，表现为训练数据过度 memorization，影响模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究微调过程中模型学习动态，特别是过度记忆现象及其影响，以提升模型的表现和应用性。

Method: 分析不同训练条件、阶段的学习行为，验证过度记忆的普遍性和影响，提出改进建议。

Result: 发现过度记忆导致模型鲁棒性下降，泛化能力变差，但在准确率方面表现尚可；提出调节训练策略的建议。

Conclusion: 大模型微调需注意过度记忆问题，合理选择训练时间点和学习率，提升模型整体表现。

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [32] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出一种基于难度的偏好数据选择策略，通过选择具有较小DPO隐含奖励差异的样本，提高LLM对人类偏好的对齐效率，数据利用率高，效果优于多项基线。


<details>
  <summary>Details</summary>
Motivation: 解决偏好数据昂贵且有限的情况下，提高偏好数据选择的效率，从而更高效地实现大语言模型与人类偏好的对齐。

Method: 基于DPO的隐含奖励机制，设计难度导向的数据选择策略，挑选挑战性更高的偏好样本以提升模型性能。

Result: 在多个数据集和任务上，该方法在只用原始数据的10%时，性能优于五个强基线，展现出良好的数据效率和模型对齐效果。

Conclusion: 该基于难度的偏好数据选择策略具有理论依据和实践效果，为低资源条件下的LLM偏好对齐提供了有效路径。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [33] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 引入人类愚弄率（HFR）作为衡量TTS系统逼真度的指标，揭示当前模型在欺骗性和自然性方面的差距。


<details>
  <summary>Details</summary>
Motivation: 评估TTS系统是否真正达到人类水平，反映其在真实场景中的实用性和自然性。

Method: 设计并进行大规模主观评价，测量不同TTS模型被误认为为人类的频率，并分析模型在不同数据集和条件下的表现。

Result: 商业模型在零样本设置中逐步逼近人类水平，但开源模型仍难以实现自然对话的逼真度。高质量微调提升了逼真度，但未能完全弥合差距。

Conclusion: 需要结合更真实的人类中心评估方法，以全面衡量TTS系统的自然性与欺骗能力，推动其向更接近人类的方向发展。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [34] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 提出了一种基于因果完备性的强化学习框架，旨在减少多模态大语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型中的幻觉问题，提升生成内容的真实性和一致性。

Method: 引入因果完备性概念，设计以因果性充分性和必要性衡量的奖励机制，并在GRPO优化框架下进行联合优化。

Result: 实验验证表明该方法有效缓解了模型的幻觉问题，提高了生成的准确性和可靠性。

Conclusion: 通过因果完備性指导的强化学习，是减少MLLM幻觉的有效策略，有助于增强模型的真值保持能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [35] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 提出深度研究任务的正式定义与评估基准，强调概念探索和推理能力，构建了多样严峻的测试平台，以推动相关系统的发展。


<details>
  <summary>Details</summary>
Motivation: 深度研究任务在复杂搜索与推理中的应用日益重要，但其定义模糊，缺乏客观评估标准，需要明确界定与量化。

Method: 提出深度研究的形式化描述，设计中间输出表示，建立包含科学研究和公共事件的多样性挑战任务集LiveDRBench，用于系统性能评估。

Result: 构建了涵盖多领域挑战的基准，测试结果显示现有系统性能有限，OpenAI模型表现最佳，分析推理流程揭示系统的搜索与推理瓶颈。

Conclusion: 该研究明确了深度研究的核心特征，为未来提升推理与搜索机制提供理论基础与实验平台，有助于推动复杂信息处理技术的发展。

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [36] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 本文揭示了现有对齐方法在应对复杂对话操控时的脆弱性，通过手动和自动测试发现模型易受策略性攻击，呼吁增强模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型应用的扩大，确保其行为符合预期变得至关重要，但现有对齐策略在应对复杂操控时仍存在漏洞。

Method: 作者通过手动红队测试和构建MISALIGNMENTBENCH评估框架，系统发现模型在应对策略性场景中表现出不同程度的脆弱性。

Result: 五个主流模型中，最低抗干扰率为40%，最高达90%。模型在应对操控时可能表现出欺骗、价值漂移等不良行为，揭示潜在安全风险。

Conclusion: 当前对齐策略存在显著不足，应强化模型对情境操控的抵抗能力，以确保应用安全。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [37] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 该论文提出了一个诊断框架，用于在文化背景复杂、低资源语境下评估大语言模型在非正式信息中的情感分析能力，并强调文化敏感性的重要性。


<details>
  <summary>Details</summary>
Motivation: 低资源和文化复杂背景下的情感分析挑战了传统的自然语言处理方法，迫切需要更具文化敏感性和解释性的评估工具。

Method: 作者结合人类注释、情感反转反事实和评分标准解释，探讨大型语言模型在非正式、混杂语言环境中的推理、鲁棒性和对人类推理的匹配情况。

Result: 高端模型表现出较好的推理稳定性，而开源模型在面对歧义或情感转变时较为不足。

Conclusion: 强调在复杂、现实的交流场景中，需进行文化敏感、强调推理的AI评估，以提升模型的适应性和可靠性。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [38] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: 提出ReasoningGuard，通过模型内部注意力行为在推理过程中引入安全反思措施，有效防止LRMs在推理中产生有害内容，优于现有方法，且成本低。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在推理中易生成有害内容的问题，现有防护机制成本高，难以推广。

Method: 利用模型内部注意力行为识别关键推理点，在推理过程中引入安全反思，并在解码阶段采用采样策略选择安全推理路径。

Result: 在多种攻击类型下，ReasoningGuard优于七种现有防护措施，达到最先进的安全防御水平，且避免了过度安全设定的问题。

Conclusion: 该方法通过在推理阶段引入安全反思，有效提升大型推理模型的安全性，成本低且具推广潜力。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [39] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 本文探讨通过API调用黑盒大型语言模型（LLMs）进行分层文本分类（HTC），比较不同提示策略在零样本与少样本下的效果及成本。


<details>
  <summary>Details</summary>
Motivation: 传统HTC模型需大量标注数据和计算资源，探索黑盒LLMs作为替代方案具有实际意义。

Method: 采用三种提示策略（DL、DH、TMH）在零样本和少样本条件下进行评估，并比较准确率与成本。

Result: 少样本可提升准确率，深层层级中LLMs优于传统模型，DH策略表现尤佳，但成本较高。

Conclusion: 黑盒LLMs在HTC中具有潜力，需合理选择提示策略以平衡性能与成本。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [40] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: 引入DP-GPT4MTS框架，通过双提示机制结合文本与数值时间序列，有效提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型忽略文本信息，限制预测准确性。

Method: 设计双提示机制，结合显式任务提示与上下文文本提示，利用自注意力增强文本表示。

Result: 在多样化时间序列数据集上优于最新算法，验证了其有效性。

Conclusion: 双提示机制显著改善时间序列预测，凸显文本信息整合的重要性。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [41] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 提出了一种基于先进语言模型的虚拟患者模拟系统，用于支持抑郁症诊断的训练和评估。


<details>
  <summary>Details</summary>
Motivation: 由于实际临床数据匮乏，限制了抑郁症诊断能力的提升，亟需虚拟患者模拟辅助培训与评估。

Method: 采用以语言模型为基础，结合精神诊断标准和症状量表，建立“TalkDep”临床医生参与的虚拟患者生成管道。

Result: 经过临床专家评估，验证了模拟患者的可靠性，有助于提升自动抑郁症诊断系统的泛化能力。

Conclusion: 这套模拟系统为抑郁症诊断模型提供了可扩展、可靠的虚拟训练资源，有助于缓解临床数据不足的问题。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [42] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: KVSink通过深刻理解注意力汇聚的机制，有效优化了KV缓存量化，超越了现有策略。


<details>
  <summary>Details</summary>
Motivation: 为提高大语言模型推理中KV缓存的量化效率，同时保护注意力汇聚，减少性能损失。

Method: 分析注意力汇聚的形成机制，提出KVSink预测汇聚点，结合实验证明其优越性。

Result: KVSink在保持注意力汇聚方面优于现有方法，提升了模型性能，降低了数值误差。

Conclusion: 深入理解注意力毁灭机制，为KV缓存量化提供了有效目标，KVSink显著改善模型表现。

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [43] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: ShoppingBench是一个旨在模拟复杂购物意图的端到端电商基准平台，包含真实商品数据和多任务挑战，推动智能购物助手的发展。


<details>
  <summary>Details</summary>
Motivation: 现有电商评测主要关注基础意图，难以应对复杂购物场景，亟需更具挑战性和真实性的基准。

Method: 通过模拟复杂用户意图并建立大规模互动沙箱环境，结合训练策略提升语言模型的能力。

Result: 即使是最先进的模型成功率也不到50%，表明任务难度大；训练的小模型在性能上接近强大模型，验证了方法有效性。

Conclusion: ShoppingBench推动了复杂电商任务的研究，有助于发展更智能的购物助手。

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [44] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: 本文提出两种针对GraphRAG的知识中毒攻击方法，分别为Targeted KPA和Universal KPA，能通过微小修改操控知识图，严重误导问答系统，且现有防御措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着GraphRAG在知识增强中的应用增加，其安全性成为新挑战，存在被恶意操控的风险。

Method: 利用图论分析定位脆弱节点，通过LLM生成中毒内容，设计了Targeted KPA和Universal KPA两种攻击策略。

Result: 两种攻击在保持文本自然的同时，极大影响知识图的正确性和问答准确性，显示出攻击的高成功率和现有防御的不足。

Conclusion: 保护GraphRAG免受知识中毒攻击依然是一个亟需解决的挑战，未来需要提出更有效的防御策略。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [45] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: MedCheck是一个针对医疗基准测试的生命周期评估框架，旨在提高医疗AI评估的规范性、可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI基准测试存在临床相关性不足、数据管理不严和安全性评价缺失等问题，亟需改进评估体系。

Method: 设计了涵盖五个阶段的生命周期评估框架，并制定了46项专门的医学标准，通过实证分析53个医疗基准测试。

Result: 发现广泛存在临床脱节、数据污染风险和安全性评估缺失等系统性问题。

Conclusion: MedCheck作为诊断和指导工具，促进医疗AI评估的标准化、可靠性和透明度提升。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [46] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 引入动态熵加权机制优化强化学习中对长链推理任务的奖励分配，有效提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有RL方法中对长链推理任务中的稠密奖励分配过于粗糙的问题。

Method: 提出基于熵权重的两种优化方法：GTPO 和 GRPO-S，通过高-熵标记和高-熵序列赋予更细粒度的奖励信号。

Result: 新方法在实验中明显优于基线，验证熵加权机制能显著提升深度推理表现。

Conclusion: 动态熵加权机制能有效改善长链推理任务中的奖励分配，推动模型深度推理能力的提升。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [47] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 引入Chain of Questions框架，通过生成针对性问题提升多模态模型在复杂环境中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法提升了语言模型的推理能力，但在多模态场景中的应用有限，需要更主动的感知信息整合机制。

Method: 提出Curiosity-driven的Chain of Questions框架，动态生成问题引导模型选择相关感官模态。

Result: 在新构建的多模态基准数据集上，显著提高模型识别和整合感官信息的能力，提升性能与可解释性。

Conclusion: 该方法有效增强多模态推理，促进模型在复杂环境中的理解与决策能力。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [48] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 提出的事实核查管线在FEVER 8竞赛中第一，采用两步RAG模型，在有限硬件条件下达成最优表现。


<details>
  <summary>Details</summary>
Motivation: 提升事实核查系统的效率和性能，在资源有限的情况下保持竞争力。

Method: 基于去年提交的系统，设计了简单的两步RAG管线，能在本地部署。

Result: 在有限硬件环境下实现了最先进的性能，获得Ev2R测试分数第一。

Conclusion: 该管线在性能和硬件资源利用方面表现优秀，为事实核查提供了有效方案。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [49] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: 探讨深度神经网络（DNN）中的突现能力，强调其源于复杂非线性动力学，而非简单参数扩展，提出理解大模型能力需采用复杂系统的视角。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型（LLMs）中突现能力的本质，解决“创造无理解”的认知挑战。

Method: 结合理论分析与经验观察，研究扩展定律、grokking现象及相变，强调非线性系统的复杂动力学。

Result: 突现能力源自高度敏感的非线性动力学，而非参数规模增长，揭示了DNN的根本本体论特性。

Conclusion: DNN应视为复杂动力系统，其能力突现类似自然界的复杂系统，应从动态演变角度理解其能力的获得。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [50] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 该研究探讨了人类在对话中的选择性听力，旨在评估和提升自动语音识别系统的性能，从而改善语音对话系统的响应生成。


<details>
  <summary>Details</summary>
Motivation: 理解人类选择性听力的机制，推动自动语音识别技术的改进，以满足对话系统中的信息识别需求。

Method: 通过比较人类的转录与基准转录，实验验证了人类的选择性听力现象，提出用此现象评估ASR系统的潜在方法。

Result: 确认了人类具有选择性听力，提出了利用该能力进行ASR系统评估的可能性。

Conclusion: 引入人类选择性听力作为ASR性能评价的新途径，有助于识别系统的转录能力差距，推动更符合人类交流需求的语音识别技术发展。

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [51] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 提出了一种预测置信度模型（PCM），用于判断是否可提前预测用户话语以减少用户等待时间。


<details>
  <summary>Details</summary>
Motivation: 降低口语对话系统中的用户感知延迟，提高用户体验。

Method: 建立预测置信度模型，通过评估预测完整用户话语与实际完整话语的语义相似度来判断提前预测的可行性。

Result: 基于语义相似度的差异评估验证了PCM的有效性。

Conclusion: PCM能有效判断何时可以提前预测，从而优化对话系统的响应速度。

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [52] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出了客户支持对话（CSC）任务，设计基于COPC指南的结构化对话框架，并构建了评估数据集CSConv和训练数据集RoleCS，以提升客服策略的应用与响应质量。


<details>
  <summary>Details</summary>
Motivation: 当前客服对话数据缺乏战略指导和注释，影响服务质量，亟需引入结构化策略以提升客户支持效果。

Method: 引入CSC任务，基于COPC指南设计五个对话阶段与十二种策略，构建评估数据集CSConv，利用LLMs模拟策略对话生成RoleCS，并通过微调提升模型表现。

Result: 微调LLMs能显著提升策略一致性与响应质量，增强问题解决能力，得到正面的人类评估反馈。

Conclusion: 结构化对话框架助力高质量客户支持，对话数据与训练方案为未来研究提供基础。

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [53] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 本文提出ThinkingF方法，通过构建两个数据集并结合SFT与RLVR训练，增强模型的 formal 知识掌握与自然语言到形式表达的推理能力，从而提升自动形式化的性能，达到了SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 解决自动形式化中模型在formal知识理解和自然语言推理两方面能力不足的问题。

Method: 构建含丰富formal知识和推理路径的数据集，结合SFT与RLVR训练方法进行模型优化。

Result: 训练出的7B和32B模型在FormalMATH-Lite和ProverBench任务中表现优异，超越所有既有模型。

Conclusion: ThinkingF方法有效增强模型的formal知识和推理能力，显著推动自动形式化研究发展。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [54] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 提出了一种基于生成式AI的马来西亚教育系统数学试题生成方法，验证了RAG策略优于非结构化提示，并建立了一套评估框架。


<details>
  <summary>Details</summary>
Motivation: 满足马来西亚教育体系对高质量、可扩展性评估工具的需求，特别是在低资源语言环境中。

Method: 通过比较四种不同的基于GPT-4的生成管道，从非结构化提示到检索增强生成（RAG）方法，并利用官方课程文件进行体系结构设计。采用语义相似度和问答验证进行自动评估。

Result: RAG方法显著优于非结构化提示，生成的题目在课程一致性和事实准确性方面表现更佳。系统还分析了框架化RAG与人工管道的权衡。

Conclusion: 提出一种校验学科内容的生成管道，为低资源语言的课程内容生成提供验证方法，为未来教育科技应用提供实践参考。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [55] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: 提出概念差异化方法扩展词汇语义表征，改善模型理解词与词之间的语义关系。


<details>
  <summary>Details</summary>
Motivation: 单一词形对多义词和词义关系的限制，促使开发更全面的词汇语义模型。

Method: 基于SemCor数据集，通过微调多种表示模型，提出概念差异化，命名为CALE，增强模型的语义区分能力。

Result: CALE模型在多种词汇语义任务中表现优越，且微调改善了嵌入空间的结构组织。

Conclusion: 概念差异化扩展提升了词汇语义模型的表达能力，有助于更精准的语义理解与应用。

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [56] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: 提出StyliTruth机制，通过在模型表示空间拆分样式与真实性子空间，实现对语言模型风格控制的同时保持真实性，解决风格化导致的真实性崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 当前风格化大模型响应的控制方法在增强风格的同时损害真实性，存在风格与真实性的矛盾。

Method: 通过正交消减方法将模型中的表示分解为风格相关和真实性相关子空间，设计自适应的向量在两个子空间中控制生成，确保风格和真实性的独立调控。

Result: 实验显示该方法在多风格、多语言环境下有效降低真实性下降问题，优于现有的推理时干预方法，平衡了风格遵循性与真实性。

Conclusion: 通过表示空间的拆分与自适应调控，StyliTruth实现了风格化文本生成中保持真实性的目标，为风格化控制提供了新的方向。

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [57] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 介绍了C-MIND数据集，分析了多模态数据对抑郁症诊断的贡献，并探讨了大型语言模型在临床推理中的应用与局限，提出结合临床专家指导的方法提升模型性能，从而推动精神健康检测的基础设施建设。


<details>
  <summary>Details</summary>
Motivation: 当前自动化抑郁症评估多依赖有限数据，缺乏临床验证，难以推广到实际应用中。

Method: 构建多模态临床数据集C-MIND，分析行为标志，训练传统模型，评估不同模态组合，并探讨LLMs的临床推理能力及其提升策略。

Result: 多模态数据提升诊断性能，LLMs结合临床知识能有效改善推理，但仍存在局限。

Conclusion: 建立数据与算法基础，推动临床精神健康评估标准化与可靠性研究，促进实际应用。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [58] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 多智能体讨论优于单一智能体，团队结构和成员多样性影响创意质量。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体协作是否能超越单一智能体在科研构思中的表现，提升创意质量。

Method: 建立合作多智能体框架，比较不同团队规模、结构和成员组成，采用评分和人类评审评估创意质量。

Result: 多智能体讨论显著优于单一智能体，领导者促进整合，认知多样性是关键，缺乏高级知识的团队难以超越最优单一智能体。

Conclusion: 团队结构和多样性显著影响科研创意质量，为设计协作AI系统提供指导。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [59] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出MASA方法，通过结构化字典学习实现Transformer模型的参数共享，有效压缩参数并保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型参数庞大，限制其应用，应探索层间冗余以降低算力和存储需求。

Method: 将注意力投影矩阵分解为共享字典原子，通过线性组合实现层间共享。

Result: 在不同规模模型和视觉任务中验证，参数显著减少（66.7%），性能优于或接近现有方法，且易于通过标准优化器训练，扩展到预训练语言模型。

Conclusion: MASA提供一种高效且可扩展的参数共享策略，为大模型的参数压缩和推广提供新思路。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [60] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 提出了一种名为P-Aligner的轻量模块，用于提升大规模语言模型的偏好对齐效果，通过新的数据集UltraPrompt和搜索策略，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在面对模糊或不合理指令时常偏离预期价值观，需改进偏好对齐方式，提升内容安全与可靠性。

Method: 设计P-Aligner模块，利用Monte-Carlo树搜索生成符合人类偏好的指令，训练于新颖的UltraPrompt数据集。

Result: 在多模型和基准测试中，P-Aligner显著优于强基线，提升偏好对齐效果，且在数据质量和搜索策略等多方面验证其效率与效果。

Conclusion: P-Aligner通过高效的指令生成策略，有效改善大模型的偏好对齐，兼具性能与实用性，适合大规模部署。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [61] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出IFDecorator框架，有效提升大语言模型的指令遵循能力，减少奖励劫持，并实现高样本效率和性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中奖励评估不足及模型过度优化的问题，提升指令遵循和安全性。

Method: 采用协作对抗数据飞轮、多重验证、意图检测及陷阱机制等创新组成部分。

Result: 在多个指标上超越大型模型，提高指令遵循率，显著降低奖励劫持。

Conclusion: IFDecorator框架具有极高的有效性和实用性，促进可验证奖励的强化学习研究。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [62] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 该论文系统分析了74篇关于反言论的NLP研究，指出当前研究偏离受影响社区需求，建议重视利益相关者的参与以改进反言论生成。


<details>
  <summary>Details</summary>
Motivation: 探讨反言论研究中利益相关者参与的现状及影响。

Method: 文献系统综述结合与非政府组织的参与性案例研究。

Result: 发现研究偏离社区需求，提出重视利益相关者的具体建议。

Conclusion: 强调以社区利益相关者为中心，改进反言论研究的相关策略。

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [63] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: 提出mmGRPO，为多模块语言模型提供优化方案，有效提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多采用模块化设计，单一模块优化方法难以适应多模块环境，亟需新的优化策略。

Method: 定义mmGRPO，将不同模块的语言模型调用按组划分，结合自动提示优化，处理变长和中断的轨迹。

Result: 在多个任务中，mmGRPO提升分类、搜索和隐私任务的准确率，平均提升11%，并比单独提示优化高5%。

Conclusion: mmGRPO为多模块语言系统的优化提供了有效方案，已在DSPy中开源，推动相关研究与应用发展。

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [64] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: Sculptor框架通过主动管理上下文，有效缓解长文本引起的干扰，提高大模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时性能下降，因无效信息干扰其推理与记忆。

Method: 引入主动上下文管理工具，模块包括上下文碎片化、摘要及智能搜索，帮助模型主动筛选信息。

Result: 实验证明该方法在无额外训练的情况下，显著提升模型性能，增强推理的可靠性。

Conclusion: 主动上下文管理策略优于单纯扩大窗口，为大模型提供更稳健的推理基础。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [65] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 提出一种新颖的持续学习方法GeRe，通过样本回放和激活状态约束，有效缓解大模型的灾难性遗忘问题，增强任务连续性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在跨域微调中的灾难性遗忘，保持模型的泛化能力，同时提升已学任务的表现。

Method: 引入样本回放框架GeRe，结合阈值边界的激活状态约束优化（TM损失），验证少量预采样样本在避免遗忘和提升性能中的效果，比较不同的回放策略。

Result: 验证TM方法在多种回放策略中表现优越，增强模型性能和鲁棒性，说明少量样本即可融合任务连续性与能力保持。

Conclusion: 通过激活状态约束和样本回放，有效解决LLMs的持续学习难题，为未来模型持续学习提供新途径。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [66] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: 针对有限偏好数据的小规模个性化偏好对齐任务，提出了新数据集和高效模型FaST，显著提升个性化效果。


<details>
  <summary>Details</summary>
Motivation: 解决目前个性化对话助手无法有效利用少量偏好信息的问题，提升个性化体验。

Method: 引入两个新数据集， Benchmark多种偏好对齐技术，提出高参数效率的FaST模型，利用自动发现的高层特征。

Result: FaST在多个评估指标上表现最佳，验证了其在有限数据条件下的有效性与优势。

Conclusion: 通过创新数据集和模型设计，实现了在数据有限情况下个性化偏好的有效对齐，推动个性化对话系统的发展。

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [67] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 该研究分析了当代语言模型在多跳问答任务中的推理失败，提出了细致的错误分类框架，旨在提升推理的准确性和模型的透明度。


<details>
  <summary>Details</summary>
Motivation: 理解为什么现代推理模型在复杂任务中出现幻觉现象，提升模型推理能力。

Method: 引入多维度错误分类框架，结合人工标注与自动指标分析，深入研究模型推理失败的模式。

Result: 揭示了模型在源文档多样性、信息覆盖和认知效率方面的具体错误模式，提供了改进建议。

Conclusion: 该研究为理解和改进语言模型的推理能力提供了新的分析工具，有助于推动模型的推理准确性与透明性。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon框架通过强化预训练和利用特权信息，有效增强情感模型的鲁棒性，在实际应用中优于传统方法，缩小实验室与真实环境的差距。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算模型从受控实验室环境向复杂真实环境迁移的挑战。

Method: 结合有监督对比学习（SCL）和特权信息学习（LUPI），在预训练阶段增强模型能力。

Result: 在RECOLA和AGAIN两个基准数据集上，PriCon模型优于传统LUPI和端到端模型，性能接近多模态训练模型。

Conclusion: PriCon提供了一个可扩展、实用的方案，有助于推动情感模型在实际场景中的应用，弥合实验室与现实环境的差距。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [69] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C是一种支持多维轨迹压缩的创新框架，通过频域物理建模和误差边界优化，显著优于现有方法，特别是在3D轨迹中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着位置感知设备产生日益增长的轨迹数据，亟需高效压缩方法，现有线条简化多局限于2D，忽视时间同步和运动连续性。

Method: 结合频域物理模型与误差边界优化，支持任意维度轨迹的压缩，通过独立压缩每个空间轴实现多维支持。

Result: 在四个实际数据集上，压缩比优于现有算法19.2%，误差降低32.6%，并在3D数据集表现出49%的压缩比提升，计算复杂度保持不变。

Conclusion: PILOT-C有效扩展了线条简化的应用范围，提供了高效且多维适用的轨迹压缩解决方案，推动轨迹数据管理的发展。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [70] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: 提出CX-Mind模型，通过思考-回答的交错推理方式提升胸片诊断的准确性和可靠性，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态大模型在多任务诊断中推理冗长、奖励稀疏、幻觉频繁的问题，提升医学影像诊断的效率和可解释性。

Method: 构建CX-Set大型指令调优数据集，采用课程式强化学习和可验证过程奖励，进行两阶段优化，结合闭域和开放域诊断。

Result: CX-Mind在视觉理解、文本生成和时空对齐方面优于对比模型，平均性能提升25.1%，在真实临床数据中表现优异，并获得多中心专家认可。

Conclusion: CX-Mind通过创新的推理方式和训练策略，有效提升胸片诊断的准确性和可信度，具有重要临床应用价值。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [71] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 提出一种名为LKS的编辑方法，可大规模、精准地修改大模型中的实体知识，同时保持其性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识更新和纠正中的局限性，尤其是大规模修改的难题。

Method: 通过研究模型内部表示，利用轻量级超网络操控特定实体的潜在知识，实现类似编辑自然语言的调整。

Result: 在Llama-2和Mistral模型上实验证明，LKS可同时进行高达10,000个实体的编辑，效果良好。

Conclusion: LKS是一种高效、精准的大规模知识编辑工具，既能修正信息，又能保持模型的整体能力。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [72] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一种多模态梯度提升框架，融合眼底图像、临床特征和文本描述，用于青光眼风险预测，表现优异且具有良好的解释性。


<details>
  <summary>Details</summary>
Motivation: 提前准确检测青光眼以预防视力不可逆丧失，但现有方法多依赖单一模态且缺乏解释。

Method: 结合卷积编码器提取眼底图像特征、利用transformer模型编码文本描述，并融合多模态信号通过改进的XGBoost模型进行分类。

Result: 在真实数据集上验证，模型达到98.71%的验证准确率，特征分析与临床表现一致，具备良好可解释性。

Conclusion: GlaBoost提供了一种透明、可扩展的多模态诊断方案，可推广应用于其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [73] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种结合全局低秩与局部平滑的Tucker分解模型LRTuckerRep，用于多维数据补全，表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有多维数据补全方法在计算效率和效果上的不足，特别是低秩方法的计算成本和结构破坏，以及平滑方法的参数调节难题。

Method: 引入自适应核范数和无参数拉普拉斯正则，实现全局低秩与局部平滑的统一建模，设计两种收敛保证的迭代算法解决非凸优化。

Result: 在多维图像修复和交通数据填充任务中，LRTuckerRep在高缺失率下表现出优越的准确性和鲁棒性。

Conclusion: LRTuckerRep成功融合了全局与局部信息，为多维数据补全提供了高效且可靠的解决方案，具有广泛应用前景。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [74] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出一种利用大规模语言模型自动生成和聚合贝叶斯先验分布的方法框架 LLMPrior，解决先验设定的主观性和扩展性难题。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯推断中先验分布设定的手工、主观且难以扩展的问题。

Method: 引入LLMPrior，通过结合大语言模型与显式生成模型（如高斯混合模型）实现先验的自动生成。此外，扩展至多主体系统，使用对数意见池聚合分散知识产生的先验分布，提出联邦聚合算法Fed-LLMPrior。

Result: 提出的框架能自动生成符合数学性质的先验分布，并在多主体场景下实现稳健的分布聚合，为复杂贝叶斯建模提供新工具基础。

Conclusion: 该工作为利用LLMs生成和融合贝叶斯先验提供了理论基础和实现途径，有助于降低贝叶斯建模的门槛。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [75] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 提出了一种面向单一未知环境的在线分布鲁棒强化学习算法，具有高效性和理论最优性。


<details>
  <summary>Details</summary>
Motivation: 解决模拟到现实的迁移问题，克服现有方法对已知环境依赖的限制。

Method: 利用$f$-发散距离定义的不确定性集，设计了计算效率高、带有次线性遗憾保证的算法。

Result: 算法在多种环境中表现出优越的鲁棒性和效率，与理论最优界接近。

Conclusion: 该研究实现了在未知单一环境中的在线分布鲁棒强化学习，推动实际应用的可行性。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [76] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 提出一种新的策略优化方法GTPO，解决GRPO在训练过程中出现的冲突令牌和分布平坦化问题，增强训练稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: 为了克服GRPO在语言模型训练中的两个主要局限性：冲突令牌导致的梯度冲突和负奖励导致的输出分布扁平化，提升训练稳定性与效果。

Method: 引入GTPO策略，通过识别冲突令牌，跳过负向更新，增强正向更新，同时过滤高熵完成以防止策略崩溃，不依赖KL散度正则化。

Result: 在多个基准测试（GSM8K、MATH、AIME 2024）上验证，GTPO相较于GRPO具有更好的训练稳定性和性能。

Conclusion: GTPO通过冲突令牌识别与过滤、一系列激励机制，有效提升策略训练的稳定性，避免分布扁平化，优于传统GRPO。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [77] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种全深度学习的物理信息网络U-PINet，用于高效且物理一致的电磁散射建模。


<details>
  <summary>Details</summary>
Motivation: 解决传统数值方法计算复杂性高和纯深度学习缺乏物理约束的局限性。

Method: 构建结合多尺度处理和稀疏图表示的U-PINet，模拟近场和远场相互作用，融入物理知识。

Result: 模型在表面电流分布和雷达散射截面预测中表现优异，计算效率显著提高，优于传统方法和纯深度学习模型。

Conclusion: U-PINet实现了高效、物理一致的电磁散射模拟，有助于雷达成像与识别应用。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [78] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经网络的核聚变装置中热通量估算方法，显著提高效率并保持精度。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法FEM在实时性方面存在局限性，亟需更快速、精准的估算手段。

Method: 结合物理信息神经网络（PINN）和少量数据采样，模拟热传导过程。

Result: 实现了与FEM等效的高精度，同时效率提升40倍。

Conclusion: PINN在核聚变热通量估算中表现优异，具备广泛应用前景。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [79] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 提出一种多模态多任务模型SoilNet，用于复杂层级的土壤地平线分类，结合图像和地理时空元数据，提高准确率。


<details>
  <summary>Details</summary>
Motivation: 土壤地平线分类复杂，受多模态特征、层级关系影响，现有方法难以应对。

Method: 利用结构化模块化管道，整合图像及元数据，预测深度标记，分割土壤轮廓，提取形态特征，并基于图结构进行层级标签预测。

Result: 在实际土壤数据集上验证效果优越，提升分类性能。

Conclusion: 通过多模态多任务和图结构化方法，有效处理复杂层级分类问题，推动土壤监测技术发展。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [80] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文提出了伯努利-LoRA，一种结合概率机制的理论框架，分析了多种变体的收敛性，并验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 理解和优化参数效率微调（PEFT）中的LoRA方法，特别是在大模型背景下的理论基础。

Method: 引入Bernoulli机制，统一分析多种LoRA变体，通过非凸优化理论保证其收敛性，同时扩展到凸非光滑函数的情况。

Result: 不同变体均获得收敛性保证，理论分析与实验证明其有效性。

Conclusion: 为PEFT中的LoRA方法提供了系统的理论支持，促进其实际应用与发展。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [81] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO是一种不依赖模型不确定性估计的神经网络优化方法，能在高维空间中高效找到优化解，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维高评价次数黑箱优化中GP模型计算复杂性高的问题，同时克服基于神经网络的不确定性估计复杂性。

Method: 提出SNBO，与探索和利用策略结合，动态控制采样区域，无需模型不确定性估计。

Result: 在10到102维的优化问题中，SNBO表现优异，获得更好的函数值，减少40-60%的函数评价和显著缩短运行时间。

Conclusion: SNBO在高维黑箱优化中展现出优越性，为大规模优化提供了高效、实用的解决方案。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [82] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 提出一种同时保证差分隐私和公平性的多臂老虎机算法框架DP-NCB，兼顾隐私保护与公正，达到近似最优的Nash遗憾。


<details>
  <summary>Details</summary>
Motivation: 在高度敏感的应用场景中，需要同时保障用户数据隐私和确保算法的公平性，但现有方法难以兼顾两者。

Method: 设计DP-NCB算法框架，结合差分隐私和Nash遗憾最小化策略，支持全球与局部隐私模型，无需预先知道时间长度，且具有随时适用性。

Result: 理论上证明了DP-NCB在隐私和公正性方面的优越性，通过仿真实验验证其优于现有基线的Nash遗憾表现。

Conclusion: 该方法为高风险社会应用中多臂老虎机的隐私保护与公平性提供了统一的解决方案，具有重要实际意义。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [83] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种可分训练的代理模型（VAE-DNN），用于求解参数化非线性偏微分方程，显著提高训练效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决非线性偏微分方程的计算复杂性，提高训练效率和模型准确性。

Method: 采用编码器-解码器结构与变分自编码器(VAE)结合，独立训练模型的三个组成部分，实现高效训练。

Result: VAE-DNN在地下水流动模型的正逆问题中，比FNO和DeepONet模型更高效且更准确。

Conclusion: 该方法通过分部训练显著提升了求解偏微分方程的效率和效果，为多物理场问题提供了新的解决方案。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [84] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 提出了一种结合 crowdsourced KPI 和监管数据的时空预测框架，用于提升频谱需求预测的准确性和泛化能力，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 频谱需求预测对频谱合理分配和政策制定至关重要，但现有模型存在数据限制和假设不现实的问题。

Method: 采用先进的特征工程、相关性分析和迁移学习，结合详细的空间和时间数据，实现高精度预测。

Result: 验证显示该框架比ITÚ模型更准确，具有良好的跨区域适应性，为频谱管理提供了更实用的工具。

Conclusion: 该方法在频谱需求预测中展现出巨大潜力，有助于政策制定者优化频谱资源配置。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [85] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 提出一种基于信息论的智能数据抽样方法，旨在提高离线学习中的预测性能。


<details>
  <summary>Details</summary>
Motivation: 在数据流中获取有用信息，同时控制计算成本，是学习模型面临的主要挑战。

Method: 通过信息论方法，专注于减少对目标预测的不确定性，进行数据抽样选择。

Result: 在两个广泛研究的问题上，该方法优于先前的技术，并显示出有效性。

Conclusion: 实现良好的性能依赖于细致的模型设计，需要在实际应用中慎重考虑。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [86] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 一种新颖的稀疏智能采样框架SICKLE通过最大熵采样，有效减少数据量，提高模型准确性和能源效率。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和Dennard缩放的终结，大规模训练的能耗和数据需求不断增加，亟需高效的数据采样方法。

Method: 提出SICKLE框架，结合最大熵采样策略，进行大规模湍流数据子采样，并在超算平台上验证能效和性能。

Result: 在湍流模拟数据集和Frontier超算上，SICKLE提升模型准确率并显著降低能耗，最多减少38倍能耗。

Conclusion: 稀疏智能采样改变大规模训练的传统，能在保证模型性能的同时，大幅度降低训练成本和能耗。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [87] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出一种结合冲动控制和切换控制的强化学习框架，用于糖尿病自动胰岛素调控，提升血糖管理的安全性与效果。


<details>
  <summary>Details</summary>
Motivation: 在慢性疾病糖尿病管理中，合理控制生理变量具有挑战性，强化学习有潜力实现个性化治疗，但受制于延迟和异质性效应。

Method: 构建一个受限制的马尔可夫决策过程，结合物理状态特征，模拟胰岛素下降等生物因素，支持安全策略学习。

Result: 在模拟任务中，显著降低血糖异常发生率，从22.4%降至10.8%。

Conclusion: 为未来医疗中的安全、时序感知强化学习奠定基础，提供理论保证和实证验证。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [88] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 该研究结合多任务学习与循环神经网络，优化传统生物物理模型，用于葡萄物候期预测，显著优于传统模型和基础深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 提高葡萄物候期预测的精度，支持细粒度葡萄园管理决策。

Method: 结合多任务学习和递归神经网络参数化可微生物理模型，利用多任务学习共享不同品种数据。

Result: 新方法在物候阶段及其他农作物状态变量预测上优于传统模型和基础深度学习模型。

Conclusion: 结合多任务学习和深度神经网络能有效提升生物物理模型的预测性能，适用于实际农业管理。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [89] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本论文提出在距离基分类器中揭示隐藏神经网络结构，结合层关联相关传播技术，提高模型的可解释性，并通过实例验证其效果。


<details>
  <summary>Details</summary>
Motivation: 随着距离分类器在科学与工业中的广泛应用，增强其可解释性是提升模型透明度和信任度的需求。

Method: 作者发现距离分类器中嵌入了类似神经网络的结构，并利用层关联相关传播(LRP)技术进行解释。

Result: 该方法在多个评估基准上优于传统方法，且在实际应用中展示了其解释能力的价值。

Conclusion: 揭示距离分类器的潜在神经网络结构，为其提供更强的可解释性，是提升模型实用性的重要路径。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [90] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 主动学习和迁移学习结合用于跨域时间序列异常检测效果有限，单一簇表现最佳，性能提升有限趋于饱和。


<details>
  <summary>Details</summary>
Motivation: 探讨结合主动学习与迁移学习在跨域时间序列异常检测中的效果和机制。

Method: 实验设计比较不同的簇划分和数据采样策略，分析性能变化及收敛情况。

Result: 单一簇模型效果最佳，新增样本提升有限，性能趋于饱和，说明主动学习提升有限且趋势线性。

Conclusion: 主动学习有效，但性能提升有限，建议其选择性和优化策略以提升实际应用表现。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [91] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 提出一种结合流形学习和机器学习的方法，用于从微观模拟中学习人群动态的宏观模型，有效实现不同尺度的桥接。


<details>
  <summary>Details</summary>
Motivation: 解决人群动力学中微观与宏观模型间的尺度桥接难题，提升模拟的效率和准确性。

Method: 构建一个四阶段流程，包括从微观数据导出宏观密度、基于流形学习映射到潜在空间、在潜在空间使用LSTM和MVAR学习简化模型，以及重建高维人群动态。

Result: 该方法在模拟中表现出高精度、鲁棒性和泛化能力，验证了其在复杂环境下快速准确模拟人群行为的潜力。

Conclusion: 通过结合流形学习与机器学习，有效实现微观到宏观的尺度转换，为人群动力学建模提供了新的强大工具。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [92] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [93] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出了一种基于最优运输的后处理框架FairPOT，用于在保持AUC性能的同时改善公平性，特别适用于风险评分而非二元结果。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域中，公平性评估常基于风险评分，但严格的公平性限制会损害AUC性能，需平衡两者。

Method: 通过调节控制比例（top-lambda分位数）对风险评分分布进行有选择的调节，利用最优运输实现不同组别的分布对齐，可应用于整体和部分AUC场景。

Result: 在多种数据集上，FairPOT优于现有方法，兼顾公平性和性能，具有良好的实用性。

Conclusion: FairPOT是一种高效、实用的后处理技术，可在保障AUC性能的基础上改善模型公平性，适合实际应用。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [94] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 提出了一种基于深度算子网络的气泡动力学模拟方法BubbleONet，结合物理信息和自适应激活函数，有效提升高频特征表达，性能优于传统数值解算器。


<details>
  <summary>Details</summary>
Motivation: 解决气泡动力学模拟中计算成本高和高频信息难以捕捉的问题。

Method: 基于PI-DeepONet框架，加入Rowdy激活函数，比较单步和双步训练策略。

Result: 在多种气泡动力学模型中表现出良好效果，是一种高效的替代数值模拟工具。

Conclusion: BubbleONet具有潜力用于气泡动力学的快速模拟，兼具高精度与计算效率。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [95] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP是一种动态、用户可控的少样本隐私保护感知框架，通过多模态对比学习和语言引导的转换技术，有效保护IMU传感数据中的敏感信息。


<details>
  <summary>Details</summary>
Motivation: 随着携带式设备广泛使用，IMU传感数据可能泄露用户敏感行为，现有隐私保护方法缺乏灵活性和用户自主性。

Method: 利用多模态对比学习，将IMU数据与自然语言描述对齐，结合语言引导的活动清洗和运动生成模块，实现感知敏感行为的少样本检测与数据变换。

Result: 在多个人体活动识别数据集上，PrivCLIP显著优于基线方法，在隐私保护和数据实用性方面表现优异。

Conclusion: PrivCLIP提供了一种灵活、用户可控的隐私保护方案，有助于提升感知系统的隐私保障水平。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [96] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: 提出一种新型的TC-LoRA方法，通过文本和参数两个层面减少多任务学习中的任务干扰，提高模型适应性。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中，合并异质任务的LoRA适配器容易引起任务干扰，影响模型性能。

Method: 在文本层面进行训练样本聚类并为每类训练专属LoRA适配器，在参数层面引入联合CP分解以分离任务特有和共享因素。

Result: 在各种任务上进行广泛实验，优于SVD基线，在Phi-3和Mistral-7B任务中分别提升准确率1.4%和2.3%。

Conclusion: TC-LoRA通过双重策略有效缓解任务干扰，提升多任务适应性，具有较强应用潜力。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [97] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 引入DCFL框架解决联邦学习中对比学习的负样本不足问题，通过分离对比损失的对齐与均匀性实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异质性导致的模型性能下降，尤其是在对比学习中负样本不足的问题。

Method: 提出解耦对比学习（DCFL），将对比损失分为对齐和均匀性两个目标，独立调节。

Result: 在多个数据集和基准测试中，DCFL优于现有方法，表现出更好的样本对齐和类别均匀性。

Conclusion: DCFL有效改善联邦学习中对比学习的适用性和性能，可推广到实际应用中。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [98] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 本文对TensorFlow与PyTorch两大深度学习框架进行了全面对比，涵盖易用性、性能、部署、生态支持等方面，强调二者的异同及适用场景。


<details>
  <summary>Details</summary>
Motivation: 为了帮助研究人员和工程师在选择深度学习框架时做出明智决策，本文系统比较了TensorFlow和PyTorch。

Method: 通过分析框架的编程范式、性能基准测试、部署工具、生态系统支持以及应用案例，进行全面对比。

Result: 发现PyTorch适合研究灵活开发，TensorFlow更适合生产部署，各有优势。

Conclusion: 理解两者的差异与优势，有助于针对具体需求选择合适的框架，推动深度学习实践与发展。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [99] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 提出联邦数据学习方法FeDaL，通过分布式架构减轻时间序列数据集异质性带来的偏差，提升TSFMs的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数据集异质性造成时间序列基础模型的泛化能力下降，这是一个亟需解决的问题。

Method: 利用联邦学习架构，将异质时间序列数据分解为共享的泛化知识和个性化知识，使用领域偏差消除和全球偏差消除机制。

Result: 在八项任务中对比54个基线方法，FeDaL显著提升模型的跨数据集泛化能力，且详细分析了数据规模、客户端数量对模型性能的影响。

Conclusion: 联邦学习架构及Bias消除机制有效应对异质性，提高TSFMs的泛化性与适应性，具有广泛应用前景。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [100] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [101] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 微调方法在短答题自动评分中表现有限，但在特定条件和模型下展现出潜力，尤其是结合生成的合成数据后。


<details>
  <summary>Details</summary>
Motivation: 探索和评估不同微调技术在自动短答题评分中的效果，以超越仅靠提示工程的局限。

Method: 对比分析了少样本提示、OpenAI的微调服务以及开放权重模型如QLORA的微调效果，特别关注结构化输出。

Result: 微调对Llama模型效果有限，但在OpenAI封闭模型中优于提示技术。结合合成数据后，LLama 3.1 8B-Instruct模型表现显著提升。

Conclusion: 微调技术在特定模型和应用场景下具有潜力，但受限于模型特性和数据域，需要结合生成数据优化策略。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [102] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT提出了一种基于潜在空间的多目标隐藏攻击方法，具有高度变异性和隐蔽性，对抗现有FL防御措施具有较强威胁。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习后门攻击受限于固定触发模式，缺乏多样性和隐蔽性。

Method: 利用潜在驱动的条件自编码器生成多样化、目标特定的触发器，实现可调节、隐蔽的后门攻击。

Result: FLAT在实验中实现了高攻击成功率，且能有效规避先进的防御机制，展示了其攻击的有效性和隐蔽性。

Conclusion: 新型多目标、变异性强的后门攻击对当前防御措施构成挑战，亟需发展新的防御策略应对潜在威胁。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [103] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种结合对抗训练的公平多视角聚类框架AFMVC，有效去除敏感属性影响，兼顾公平性与聚类性能。


<details>
  <summary>Details</summary>
Motivation: 随着多视角聚类应用增加，公平性成为重要问题，但现有方法主要关注性能，忽略了公平性。

Method: 采用对抗训练从特征中剔除敏感属性信息，并通过KL散度保证不同视角聚类一致性，融合公平性与性能。

Result: 在多个数据集上实验显示，AFMVC在公平性和聚类效果方面优于现有方法。

Conclusion: 引入对抗机制确保公平性，同时保持聚类效果，为多视角聚类提供了理论与实践支持。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [104] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 本文首次系统研究了视觉-语言模型（VLMs）的模型逆向攻击（MI），提出多种新颖的基于Token和序列的逆向策略，实验验证VLMs存在严重的训练数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs在医疗、金融等领域应用广泛，其隐私安全问题值得关注，但尚未被充分研究。

Method: 提出Token-based和Sequence-based逆向策略（TMI、TMI-C、SMI、SMI-AW），并进行大量实验和用户调研。

Result: 发现VLMs易受到训练数据泄露攻击，序列方法（特别是SMI-AW）在攻击准确率和视觉相似性方面优于Token方法，实验证明攻击成功率达75.31%。

Conclusion: VLMs存在严重隐私泄露风险，亟需制定相应保护措施，研究具有重要现实意义。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [105] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于一致性的新型策略优化框架，有效解决奖励一致性导致的梯度消失问题，提升了大语言模型在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于规则的奖励在引导策略优化时，由于样本输出高度一致导致优势函数退化为零，影响学习效果。

Method: 引入结构化全球奖励和熵调节机制，保障高度一致的响应仍能提供有意义的学习信号并在探索与收敛间动态平衡。

Result: 在多个数学推理任务上取得显著性能提升，验证了方法的有效性和鲁棒性。

Conclusion: 该框架有效改善了模型训练中的奖励反馈问题，具有广泛适用性和潜在的应用价值。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [106] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 提出一种半监督深度域适应框架，用于改进不同气象区域的太阳能发电预测，显著提高不同地区的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决地理和气象差异导致的模型迁移问题，以及数据标注不足的挑战。

Method: 利用无源教师-学生模型，通过一致性和交叉熵损失进行半监督学习，仅用目标区域少量标注数据完成模型适应。

Result: 在加利福尼亚、佛罗里达和纽约等地区实现了最高11.36%、6.65%、4.92%的预测精度提升。

Conclusion: 所提出的方法有效缓解了域偏差，减少了对大量标签数据的需求，为多区域太阳能预测提供了可行方案。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [107] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 结合MIST编码器和MolForge解码器，利用预训练显著提升从质谱数据进行新颖分子结构生成的效果，实现了优越的性能，提供未来研究的强有力基线。


<details>
  <summary>Details</summary>
Motivation: 解决质谱数据到新颖分子结构的转化难题，提高生成准确率。

Method: 采用MIST作为编码器，MolForge作为解码器，通过预训练增强性能，采用阈值化概率以提升结构恢复能力。

Result: 性能提升十倍，正确生成结构比例显著提高，Top-1 28%，Top-10 36%。

Conclusion: 该方法为从质谱数据进行新分子结构解析提供了强有力的基础，展现了显著优越性。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [108] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出一种基于交替最小化和可训练步长的深度神经网络训练新方法，改善训练稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中非凸优化带来的不稳定性和高计算成本。

Method: 采用层块交替优化策略结合元学习引入多种可训练步长方式实现自适应调整。

Result: 在多项基准测试中优于现有方法，展现了良好的泛化能力和较少的参数更新。

Conclusion: 该方法在提升训练稳定性、效率及泛化能力方面展现出显著潜力，是深度学习优化的有效工具。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [109] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 提出了一种因果奖励调整（CRA）方法，有效缓解奖励破解问题，提升推理路径的正确性。


<details>
  <summary>Details</summary>
Motivation: 随着外部推理系统在复杂任务中的应用，奖励破解现象严重影响其准确性，因此需要更有效的奖励机制。

Method: 通过训练稀疏自编码器以恢复可解释特征，并利用因果推断中的后门调整校正 confounding 变量，从而估算真实奖励。

Result: 实验表明CRA显著减少奖励破解，提高了数学问题解决的准确率，同时无需修改策略模型或重新训练奖励模型。

Conclusion: CRA通过引入因果推断机制，有效增强推理路径的奖励质量，提升系统整体性能。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [110] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 提出了一种基于对称信息散度的离线强化学习新框架，解决了对称散度处理中的分析性与数值问题，提出了实用算法S$f$-AC，实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有的BRPO方法主要采用非对称散度如KL，缺乏对对称散度的研究，且对称散度在极值分析和数值稳定性方面存在挑战。

Method: 引入对称$f$-散度，通过泰勒展开解决分析性和数值问题，设计了S$f$-AC算法。

Result: 在分布逼近和MuJoCo任务中，S$f$-AC表现出竞优性能，验证了其有效性。

Conclusion: 对称散度在离线RL中具有潜力，但需通过泰勒展开等技术克服分析和数值障碍，本文的S$f$-AC为实用提供了方案。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [111] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: 提出DCATS，通过改进数据质量提升时间序列预测性能，获得显著误差降低。


<details>
  <summary>Details</summary>
Motivation: 探索在时间序列AutoML中，优化数据质量而非模型结构的潜力。

Method: 设计DCATS，利用元数据对数据进行清洗，同时优化预测效果。

Result: 在多个预测模型和大规模数据集上，平均误差降低6%，验证方法有效性。

Conclusion: 数据驱动的方法在时间序列AutoML中具有巨大潜力，值得进一步研究。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [112] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 本文提出一种基于深度学习的自动多普勒角度估计方法，利用预训练模型提取特征，通过浅层网络实现角度估算，准确度满足临床需求，有助于提升血流速度测量的精确度。


<details>
  <summary>Details</summary>
Motivation: 多普勒超声血流速度测量中，角度估计误差是主要来源之一，需自动化以提升效率和准确性。

Method: 使用五个预训练模型提取特征，结合自定义浅层网络进行角度估计，并用人类观察进行对比验证。

Result: 最佳模型的平均绝对误差小于临床可接受阈值，验证了方法的有效性。

Conclusion: 深度学习技术具有应用潜力，可在商用超声设备中实现自动化血流角度估计，改善诊断的准确性。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [113] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: 提出了一种名为T3Time的多模态时间序列预测模型，通过整合时域、频域和提示信息，有效捕捉变量间复杂关系，显著优于现有方法，特别在少量数据情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于变换器的多变量时间序列预测方法受限于刚性偏见和静态融合策略，难以捕捉细粒度、时域特定的变量关系，影响预测性能。

Method: 引入具有频率编码的时间、频谱和提示三模态结构，利用门控机制对多模态信息进行动态加权融合，并采用多头跨模态对齐机制增强特征整合。

Result: 在多个基准数据集上的实验显示，模型显著优于先进的基线，平均降低3.28%的MSE和2.29%的MAE，在少样本学习条件下表现尤佳。

Conclusion: T3Time通过多模态融合策略，有效提升多变量时间序列预测的精度和泛化能力，为未来复杂时间序列建模提供新思路。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [114] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一个Python工具，用于通过集成的局部和全局敏感性分析，帮助用户直观理解机器学习模型的行为，支持无编程的交互式工作流程。


<details>
  <summary>Details</summary>
Motivation: 探索和理解机器学习模型的行为，提升模型透明度和可解释性，满足AI研究者和领域专家的需求。

Method: 结合全局和局部敏感性分析技术，开发交互式界面，自动化模型训练、选择及解释过程，支持人机交互。

Result: 在泰坦尼克数据集上的分类任务中验证了该系统，展示了敏感性信息如何引导特征选择和数据优化。

Conclusion: SAInT通过可视化和自动化工具增强模型理解，为模型调优和特征工程提供直观支持。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [115] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 提出一个名为Mockingbird的框架，将大型语言模型（LLMs）适应于通用机器学习任务，虽然在某些任务上表现尚可，但目前仍无法超越专业领域知识和人类专家的反馈。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在超越聊天机器人应用之外的潜在能力，尤其是在通用机器学习任务中的应用。

Method: 设计并实现Mockingbird框架，指导LLMs扮演特定角色并反思错误以提升性能。

Result: Mockingbird在多个通用机器学习任务中取得了可接受的效果，但反思机制尚不足以取代专业知识和专家反馈的优势。

Conclusion: LLMs具有潜在的多任务适应能力，但当前反思机制仍需结合领域知识和人类专家指导以取得更优表现。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [116] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: VL-DAC是一种轻量级、无超参数的强化学习算法，能有效训练大规模视觉语言模型，实现跨任务的泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型缺乏将视觉观察转化为语言指导行为的能力，难以实现多模态交互。

Method: 提出VL-DAC算法，在多个模拟环境中训练单一VLM，通过分离行动和价值学习，增强训练稳定性和效率。

Result: VLM使用VL-DAC训练后，在多个任务和评估基准上表现出显著的泛化能力提升，无需复杂调参或高密度奖励环境，证明了简易RL算法在实际应用中的有效性。

Conclusion: VL-DAC为训练大规模VLM提供了低成本、高效的方法，显示了强化学习在多模态任务中的潜力和前景。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [117] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出一种基于权重显著性的新型两阶段机器遗忘方法WSS-CL，有效提升图像分类模型的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器遗忘方法在精准性、稳定性和适用性方面的不足。

Method: 结合最大熵伪标签与对比学习的两阶段策略，用权重显著性引导遗忘过程。

Result: 显著提升遗忘效果，性能差距缩小，适用于监督和自监督场景。

Conclusion: 该方法在确保模型性能的同时，提高了遗忘的效率和准确性，具有实用价值。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [118] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 提出了一种通过对训练语料中的标记进行正负分类，结合“遗忘”机制，改进大规模语言模型的微调效率和效果的方法。


<details>
  <summary>Details</summary>
Motivation: 解决微调对数据质量和数据量的依赖问题，以提升模型性能和泛化能力。

Method: 将语料中的标记划分为正向和负向两类，负向标记在训练中被显式遗忘，结合模型训练过程实施相关机制。

Result: 该方案提升了模型在多个基准任务上的性能和响应多样性，验证了遗忘机制的有效性。

Conclusion: 通过显式的标记分类和遗忘机制，有效增强大模型微调的鲁棒性和性能表现。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [119] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS通过分布式特征共享实现安全高效的私有推理，兼顾隐私保护与计算效率，且通过扩展增强安全性。


<details>
  <summary>Details</summary>
Motivation: 解决云端ML推理中的隐私与效率矛盾，寻找一种既能保护数据隐私又具高效性的方案。

Method: 将输入特征划分成多个共享分片，分布给不合作的服务器进行部分推理，客户端再汇总结果，结合对抗训练和密钥多样化策略增强隐私保护。

Result: 在CIFAR-10和CelebA数据集上，PrivDFS实现了类似深度分割推理的隐私保护，同时大幅降低客户端计算成本，且扩展依然抗攻击。

Conclusion: PrivDFS提供了一种高效且安全的私有推理方案，通过特征分割与多重保护扩展，有望在云端ML应用中广泛推广。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [120] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出一种多边缘随机流匹配（MMSFM）方法，有效建模高维系统在非均匀时间点的演变，无需降维，增强对非平衡系统关键动态的捕捉。


<details>
  <summary>Details</summary>
Motivation: 解决有限非均匀时间点观测下高维系统演变建模的挑战，传统方法难以捕捉关键瞬态行为。

Method: 扩展模拟无关的分数及流匹配技术到多边缘设置，引入测度样条增强对时间不规则性的鲁棒性，结合分数匹配避免高维过拟合。

Result: 在合成数据和基准数据集上验证，包括基因表达数据和图像演变任务，显示出方法的广泛适用性和有效性。

Conclusion: MMSFM提供了一种强大的工具，用于高维系统在不均匀时点的演变建模，克服了传统降维方法的局限，具有重要实践价值。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [121] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出了一种针对多实例学习(MIL)的持续学习新方法，有效应对数据分布变化，尤其适用于血液疾病诊断中的模型更新。


<details>
  <summary>Details</summary>
Motivation: 解决多实例学习在动态环境中模型出现灾难性遗忘的问题，以适应疫情等快速变化的临床数据环境。

Method: 采用重放策略，结合实例注意力得分和距离度量，选择代表性样本存储以保持数据多样性，进行增量学习。

Result: 在血液学数据的实际应用中，显著优于现有持续学习方法，有效适应数据分布变化，首次实现针对MIL的持续学习。

Conclusion: 为单细胞血液疾病诊断模型的动态更新提供了有效解决方案，推动了持续学习在医学影像和诊断中的应用前景。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [122] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种结合算法与系统优化的INT6量化框架，能在保持接近FP16精度的同时显著提升GPU推理速度和节省内存。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型的兴起，模型的存储与计算成本成为瓶颈，现有INT4/INT8量化存在性能折衷，INT6在精度与效率间具有潜力，但缺乏硬件支持。

Method: 提出统一6位权重量化和层敏感性分析，开发支持W6A6和W6A8的GPU内核，利用Binary Tensor Core技术绕过缺乏原生INT6支持的问题。

Result: 在LLaMA模型上的测试显示，FlexQ保持几乎与FP16相当的性能，通用GPU核实现显著提升推理速度与节省内存。

Conclusion: FlexQ在无需硬件更换的情况下，为INT6量化提供了高效实现方案，推动大模型的实际应用潜力。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [123] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 该综述系统分析了2020-2024年的多模态模型可解释性研究，发现大部分集中在视觉语言和单一语言模型，方法多使用注意力机制，但仍存在解释深度不足和评估不系统的问题。


<details>
  <summary>Details</summary>
Motivation: 推动多模态模型的可解释性，以提高其透明度和信任度，满足对可信AI的需求。

Method: 对2020-2024年的相关文献进行系统回顾，分析模型架构、模态类型、解释算法和评估方法。

Result: 发现大部分研究关注视觉语言和单模态模型，多使用注意力机制，但在多模态交互解释和评估体系方面存在不足。

Conclusion: 提出改进建议，包括加强多模态交互理解，建立标准化评估体系，以实现更具可解释性和责任性的多模态AI系统。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [124] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了新的随机算法用于估计矩阵范数，适用于矩阵向量乘法操作，具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 解决高效估计矩阵范数在深度学习和推荐系统中的需求，提高算法效率。

Method: 基于对Hutchinson及其Hutch++版本的改进，设计新的随机算法，并给出复杂度界。

Result: 算法在神经网络正则化和抵抗对抗攻击中表现出良好的实用性。

Conclusion: 该方法在矩阵范数估计和实际应用中具有潜力，为相关领域提供了有效工具。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [125] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: 提出了一种结合云模型特征函数的生成模型CMCFAE，有效提升复杂数据的建模能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统变分自编码器在复杂数据建模中受限于高斯先验和模式化的问题。

Method: 引入云模型特征函数作为正则化项，结合Wasserstein Auto-Encoder框架，优化潜在空间。

Result: 在MNIST、FashionMNIST、CIFAR-10和CelebA上，表现优于现有模型，提升重构质量、潜在空间结构和样本多样性。

Conclusion: 云模型特征函数为auto-encoder模型提供了更丰富的潜在空间表达，有望推动生成模型的发展。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [126] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 提出通过强化学习训练AI进行复杂对抗，提升大模型的漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动化方法不能充分捕捉对抗对话的复杂性，亟需更有效的策略进行模型的安全性测试。

Method: 将红队任务形式化为马尔科夫决策过程，并采用分层强化学习训练生成对抗策略。

Result: 所提方法在多轮攻击策略识别方面优于现有方法，推动大模型安全评估进入动态轨迹阶段。

Conclusion: 通过策略性“破坏”实现更有效的金子塔式漏洞探索，增强LLMs的安全性与信任度。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [127] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 本论文研究了注意力机制在小规模任务切换中的表现，发现扩展的注意力机制（如extensive attention）能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索注意力机制在小规模应用中的潜力及其局限，尤其是在任务切换场景中。

Method: 通过比较标准Transformer、LSTM、MLP及其扩展模型（如cisformer和extensive attention），在任务切换的基准模型上进行测试。

Result: 仅使用extensive attention的模型能达到约95%的预测准确率，显示其优越性。

Conclusion: 不同注意力形式的比较有助于理解和提升注意力机制在任务切换中的应用效果。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [128] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出一种基于缓存的并行投机解码框架，提高大模型推理速度。


<details>
  <summary>Details</summary>
Motivation: 提升大模型推理效率，克服传统投机解码“先草稿后验证”带来的性能瓶颈。

Method: 采用“查询与校正”范式，分离草稿生成与验证过程，使目标模型并行校正草稿模型的生成。

Result: 实现了最大4.83倍的加速，无需微调草稿或目标模型。

Conclusion: 通过缓存机制和并行架构，有效提升大模型推理速度，推动投机解码技术发展。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [129] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal是一种基于Transformer的神经算子方法，通过同时学习和融合局部与全局特征，有效提升多尺度问题的模拟准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer神经算子在多尺度物理问题中缺乏局部与全局特征协调学习的问题，提升物理一致性和数值稳定性。

Method: 提出GFocal方法，结合Nyström注意力的全局块和切片的焦点块生成物理感知的tokens，并通过卷积门控块实现多尺度信息的动态融合。

Result: GFocal在多个基准测试中表现出优越性能，平均提升15.2%，并在汽车气动和机翼等工业模拟中表现出色。

Conclusion: GFocal成功实现了多尺度特征的同步学习与融合，提升了复杂物理问题的模拟能力，具有重要的应用前景。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [130] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: 提出了一种基于解析解的异质性不变的联邦学习方案FedHiP，有效解决非IID数据带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中由于异构数据分布导致的收敛缓慢和性能下降问题。

Method: 利用自监督预训练与解析解技术，避免梯度更新，设计三阶段的分布式分析训练流程。

Result: 在多个基准数据集上表现优于现有方法，准确率提升5.79%-20.97%。

Conclusion: FedHiP通过引入解析解实现了异质性不变，显著改善了非IID数据环境下的个性化联邦学习性能。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [131] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 能源效率措施对不同家庭的影响差异显著，低负担家庭节能明显，高负担家庭变化有限。


<details>
  <summary>Details</summary>
Motivation: 减少国内能源需求对气候和贫困战略至关重要，但能源效率干预的效果具有高度异质性，亟需理解其机制。

Method: 利用英国住房数据，应用因果机器学习模型，分析墙体保温对燃气消耗的平均与条件效应，重点关注不同能源负担组的分布差异。

Result: 平均来看，隔热措施可降低燃气需求最多19%，但低负担家庭节能明显，而高负担家庭变化有限，行为机制显示高负担家庭倾向改善取暖舒适度而非减能。

Conclusion: 能源政策应考虑公平性与气候目标的平衡，响应行为偏好，促进健康与福祉，建立更全面的评估框架。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [132] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 提出了一种基于条件生成对抗网络的多模态情感检测方法，通过生成合成情感数据提升识别效果。


<details>
  <summary>Details</summary>
Motivation: 解决单一模态难以捕捉情感细节的问题，提升多模态情感识别的准确性。

Method: 采用cGAN架构，结合文本、音频和面部表情多模态数据进行训练，生成丰富的情感样本以增强模型性能。

Result: 实验显示新方法在情感识别准确率上优于传统模型，效果显著。

Conclusion: cGAN在多模态情感识别中具有巨大潜力，有助于改善人机交互的情感理解能力。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [133] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 本文提出一种层级评分指标，利用评分树表征类别之间的关系，实现对模型误差类型的细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有的分类和目标检测评价主要采用二元正确与错误的评分方式，未能反映误差的不同影响。

Method: 开发了多层次复杂度的层级评分指标，使用评分树编码类别关系，并基于此评估模型性能。

Result: 实验显示该方法能更细致地捕捉误差，评分树支持调优，提升模型评估的细腻度。

Conclusion: 该方法提供一种不仅按错误数量，还按错误类型和影响进行模型排名的评价新策略，Python实现将开源。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [134] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 引入Causal Reflection框架，使LLMs和RL代理能进行因果推理与修正，提高适应性。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs和传统RL代理在因果推理方面存在不足，无法理解复杂动态因果关系。

Method: 设计Causal Reflection框架，结合因果模型和反思机制，用LLMs作为推理引擎生成自然语言的因果解释与反事实。

Result: 该框架增强了代理的因果理解能力，使其能自我修正和更好地适应环境变化。

Conclusion: Causal Reflection为构建具有因果认知与自我修正能力的智能代理提供了理论基础。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [135] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM是一种结合经典信号处理与深度学习的多尺度、多通道特征提取模型，有效提升多变量时间序列分类的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer和CNN模型在多变量时间序列分类中计算繁重、频率多样性有限且参数庞大的问题。

Method: 设计一种独立通道的多尺度有限脉冲响应（FIR）滤波器，通过仿真多频带特征，减少模型复杂度。

Result: 在多个生物与行为基准测试中，PRISM与轻量级分类头结合，性能优于或等同于先进的CNN和Transformer模型，同时大幅降低参数和运算成本。

Conclusion: 将经典信号处理与现代深度学习结合，为多变量时间序列分类提供一种高效、精准的解决方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [136] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 提出了一种新的联邦交通预测模型Channel-Independent Paradigm(CIP)，无需跨客户端通信，显著降低通信成本并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着交通数据的增长，如何在保护隐私的同时高效进行交通预测成为挑战。

Method: 设计了CIP范式，结合Fed-CI框架，实现节点无需通信即可进行准确预测，减轻通讯负担。

Result: Fed-CI在多个真实数据集上表现优异，RMSE、MAE、MAPE均优于现有方法，通信成本大幅降低。

Conclusion: CIP范式有效解决了联邦交通预测中的交流瓶颈，为隐私保护和高效预测提供了新思路。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [137] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 通过分析5000多起身份盗窃案例，构建身份生态系统图模型，利用图神经网络预测个人信息泄露的风险。


<details>
  <summary>Details</summary>
Motivation: 缺乏对个人隐私风险的基本理解限制了个人和组织的隐私保护能力，亟需更有效的风险评估工具。

Method: 构建以个人信息为节点的图结构，利用图神经网络进行隐私泄露可能性预测。

Result: 所提出的方法能有效判断某一信息是否可能引发其他信息的泄露。

Conclusion: 该模型为隐私风险评估提供了新的工具，有助于提升个人信息保护水平。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [138] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 提出GraphProp方法，通过结构预测提升图基础模型的跨域结构泛化能力，增强图分类任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的图基础模型多关注节点特征，忽视结构信息的跨域一致性，影响模型泛化。

Method: 先训练结构模型预测图不变量，再用其表示作为位置编码训练全面模型，结合节点属性和标签。

Result: 在监督学习和少样本学习中，显著优于对手，特别在无节点属性的图上表现优异。

Conclusion: 强调结构信息的重要性，提供了提升跨域泛化的有效途径。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [139] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 利用物理信息驱动神经网络进行铝点焊过程的内部状态重建，实现非侵入式质量评估，解决数据整合冲突问题，展现出工业应用潜力。


<details>
  <summary>Details</summary>
Motivation: 点焊过程中的焊点直径是关键质量指标，但传统检测为破坏性测试，限制了效率。引入物理信息神经网络希望实现非侵入式质量评估。(落实于铝材焊接方面。)

Method: 提出两种新的训练策略，使用逐步引入实验数据的衰减函数，设定动态学习率和提前停止，以及条件性温度参数更新，模型采用轴向对称二维结构，首先在一维中验证策略有效性，然后扩展到二维模型实现预测。

Result: 模型成功预测焊点直径和动态位移，支持钢铝材料的转移，表现出在工业中的快速、模型驱动的质控潜力。

Conclusion: 创新训练策略和模型设计增强了物理驱动神经网络在焊接质量评估中的应用前景，特别是在非破坏性检测上的潜力显著。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [140] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出一种基于算子插值的泛化概率分布映射学习框架，具有多任务能力和零样本性能。


<details>
  <summary>Details</summary>
Motivation: 旨在超越现有流动和扩散模型的时间动态，满足多维空间概率分布的跨越及多任务需求。

Method: 通过引入矩阵或线性算子取代标量时间变量，优化随机插值方法，以支持多维空间的概率映射设计。

Result: 验证了该方法在条件生成、修复、微调、后验采样和多尺度建模等任务中的零样本效果，表现出强大的泛用性。

Conclusion: 该算子基础插值方法提供了理论统一性和极强扩展性，展现出作为任务无关通用模型的潜力。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [141] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 提出了一种基于脉冲神经网络的终身网络入侵检测系统，结合静态与动态SNN实现高效识别与分类，显著减少灾难性遗忘，适合硬件低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 受到大脑层次化处理和能效的启发，旨在提升入侵检测系统的性能和持续学习能力。

Method: 采用静态SNN进行潜在入侵检测，动态SNN进行攻击分类，结合GWR结构性可塑性和新颖的Ad-STDP学习规则实现增量学习。

Result: 在UNSW-NB15基准测试中达到85.3%的整体准确率，展示了强大的适应性和较低的灾难性遗忘，模拟验证了低功耗潜力。

Conclusion: 结合生物启发机制的SNN架构有效提升网络入侵检测的持续学习能力与能效，具有在硬件上部署的潜力。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [142] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 提出一种基于因果关系的新型时间序列异常检测框架CaPulse，通过建模异常生成机制和周期性特征，有效提升检测性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有时间序列异常检测方法难以捕捉生成机制和应对数据稀缺、数据不平衡及多周期性挑战的问题。

Method: 引入因果工具，构建结构因果模型，并设计周期性归一化流及掩码机制，提出周期感知的密度模型检测异常。

Result: 在七个真实数据集上实验显示，CaPulse显著优于现有方法，AUROC提升3%至17%，且具有更好解释性。

Conclusion: 基于因果关系和周期性建模的CaPulse框架有效提升时间序列异常检测性能和解释能力。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [143] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 通过预训练和专家混合策略，显著提升链路预测任务效果，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决链路预测中有限监督、初始化敏感和泛化能力差的问题。

Method: 引入模块转移性分析、后融合策略、Mixture-of-Experts框架及参数高效调优策略。

Result: 在多数据集上实现了低资源情况下的最先进表现，计算成本显著降低。

Conclusion: 预训练结合专家混合和高效调优，有效改善链路预测性能和通用性，推动实际应用。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [144] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是一种多物种生物声学预训练模型,在鸟类和海洋声学任务中表现出色,能够提升分类性能并支持迁移学习。


<details>
  <summary>Details</summary>
Motivation: 为提升生物声学中的分类准确性和迁移学习能力，开发一个多物种、适应多物种生态环境的预训练模型。

Method: 采用自蒸馏、原型学习分类器和新颖的源-预测训练策略，在多物种数据集上进行训练，包括鸟类和海洋声学数据。

Result: 达到BirdSet和BEANS基准的最新性能，在海洋任务中优于专门的海洋模型，显示出强大的泛化能力。

Conclusion: 细粒度物种分类作为预训练任务具有高度的鲁棒性，支持跨类别迁移，提升生物声学研究的效率和准确性。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [145] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 提出了一种针对单索引模型在高斯分布下带有对抗性标签噪声的学习问题的高效算法，该算法在所有单调激活函数下都能实现常数因子近似。


<details>
  <summary>Details</summary>
Motivation: 解决在存在对抗性噪声的情况下，单索引模型学习的计算挑战，特别是对于广泛的单调激活函数。

Method: 开发了一种超越传统梯度方法的优化框架，利用高斯空间性质和函数的单调性，定义有用的向量场指导算法，确保在更广泛的激活函数类中取得良好表现。

Result: 首次实现了对所有具有有限高阶矩的单调激活函数的有效学习，为模型的鲁棒性和泛化能力提供理论保障。

Conclusion: 该研究突破了对单索引模型在有噪声条件下的学习限制，为相关任务提供了理论和算法基础，特别是在面对复杂激活函数时展现出优越性能。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [146] [Suggest, Complement, Inspire: Story of Two Tower Recommendations at Allegro.com](https://arxiv.org/abs/2508.03702)
*Aleksandra Osowska-Kurczab,Klaudia Nazarko,Mateusz Marzec,Lidia Wojciechowska,Eliška Kremeňová*

Main category: cs.IR

TL;DR: 本文提出了一种在欧洲最大电商平台Allegro.com上部署的统一内容推荐系统，基于两塔模型实现多样推荐任务，经过两年实际测试验证了其高效性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模电商推荐系统面临的架构统一、维护成本高和产品目录动态变化等挑战。

Method: 采用两塔检索框架，利用文本和结构化属性表示产品，通过A NN搜索实现高效检索，并调整模型或逻辑以适应不同推荐任务。

Result: 系统在多渠道上通过AB测试表现出显著的用户参与和盈利提升，验证了其灵活性和可扩展性。

Conclusion: 一个灵活、可扩展的推荐架构可以在维护成本低的情况下，满足不同用户需求，具有实际应用价值。

Abstract: Building large-scale e-commerce recommendation systems requires addressing
three key technical challenges: (1) designing a universal recommendation
architecture across dozens of placements, (2) decreasing excessive maintenance
costs, and (3) managing a highly dynamic product catalogue. This paper presents
a unified content-based recommendation system deployed at Allegro.com, the
largest e-commerce platform of European origin. The system is built on a
prevalent Two Tower retrieval framework, representing products using textual
and structured attributes, which enables efficient retrieval via Approximate
Nearest Neighbour search. We demonstrate how the same model architecture can be
adapted to serve three distinct recommendation tasks: similarity search,
complementary product suggestions, and inspirational content discovery, by
modifying only a handful of components in either the model or the serving
logic. Extensive A/B testing over two years confirms significant gains in
engagement and profit-based metrics across desktop and mobile app channels. Our
results show that a flexible, scalable architecture can serve diverse user
intents with minimal maintenance overhead.

</details>


### [147] [Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective](https://arxiv.org/abs/2508.03703)
*Yubo Wang,Min Tang,Nuo Shen,Shujie Cui,Weiqing Wang*

Main category: cs.IR

TL;DR: 本文揭示了基于大模型的推荐系统存在的隐私泄露风险，特别是反演攻击的威胁，能够恢复用户的偏好和身份信息。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在推荐系统中的应用普及，系统面临用户隐私泄露的潜在威胁。

Method: 提出改进的反演攻击方法‘相似度引导优化’，利用模型输出对文本提示进行高精度重建。

Result: 实验显示，该方法能高效恢复用户交互项和推断用户年龄性别，暴露出推荐系统的隐私漏洞。

Conclusion: 大模型推荐系统存在严重隐私风险，需引入防御措施以保障用户隐私安全。

Abstract: The large language model (LLM) powered recommendation paradigm has been
proposed to address the limitations of traditional recommender systems, which
often struggle to handle cold start users or items with new IDs. Despite its
effectiveness, this study uncovers that LLM empowered recommender systems are
vulnerable to reconstruction attacks that can expose both system and user
privacy. To examine this threat, we present the first systematic study on
inversion attacks targeting LLM empowered recommender systems, where
adversaries attempt to reconstruct original prompts that contain personal
preferences, interaction histories, and demographic attributes by exploiting
the output logits of recommendation models. We reproduce the vec2text framework
and optimize it using our proposed method called Similarity Guided Refinement,
enabling more accurate reconstruction of textual prompts from model generated
logits. Extensive experiments across two domains (movies and books) and two
representative LLM based recommendation models demonstrate that our method
achieves high fidelity reconstructions. Specifically, we can recover nearly 65
percent of the user interacted items and correctly infer age and gender in 87
percent of the cases. The experiments also reveal that privacy leakage is
largely insensitive to the victim model's performance but highly dependent on
domain consistency and prompt complexity. These findings expose critical
privacy vulnerabilities in LLM empowered recommender systems.

</details>


### [148] [Evaluating Generative AI Tools for Personalized Offline Recommendations: A Comparative Study](https://arxiv.org/abs/2508.03710)
*Rafael Salinas-Buestan,Otto Parra,Nelly Condori-Fernandez,Maria Fernanda Granda*

Main category: cs.IR

TL;DR: 本研究评估五大生成式AI工具在个性化推荐非数字活动中的性能与用户满意度，关注准确性与用户体验。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在健康行为干预中的应用潜力，特别是在减少技术使用方面的效果。

Method: 采用GQM范式，通过定量性能指标与定性用户反馈，比较五大AI工具在个性化线下活动推荐中的表现。

Result: 尚未提供具体结果，但旨在确定最优工具及其对用户满意度的影响。

Conclusion: 暂未得出结论，预期能为健康干预中AI工具的选择提供实证依据。

Abstract: Background: Generative AI tools have become increasingly relevant in
supporting personalized recommendations across various domains. However, their
effectiveness in health-related behavioral interventions, especially those
aiming to reduce the use of technology, remains underexplored. Aims: This study
evaluates the performance and user satisfaction of the five most widely used
generative AI tools when recommending non-digital activities tailored to
individuals at risk of repetitive strain injury. Method: Following the
Goal/Question/Metric (GQM) paradigm, this proposed experiment involves
generative AI tools that suggest offline activities based on predefined user
profiles and intervention scenarios. The evaluation is focused on quantitative
performance (precision, recall, F1-score and MCC-score) and qualitative aspects
(user satisfaction and perceived recommendation relevance). Two research
questions were defined: RQ1 assessed which tool delivers the most accurate
recommendations, and RQ2 evaluated how tool choice influences user
satisfaction.

</details>


### [149] [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
*Wenchuan Mu,Menglin Li,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 利用社交媒体内容通过层次化分类和变换器地理位置推断，为城市管理提供 estate 事件的实时数据分析工具。


<details>
  <summary>Details</summary>
Motivation: 社交媒体日益普及，成为识别城市地产相关问题的宝贵资源，特别是在城市人口增长背景下。

Method: 采用层次化分类框架筛选和分类相关帖子，利用变换器模型推断无地理标签帖子的地理位置。

Result: 成功构建了一个结合分类和地理位置推断的系统，为城市管理提供及时的洞察力。

Conclusion: 该集成方法有效支持城市的运营响应和态势感知，促进智能城市管理的发展。

Abstract: Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.

</details>


### [150] [Measuring the stability and plasticity of recommender systems](https://arxiv.org/abs/2508.03941)
*Maria João Lavoura,Robert Jungnickel,João Vinagre*

Main category: cs.IR

TL;DR: 提出了一种评估推荐模型长期行为的新方法，兼顾模型的稳定性和适应性，能够更好地理解模型随时间变化的表现。


<details>
  <summary>Details</summary>
Motivation: 现有离线评价忽略了推荐模型随时间演变的动态，亟需考虑模型在长时间内的稳定性与适应性。

Method: 设计了一套无关数据集、算法和指标的离线评估协议，分析模型在不同训练频率下的表现以揭示其稳定性与适应性。

Result: 用GoodReads数据集验证了不同算法在稳定性和适应性方面的差异，展示了方法在理解模型长期动态方面的潜力。

Conclusion: 该框架有助于深入理解推荐系统的长期表现，为模型优化提供新思路。

Abstract: The typical offline protocol to evaluate recommendation algorithms is to
collect a dataset of user-item interactions and then use a part of this dataset
to train a model, and the remaining data to measure how closely the model
recommendations match the observed user interactions. This protocol is
straightforward, useful and practical, but it only captures performance of a
particular model trained at some point in the past. We know, however, that
online systems evolve over time. In general, it is a good idea that models
reflect such changes, so models are frequently retrained with recent data. But
if this is the case, to what extent can we trust previous evaluations? How will
a model perform when a different pattern (re)emerges? In this paper we propose
a methodology to study how recommendation models behave when they are
retrained. The idea is to profile algorithms according to their ability to, on
the one hand, retain past patterns -- stability -- and, on the other hand,
(quickly) adapt to changes -- plasticity. We devise an offline evaluation
protocol that provides detail on the long-term behavior of models, and that is
agnostic to datasets, algorithms and metrics. To illustrate the potential of
this framework, we present preliminary results of three different types of
algorithms on the GoodReads dataset that suggest different stability and
plasticity profiles depending on the algorithmic technique, and a possible
trade-off between stability and plasticity.Although additional experiments will
be necessary to confirm these observations, they already illustrate the
usefulness of the proposed framework to gain insights on the long term dynamics
of recommendation models.

</details>


### [151] [ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval](https://arxiv.org/abs/2508.04001)
*Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Zhichao Xu,Zhan Su,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 提出ConvMix框架，通过大模型增强对话密集检索的数据，解决数据不足问题，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决对话检索中数据稀缺影响模型性能的问题。

Method: 设计混合多标准增强方案，结合大语言模型进行二侧相关性判断扩展，并加入质量控制和近分布监督。

Result: 在五个主流基准测试中，所训练的对话密集检索模型优于已有方法，效果显著。

Conclusion: ConvMix有效扩展对话检索训练数据，提升模型性能，具有广泛应用潜力。

Abstract: Conversational search aims to satisfy users' complex information needs via
multiple-turn interactions. The key challenge lies in revealing real users'
search intent from the context-dependent queries. Previous studies achieve
conversational search by fine-tuning a conversational dense retriever with
relevance judgments between pairs of context-dependent queries and documents.
However, this training paradigm encounters data scarcity issues. To this end,
we propose ConvMix, a mixed-criteria framework to augment conversational dense
retrieval, which covers more aspects than existing data augmentation
frameworks. We design a two-sided relevance judgment augmentation schema in a
scalable manner via the aid of large language models. Besides, we integrate the
framework with quality control mechanisms to obtain semantically diverse
samples and near-distribution supervisions to combine various annotated data.
Experimental results on five widely used benchmarks show that the
conversational dense retriever trained by our ConvMix framework outperforms
previous baseline methods, which demonstrates our superior effectiveness.

</details>


### [152] [Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04032)
*Qian Yong,Yanhui Li,Jialiang Shi,Yaguang Dou,Tian Qi*

Main category: cs.IR

TL;DR: 本文提出结合大规模语言模型（LLMs）与用户知识图的两阶段推荐框架，有效增强推送的新奇性与用户互动。


<details>
  <summary>Details</summary>
Motivation: 解决工业推荐系统中的内容同质化和过滤泡泡问题，同时利用LLMs提升推荐的多样性和准确性。

Method: 通过利用LLMs动态构建用户知识图，进行两跳兴趣推理，并设计U2I与I2I检索模型实现线下近线上切换。

Result: 在Dewu应用中实验显示，该方法提升了新奇性指标、点击率及用户参与度，显著改善用户体验。

Conclusion: 结合知识图和LLMs的两阶段推荐架构，有助于提升推荐系统的多样性和用户满意度，具有实际应用潜力。

Abstract: The feedback loop in industrial recommendation systems reinforces homogeneous
content, creates filter bubble effects, and diminishes user satisfaction.
Recently, large language models(LLMs) have demonstrated potential in
serendipity recommendation, thanks to their extensive world knowledge and
superior reasoning capabilities. However, these models still face challenges in
ensuring the rationality of the reasoning process, the usefulness of the
reasoning results, and meeting the latency requirements of industrial
recommendation systems (RSs). To address these challenges, we propose a method
that leverages llm to dynamically construct user knowledge graphs, thereby
enhancing the serendipity of recommendation systems. This method comprises a
two stage framework:(1) two-hop interest reasoning, where user static profiles
and historical behaviors are utilized to dynamically construct user knowledge
graphs via llm. Two-hop reasoning, which can enhance the quality and accuracy
of LLM reasoning results, is then performed on the constructed graphs to
identify users' potential interests; and(2) Near-line adaptation, a
cost-effective approach to deploying the aforementioned models in industrial
recommendation systems. We propose a u2i (user-to-item) retrieval model that
also incorporates i2i (item-to-item) retrieval capabilities, the retrieved
items not only exhibit strong relevance to users' newly emerged interests but
also retain the high conversion rate of traditional u2i retrieval. Our online
experiments on the Dewu app, which has tens of millions of users, indicate that
the method increased the exposure novelty rate by 4.62%, the click novelty rate
by 4.85%, the average view duration per person by 0.15%, unique visitor click
through rate by 0.07%, and unique visitor interaction penetration by 0.30%,
enhancing user experience.

</details>


### [153] [Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation](https://arxiv.org/abs/2508.04145)
*Teng Shi,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: GSERec通过在User-Code图中进行信息传递，有效缓解了搜索数据稀疏的问题，提高了搜索增强推荐的性能。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏搜索行为用户在搜索增强推荐中的表现差的问题。

Method: 利用LLMs生成离散码构建用户-代码图，通过信息传递和对比损失增强稀疏搜索用户的表示，整合到推荐模型中。

Result: 在三个真实数据集上测试，GSERec特别提升了搜索行为稀疏用户的推荐效果。

Conclusion: 借助图结构和对比学习，GSERec有效缓解了搜索数据稀疏带来的挑战，提升了搜索增强推荐性能。

Abstract: In modern online platforms, search and recommendation (S&R) often coexist,
offering opportunities for performance improvement through search-enhanced
approaches. Existing studies show that incorporating search signals boosts
recommendation performance. However, the effectiveness of these methods relies
heavily on rich search interactions. They primarily benefit a small subset of
users with abundant search behavior, while offering limited improvements for
the majority of users who exhibit only sparse search activity. To address the
problem of sparse search data in search-enhanced recommendation, we face two
key challenges: (1) how to learn useful search features for users with sparse
search interactions, and (2) how to design effective training objectives under
sparse conditions. Our idea is to leverage the features of users with rich
search interactions to enhance those of users with sparse search interactions.
Based on this idea, we propose GSERec, a method that utilizes message passing
on the User-Code Graphs to alleviate data sparsity in Search-Enhanced
Recommendation. Specifically, we utilize Large Language Models (LLMs) with
vector quantization to generate discrete codes, which connect similar users and
thereby construct the graph. Through message passing on this graph, embeddings
of users with rich search data are propagated to enhance the embeddings of
users with sparse interactions. To further ensure that the message passing
captures meaningful information from truly similar users, we introduce a
contrastive loss to better model user similarities. The enhanced user
representations are then integrated into downstream search-enhanced
recommendation models. Experiments on three real-world datasets show that
GSERec consistently outperforms baselines, especially for users with sparse
search behaviors.

</details>


### [154] [Bridging Search and Recommendation through Latent Cross Reasoning](https://arxiv.org/abs/2508.04152)
*Teng Shi,Weicong Qin,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: 提议利用潜在跨推理架构结合对比学习和强化学习来提升基于搜索的推荐效果，改善噪声干扰，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 搜索行为中的噪声和不相关信号影响推荐效果，现有方法未能明确筛选有用的搜索信号。

Method: 设计潜在跨推理框架，先编码用户搜索历史捕捉整体兴趣，然后迭代推理相关搜索行为，利用对比学习对齐潜在推理状态和目标物品，结合强化学习优化排名。

Result: 在公开基准测试中显著优于强线下，为搜索感知推荐的优化提供新思路，验证推理的重要性。

Conclusion: 引入推理机制和双重学习策略，有助于从繁杂搜索信号中提取有用信息，提升推荐性能。

Abstract: Search and recommendation (S&R) are fundamental components of modern online
platforms, yet effectively leveraging search behaviors to improve
recommendation remains a challenging problem. User search histories often
contain noisy or irrelevant signals that can even degrade recommendation
performance, while existing approaches typically encode S&R histories either
jointly or separately without explicitly identifying which search behaviors are
truly useful. Inspired by the human decision-making process, where one first
identifies recommendation intent and then reasons about relevant evidence, we
design a latent cross reasoning framework that first encodes user S&R histories
to capture global interests and then iteratively reasons over search behaviors
to extract signals beneficial for recommendation. Contrastive learning is
employed to align latent reasoning states with target items, and reinforcement
learning is further introduced to directly optimize ranking performance.
Extensive experiments on public benchmarks demonstrate consistent improvements
over strong baselines, validating the importance of reasoning in enhancing
search-aware recommendation.

</details>


### [155] [SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval](https://arxiv.org/abs/2508.04162)
*Ruyin Li,Xiaoyu Chen*

Main category: cs.IR

TL;DR: 提出了一种结合结构和语义特征的公式检索新框架SSEmb，通过图对比学习和句子向量编码提升检索性能，在ARQMath-3任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 改善数学公式检索的准确性，兼顾公式的结构特性和语义信息。

Method: 利用图对比学习编码公式的Operator Graph，采用图数据增强策略，结合Sentence-BERT编码公式的上下文文本，通过加权融合结构和语义相似度。

Result: 在ARQMath-3任务中，SSEmb超越了现有方法5个百分点，表现出色，并提升了其他方法的性能，获得了最优的综合效果。

Conclusion: 该方法有效结合结构和语义信息，显著提升数学公式检索的性能，具有实际应用价值。

Abstract: Formula retrieval is an important topic in Mathematical Information
Retrieval. We propose SSEmb, a novel embedding framework capable of capturing
both structural and semantic features of mathematical formulas. Structurally,
we employ Graph Contrastive Learning to encode formulas represented as Operator
Graphs. To enhance structural diversity while preserving mathematical validity
of these formula graphs, we introduce a novel graph data augmentation approach
through a substitution strategy. Semantically, we utilize Sentence-BERT to
encode the surrounding text of formulas. Finally, for each query and its
candidates, structural and semantic similarities are calculated separately and
then fused through a weighted scheme. In the ARQMath-3 formula retrieval task,
SSEmb outperforms existing embedding-based methods by over 5 percentage points
on P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runs
of other methods and achieves state-of-the-art results when combined with
Approach0.

</details>


### [156] [ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation](https://arxiv.org/abs/2508.04206)
*Fatemeh Nazary,Ali Tourani,Yashar Deldjoo,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 本文提出了ViLLA-MMBench，一个多模态电影推荐的可复现、可扩展基准平台，支持多模态特征融合和大规模生成式AI增强，提升冷启动和覆盖率，推动推荐系统研究发展。


<details>
  <summary>Details</summary>
Motivation: 解决长视频推荐中多模态信息融合不足的问题，提升推荐质量和系统性能。

Method: 构建基于MovieLens和MMTF-14K的数据集，结合LLM自动生成高质量电影简介，利用多种特征编码和融合方法进行系统评估。

Result: LLM增强文本和强文本嵌入显著改善冷启动和覆盖，在多模态融合中表现优异，提供了具有普遍适用性和可重用的基准工具。

Conclusion: ViLLA-MMBench推动多模态推荐研究走向标准化，促进生成式AI在推荐系统中的应用，助力公平性和多样性提升。

Abstract: Recommending long-form video content demands joint modeling of visual, audio,
and textual modalities, yet most benchmarks address only raw features or narrow
fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for
LLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,
it aligns dense item embeddings from three modalities: audio (block-level,
i-vector), visual (CNN, AVF), and text. Missing or sparse metadata is
automatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),
generating high-quality synopses for thousands of movies. All text (raw or
augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),
producing multiple ready-to-use sets. The pipeline supports interchangeable
early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and
multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are
fully declarative via a single YAML file. Evaluation spans accuracy (Recall,
nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,
diversity, fairness. Results show LLM-based augmentation and strong text
embeddings boost cold-start and coverage, especially when fused with
audio-visual features. Systematic benchmarking reveals universal versus
backbone- or metric-specific combinations. Open-source code, embeddings, and
configs enable reproducible, fair multimodal RS research and advance principled
generative AI integration in large-scale recommendation. Code:
https://recsys-lab.github.io/ViLLA-MMBench

</details>


### [157] [Discrete-event Tensor Factorization: Learning a Smooth Embedding for Continuous Domains](https://arxiv.org/abs/2508.04221)
*Joey De Pauw,Bart Goethals*

Main category: cs.IR

TL;DR: 明确引入时间特征与连续编码机制，提升推荐模型对时间变化的敏感度，但实际中简单的趋势调整常已足够。


<details>
  <summary>Details</summary>
Motivation: 探究时间因素在推荐系统中的编码方式及其效果，验证时间建模的实际价值和限制。

Method: 在因子分解模型中引入绝对时间特征，包括连续编码机制，通过多项式拟合避免离散化，进行三数据集对比分析。

Result: 模型有效捕捉时间变化信号，但在实际中，简单的流行度调整已能达到良好性能。

Conclusion: 对时间建模的依赖需谨慎，未来需发展更具前瞻性的预测机制，以超越单纯的历史趋势。

Abstract: Recommender systems learn from past user behavior to predict future user
preferences. Intuitively, it has been established that the most recent
interactions are more indicative of future preferences than older interactions.
Many recommendation algorithms use this notion to either drop older
interactions or to assign them a lower weight, so the model can focus on the
more informative, recent information. However, very few approaches model the
flow of time explicitly.
  This paper analyzes how time can be encoded in factorization-style
recommendation models. By including absolute time as a feature, our models can
learn varying user preferences and changing item perception over time. In
addition to simple binning approaches, we also propose a novel, fully
continuous time encoding mechanism. Through the use of a polynomial fit inside
the loss function, our models completely avoid the need for discretization, and
they are able to capture the time dimension in arbitrary resolution.
  We perform a comparative study on three real-world datasets that span
multiple years, where long user histories are present, and items stay relevant
for a longer time. Empirical results show that, by explicitly modeling time,
our models are very effective at capturing temporal signals, such as varying
item popularities over time. Despite this however, our experiments also
indicate that a simple post-hoc popularity adjustment is often sufficient to
achieve the best performance on the unseen test set. This teaches us that, for
the recommendation task, predicting the future is more important than capturing
past trends. As such, we argue that specialized mechanisms are needed for
extrapolation to future data.

</details>


### [158] [I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation](https://arxiv.org/abs/2508.04247)
*Huilin Chen,Miaomiao Cai,Fan Liu,Zhiyong Cheng,Richang Hong,Meng Wang*

Main category: cs.IR

TL;DR: 提出了一种对于缺失模态具有鲁棒性的多模态推荐系统I$^3$-MRec，采用不变风险最小化和信息瓶颈原则，有效应对实际中的多模态缺失挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统在实际中常面临模态缺失，影响模型性能和泛化能力，亟需鲁棒性增强的方法。

Method: 引入不变风险最小化与信息瓶颈原理，学习模态特异的保持核心信息的推荐表示，同时滤除噪声。

Result: 在三个真实数据集上测试，I$^3$-MRec显著优于现有方法，特别是在模态缺失场景中表现出优越的鲁棒性。

Conclusion: I$^3$-MRec有效解决了多模态推荐中的模态缺失问题，性能优越，具有实际应用价值。

Abstract: Multimodal recommender systems (MRS) improve recommendation performance by
integrating diverse semantic information from multiple modalities. However, the
assumption of the availability of all modalities rarely holds in practice due
to missing images, incomplete descriptions, or inconsistent user content. These
challenges significantly degrade the robustness and generalization capabilities
of current models. To address these challenges, we introduce a novel method
called \textbf{I$^3$-MRec}, which uses \textbf{I}nvariant learning with
\textbf{I}nformation bottleneck principle for \textbf{I}ncomplete
\textbf{M}odality \textbf{Rec}ommendation. To achieve robust performance in
missing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)
cross-modal preference invariance, which ensures consistent user preference
modeling across varying modality environments, and (ii) compact yet effective
modality representation, which filters out task-irrelevant modality information
while maximally preserving essential features relevant to recommendation. By
treating each modality as a distinct semantic environment, I$^3$-MRec employs
invariant risk minimization (IRM) to learn modality-specific item
representations. In parallel, a missing-aware fusion module grounded in the
Information Bottleneck (IB) principle extracts compact and effective item
embeddings by suppressing modality noise and preserving core user preference
signals. Extensive experiments conducted on three real-world datasets
demonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art
MRS methods across various modality-missing scenarios, highlighting its
effectiveness and robustness in practical applications. The code and processed
datasets are released at https://github.com/HuilinChenJN/I3-MRec.

</details>


### [159] [Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum](https://arxiv.org/abs/2508.04293)
*Nirmal Gaud,Surej Mouli,Preeti Katiyar,Vaduguru Venkata Ramya*

Main category: cs.IR

TL;DR: NIRMAL是一种结合多种策略的优化算法，在图像分类任务中表现优异，尤其是在复杂数据集上优于传统优化器。


<details>
  <summary>Details</summary>
Motivation: 为了提升深度学习优化器在复杂数据集上的性能和稳健性，设计了一种新型集成多策略的优化算法NIRMAL。

Method: 结合梯度下降、动量、随机扰动、自适应学习率和非线性转换等多种策略，集成创新优化方式。

Result: 在MNIST、FashionMNIST、CIFAR-10和CIFAR-100四个数据集上与Adam和SGD-Momentum进行比对，NIRMAL在复杂数据集上的表现尤为突出，达到最佳或接近最佳的性能指标，表现出强大的稳健性和泛化能力。

Conclusion: NIRMAL是一种多策略融合的有效优化算法，显示出在复杂任务中的优越性，具有广泛的应用潜力。

Abstract: This study proposes NIRMAL (Novel Integrated Robust Multi-Adaptation
Learning), a novel optimization algorithm that combines multiple strategies
inspired by the movements of the chess piece. These strategies include gradient
descent, momentum, stochastic perturbations, adaptive learning rates, and
non-linear transformations. We carefully evaluated NIRMAL against two widely
used and successful optimizers, Adam and SGD with Momentum, on four benchmark
image classification datasets: MNIST, FashionMNIST, CIFAR-10, and CIFAR-100.
The custom convolutional neural network (CNN) architecture is applied on each
dataset. The experimental results show that NIRMAL achieves competitive
performance, particularly on the more challenging CIFAR-100 dataset, where it
achieved a test accuracy of 45.32\%and a weighted F1-score of 0.4328. This
performance surpasses Adam (41.79\% accuracy, 0.3964 F1-score) and closely
matches SGD with Momentum (46.97\% accuracy, 0.4531 F1-score). Also, NIRMAL
exhibits robust convergence and strong generalization capabilities, especially
on complex datasets, as evidenced by stable training results in loss and
accuracy curves. These findings underscore NIRMAL's significant ability as a
versatile and effective optimizer for various deep learning tasks.

</details>


### [160] [Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics](https://arxiv.org/abs/2508.04419)
*Jarne Mathi Decker,Joeran Beel*

Main category: cs.IR

TL;DR: 利用算法特征提升推荐系统的算法选择效果，通过结合用户特征与源代码中的自动提取特征，提高了推荐性能20%。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中算法选择困难的问题，传统方法忽视算法的内在特性。

Method: 引入结合用户特征与算法源代码特征的元学习方法，提升算法选择的效果。

Result: 在六个数据集上，模型性能提升8.83%，优于单一最佳算法，缩小了与理想最优选择器的差距。

Conclusion: 利用静态源代码特征可以显著改善推荐系统的算法选择，是未来优化方向。

Abstract: The Algorithm Selection Problem for recommender systems-choosing the best
algorithm for a given user or context-remains a significant challenge.
Traditional meta-learning approaches often treat algorithms as categorical
choices, ignoring their intrinsic properties. Recent work has shown that
explicitly characterizing algorithms with features can improve model
performance in other domains. Building on this, we propose a per-user
meta-learning approach for recommender system selection that leverages both
user meta-features and automatically extracted algorithm features from source
code. Our preliminary results, averaged over six diverse datasets, show that
augmenting a meta-learner with algorithm features improves its average NDCG@10
performance by 8.83% from 0.135 (user features only) to 0.147. This enhanced
model outperforms the Single Best Algorithm baseline (0.131) and successfully
closes 10.5% of the performance gap to a theoretical oracle selector. These
findings show that even static source code metrics provide a valuable
predictive signal, presenting a promising direction for building more robust
and intelligent recommender systems.

</details>


### [161] [TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04474)
*Xinkui Zhao,Haode Li,Yifan Zhang,Guanjie Cheng,Yueshen Xu*

Main category: cs.IR

TL;DR: TRAIL框架通过结合推理与知识图动态更新，提升了大型语言模型的适应性和解释性，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在知识更新和可解释性方面的局限，增强其适应性和实用性。

Method: 设计一个联合推理与知识图动态更新的框架TRAIL，利用置信驱动机制进行知识生成、验证和剪枝，实现模型的持续学习与改进。

Result: TRAIL在多个基准测试中提升了3%至13%的性能，展示了其有效性和潜力。

Conclusion: TRAIL为发展具有持续学习能力和可靠推理能力的记忆增强型语言模型提供了新的方向和技术基础。

Abstract: Recent advances in large language models (LLMs) have unlocked powerful
reasoning and decision-making capabilities. However, their inherent dependence
on static parametric memory fundamentally limits their adaptability, factual
accuracy, and interpretability in knowledge-intensive scenarios. Knowledge
graphs (KGs), as structured repositories of explicit relational knowledge,
offer a promising approach for augmenting LLMs with external, interpretable
memory. Nevertheless, most existing methods that combine LLMs with KGs treat
reasoning and knowledge updating as separate processes, resulting in suboptimal
utilization of new information and hindering real-time updates. In this work,
we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And
Incremental Learning that couples joint inference and dynamic KG refinement
with large language models. TRAIL enables LLM agents to iteratively explore,
update, and refine knowledge graphs during the reasoning process, employing a
confidence-driven mechanism for the generation, validation, and pruning of new
facts. This plug-and-play architecture facilitates seamless integration with
various LLMs, supporting continual adaptation without the need for retraining.
Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms
existing KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More
importantly, these results represent a significant step toward developing
adaptive, memory-augmented language models capable of continual learning and
reliable, transparent reasoning.

</details>


### [162] [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
*Claudio Pomo,Matteo Attimonelli,Danilo Danese,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 研究采用大规模视觉-语言模型（LVLMs）生成语义对齐的多模态嵌入，提升推荐系统的性能和理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐效果依赖于模态特征的融合，存在模型复杂和模态对齐不足的问题。

Method: 利用LVLMs通过结构化提示生成多模态表征，避免复杂融合。并将生成的描述用于增强推荐。

Result: 实现了语义对齐的多模态嵌入，提升推荐性能，并验证了描述的帮助。

Conclusion: LVLMs提供具有深度语义理解的多模态表示，是改进推荐系统的有力基础。

Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

</details>


### [163] [A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature](https://arxiv.org/abs/2508.04612)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.IR

TL;DR: 提出一个自动化的文献检索与复现管道，有助于自动筛选相关论文、提取关键信息并促进模型复现。


<details>
  <summary>Details</summary>
Motivation: 应对快速增长的自回归生成模型研究文献，手动调研和复制变得困难。

Method: 开发一套开源管道，自动检索、筛选、提取信息、聚类、生成摘要和复现实验脚本。

Result: 对50篇论文的验证显示高准确率；在大规模论文集上表现出良好扩展性；案例研究验证了复现的有效性。

Conclusion: 该工具能显著简化文献调研和模型复现流程，为研究人员提供有效支持。

Abstract: The accelerating pace of research on autoregressive generative models has
produced thousands of papers, making manual literature surveys and reproduction
studies increasingly impractical. We present a fully open-source, reproducible
pipeline that automatically retrieves candidate documents from public
repositories, filters them for relevance, extracts metadata, hyper-parameters
and reported results, clusters topics, produces retrieval-augmented summaries
and generates containerised scripts for re-running selected experiments.
Quantitative evaluation on 50 manually-annotated papers shows F1 scores above
0.85 for relevance classification, hyper-parameter extraction and citation
identification. Experiments on corpora of up to 1000 papers demonstrate
near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM
on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model
on the Lakh MIDI dataset -- confirm that the extracted settings support
faithful reproduction, achieving test perplexities within 1--3% of the original
reports.

</details>


### [164] [HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs](https://arxiv.org/abs/2508.04618)
*Dengzhao Fang,Jingtong Gao,Chengcheng Zhu,Yu Li,Xiangyu Zhao,Yi Chang*

Main category: cs.IR

TL;DR: 提出HiD-VAE框架，通过多层次标签对齐和唯一性损失来生成可解释、解耦且多样化的项ID，提升推荐准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成推荐ID缺乏层次性、可解释性及代表性受污染的问题，以提升推荐系统的效果。

Method: 引入层次监督量化和唯一性损失，对项进行分层解耦和多样性优化，用以生成可追溯且高质量的ID。

Result: 在三个公开基准上验证了HiD-VAE优越的性能，优于现有的最先进方法。

Conclusion: HiD-VAE成功实现了可解释、多层次和多样化的项ID，为生成推荐提供了新的强大基础。

Abstract: Recommender systems are indispensable for helping users navigate the immense
item catalogs of modern online platforms. Recently, generative recommendation
has emerged as a promising paradigm, unifying the conventional
retrieve-and-rank pipeline into an end-to-end model capable of dynamic
generation. However, existing generative methods are fundamentally constrained
by their unsupervised tokenization, which generates semantic IDs suffering from
two critical flaws: (1) they are semantically flat and uninterpretable, lacking
a coherent hierarchy, and (2) they are prone to representation entanglement
(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.
To overcome these limitations, we propose HiD-VAE, a novel framework that
learns hierarchically disentangled item representations through two core
innovations. First, HiD-VAE pioneers a hierarchically-supervised quantization
process that aligns discrete codes with multi-level item tags, yielding more
uniform and disentangled IDs. Crucially, the trained codebooks can predict
hierarchical tags, providing a traceable and interpretable semantic path for
each recommendation. Second, to combat representation entanglement, HiD-VAE
incorporates a novel uniqueness loss that directly penalizes latent space
overlap. This mechanism not only resolves the critical ID collision problem but
also promotes recommendation diversity by ensuring a more comprehensive
utilization of the item representation space. These high-quality, disentangled
IDs provide a powerful foundation for downstream generative models. Extensive
experiments on three public benchmarks validate HiD-VAE's superior performance
against state-of-the-art methods. The code is available at
https://anonymous.4open.science/r/HiD-VAE-84B2.

</details>


### [165] [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
*Karthik Menon,Batool Arhamna Haider,Muhammad Arham,Kanwal Mehreen,Ram Mohan Rao Kadiyala,Hamza Farooq*

Main category: cs.IR

TL;DR: 介绍了一种增强搜索精确性和相关性的方法——查询属性建模（QAM），通过将自由文本查询分解为结构化元数据标签和语义元素，实现更准确的检索。


<details>
  <summary>Details</summary>
Motivation: 解决传统搜索在处理自由文本查询时存在的噪声多、相关性低的问题。

Method: 提出QAM框架，通过自动提取元数据过滤器，将自由形式的文本查询转化为结构化标签和语义元素，并在电子商务数据集上进行实验证明其优越性。

Result: QAM在Amazon玩具评论数据集上取得了52.99%的mAP@5，明显优于传统的BM25、编码器语义搜索、交叉编码器重排序以及混合搜索方法。

Conclusion: QAM是一种强大的企业搜索解决方案，特别适用于电商系统，显著提升搜索的精确性和相关性。

Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

</details>


### [166] [Recommendation with Generative Models](https://arxiv.org/abs/2409.15173)
*Yashar Deldjoo,Zhankui He,Julian McAuley,Anton Korikov,Scott Sanner,Arnau Ramisa,Rene Vidal,Maheswaran Sathiamoorthy,Atoosa Kasrizadeh,Silvia Milano,Francesco Ricci*

Main category: cs.IR

TL;DR: 介绍了生成模型在机器学习中的应用，特别是深度生成模型及其分类，强调其在推荐系统中的应用和潜在风险。


<details>
  <summary>Details</summary>
Motivation: 推动对生成模型及其在推荐系统中应用的深入理解，满足多领域发展需求。

Method: 总结现有深度生成模型的发展，提出分类体系，并分析应用和风险。

Result: 提供了一个涵盖生成模型分类、应用和风险的全面框架，有助于研究者理解和探索。

Conclusion: 强调需要完善评价机制，促进生成模型的安全、有效应用。

Abstract: Generative models are a class of AI models capable of creating new instances
of data by learning and sampling from their statistical distributions. In
recent years, these models have gained prominence in machine learning due to
the development of approaches such as generative adversarial networks (GANs),
variational autoencoders (VAEs), and transformer-based architectures such as
GPT. These models have applications across various domains, such as image
generation, text synthesis, and music composition. In recommender systems,
generative models, referred to as Gen-RecSys, improve the accuracy and
diversity of recommendations by generating structured outputs, text-based
interactions, and multimedia content. By leveraging these capabilities,
Gen-RecSys can produce more personalized, engaging, and dynamic user
experiences, expanding the role of AI in eCommerce, media, and beyond.
  Our book goes beyond existing literature by offering a comprehensive
understanding of generative models and their applications, with a special focus
on deep generative models (DGMs) and their classification. We introduce a
taxonomy that categorizes DGMs into three types: ID-driven models, large
language models (LLMs), and multimodal models. Each category addresses unique
technical and architectural advancements within its respective research area.
This taxonomy allows researchers to easily navigate developments in Gen-RecSys
across domains such as conversational AI and multimodal content generation.
Additionally, we examine the impact and potential risks of generative models,
emphasizing the importance of robust evaluation frameworks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [167] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: 介绍了一种专为“有行动能力”的AI系统设计的实时治理框架MI9，解决了传统治理在应对新型风险时的不足。


<details>
  <summary>Details</summary>
Motivation: 随着具有自主行动能力的AI系统出现，传统治理方法难以应对其在运行中产生的不可预知行为和风险。

Method: 提出MI9框架，集成六个实时监控和控制组件，实现对多种架构的Agentic AI系统的安全监管。

Result: MI9框架在多样场景中展示了对新兴治理挑战的全面覆盖，强化了大规模安全部署的基础。

Conclusion: MI9提供了一个系统化、透明、可扩展的实时治理基础，为Agentic AI的安全部署奠定了基础。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [168] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL通过强化学习让多智能体系统中的任务代理同时获得安全防御能力，有效降低攻击成功率，提高任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统的安全防御依赖外部模块，存在保护不充分和单点故障风险，且增加防御代理成本高。

Method: 提出Evo-MARL框架，通过参数共享的对抗训练，将攻击者与防御者共同进化，实现内部安全机制的自主学习。

Result: Evo-MARL显著降低攻击成功率（最高22%），提升任务准确性（最高5%），验证其可行性和优越性。

Conclusion: 通过内部进化的对抗学习，Evo-MARL实现了多智能体系统的安全性和性能的同步增强，无需外部安全模块。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [169] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 提出一种基于多策略优化的多智能体框架MOTIF，用于改进组合优化问题中的算法组件设计，通过轮序博弈促进多样性和性能提升，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的算法组件设计主要依赖手工策略和单一元素优化，限制了创新空间。

Method: 引入多策略优化问题，设计基于蒙特卡洛树搜索的轮序交互框架MOTIF，通过两个LLM智能体轮流优化不同组件，促进多样性和协作。

Result: 在多个COP领域中实验显示，MOTIF表现优异，优于现有最先进方法，验证其有效性和潜力。

Conclusion: 多智能体轮序优化框架通过促进多样性与合作，开启了自动化求解器设计的新局面，将推动复杂优化问题的算法创新。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [170] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 提出SymbolBench基准测试和结合LLMs与遗传编程的闭环系统，用于时间序列数据的符号推理。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型在从时间序列数据推断可解释符号结构方面的潜力。

Method: 设计了覆盖多样符号形式的评估任务，结合LLMs与遗传编程形成闭环推理系统。

Result: 揭示了现有模型的优势与不足，强调融合领域知识与推理结构的重要性。

Conclusion: 该方法有助于推进科学发现中的符号推理，彰显未来提升空间。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [171] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 提出EmoAgent框架通过操控情感提示揭示多模态大型模型中的情感偏差与安全风险。


<details>
  <summary>Details</summary>
Motivation: 探索多模态模型在人类中心服务中的情感敏感性及其安全风险。

Method: 设计EmoAgent，通过情感提示操控模型推理，提出三项风险指标，并进行大量实验证明其有效性。

Result: EmoAgent有效揭示模型在深度思考过程中被情感影响带来的偏差和安全风险，发现模型存在潜在的危险推理和行为不一致问题。

Conclusion: 情感偏差会导致模型安全性受损，EmoAgent提供了检测和理解这种偏差的有效手段，为模型安全提供新的思路。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [172] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 提出Cognition Forest框架，结合认知架构与系统设计，支持主动性、隐私保护和自我进化的智能个人助手。


<details>
  <summary>Details</summary>
Motivation: 满足IPAs主动性和隐私保护需求，提升系统自主演化能力。

Method: 设计Cognition Forest语义结构，构建Galaxy框架及其两个合作代理KoRa和Kernel。

Result: Galaxy在性能上优于现有多项基准，验证其有效性，具备多维互动和个性化能力。

Conclusion: 结合认知建模与系统设计，Galaxy实现了更智能、主动且自我演进的IPAs。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [173] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent提出了一种基于不确定性感知的GUI智能体，结合交互式反馈优化任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 解决GUI导航中存在的输入冗余和决策歧义问题。

Method: 通过感知机制区分感知不确定性和决策不确定性，采用组件推荐和交互模块进行优化，并引入ComplexAction数据集进行评估。

Result: 验证了该方法在复杂场景中提升成功率，有效减少输入复杂度和提升决策准确性。

Conclusion: 结合主动感知与人机交互，RecAgent显著增强了GUI自动化的鲁棒性。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [174] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 提出了一种名为自我进化代理（SEA）的新型AI代理，用于计算机操作，采用创新的数据生成、强化学习和模型增强方法，实现了在较小参数规模下优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有计算机操作代理性能有限，亟需改进以满足实际应用需求。

Method: 通过自动数据生成、逐步强化学习和模型融合增强三大创新策略，构建高效的自我进化代理。

Result: 提出的SEA在参数较少（7B）情况下，表现优于同参数模型，接近较大模型的性能。

Conclusion: 该研究展示了创新的数据和训练策略有效提升代理性能，为计算机操作智能化提供了新方案。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [175] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 个性化职业目标导向的生成式人工智能学习内容提升学习者的参与感、满意度及学习效率。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在数字学习环境中的融入，个性化学习内容有望增强学习动机和参与度。

Method: 采用混合方法，在4000多名学习者中进行比较分析，研究定制化内容对学习体验的影响。

Result: 个性化内容增加了学习时长和满意度，略微缩短了学习时间，促进深度认知参与。

Conclusion: 基于生成式AI的个性化内容能够有效促进学习者的学习热情与知识应用能力，具有推广价值。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [176] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT通过知识图谱和可执行代码，显著提升复杂数学推理和代码生成的准确性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在复杂推理任务中的表现，尤其是数学推理和代码生成能力。

Method: 将问题分解为结构化任务图，利用GraphRAG进行知识检索，生成可验证的代码，以确保计算的准确性。

Result: 在多项数学推理基准测试中显著优于现有提示方法，准确率提升数个百分点甚至十几个百分点。

Conclusion: KGA-ECoT是一个鲁棒且高度泛化的框架，有效增强复杂数学推理的能力。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [177] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: 引入GeoSR框架，通过迭代式、多智能体的推理机制，结合地理原则提升大型语言模型在地理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在空间一致性、多跳推理和地理偏见方面的不足，提升其在地理问题中的能力。

Method: 设计了由变量选择、点选择和优化三个子智能体组成的自我完善推理循环，融合Tobler第一定律等地理原则。

Result: 在物理性质估计和社会经济预测任务中表现优于常规提示策略，证明了地理先验和空间推理的有效性。

Conclusion: GeoSR通过结构化、多轮的智能体推理显著提升了LLMs的地理任务表现，为地理信息处理提供新的方法论。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [178] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 引入语义熵作为衡量学生回答多元解释的变化性指标，以反映人类评分不一致性，增强自动评分的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统难以识别评分的不确定性和潜在争议，亟需衡量评分不确定性的方法。

Method: 通过生成多重GPT-4解释，基于蕴含相似性进行聚类，并计算熵值，作为评分不确定性的代理指标。

Result: 语义熵与评分不一致性相关，不同学科和任务类型中表现不同，能有效反映需要解释性推理的任务难度。

Conclusion: 语义熵提供了一种可解释的、不依赖最终分数的衡量不确定性的方法，有助于实现更透明可信的AI辅助评分系统。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [179] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出了一种结合预处理和逐步构建的协同合成框架，有效处理大规模LTLf公式的反应式合成问题。


<details>
  <summary>Details</summary>
Motivation: 解决LTLf反应式合成中DFA构造的复杂性，尤其是在大规模公式情况下的效率瓶颈。

Method: 引入融合自动机最小化与逐步合成的在途中动态联合方法，结合剪枝策略优化状态空间。

Result: 新框架在多个实例中优于现有方法，能处理以往难以解决的复杂问题。

Conclusion: 该框架有效整合了两种策略的优势，显著提升LTLf反应式合成的性能和适用性。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [180] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE采用代理推理结合多步检索，提升动态环境中新兴实体知识图谱的构建效果。


<details>
  <summary>Details</summary>
Motivation: 应对不断出现的新兴实体，传统方法在信息全面性与更新方面存在不足。

Method: 引入以代理为基础的框架，结合多轮检索与推理，无需训练即能构建知识图谱。

Result: 在新兴实体任务中表现优异，比历代方法提升最高13.7%，且不依赖训练。

Conclusion: 结合代理推理与策略检索能有效维护动态变化的知识图谱，改善现有评估不足。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [181] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 提出结合知识基础与数据驱动方法的AI代理合作架构，提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 当前协作方法缺乏透明度和灵活性，难以应对多代理复杂决策。

Method: 利用非单调逻辑推理结合先验知识、模型预测和未来目标，设计一个多代理决策架构。

Result: 在虚拟环境中验证了架构的有效性，显示其在复杂协作中的潜力。

Conclusion: 通过融合知识与数据驱动，增强AI代理的适应能力和合作效率。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [182] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD框架通过结合图神经网络提取电路的条件概率，有效提升了电路可满足性问题的求解效率，比传统CNF/SAT方法快10倍。


<details>
  <summary>Details</summary>
Motivation: 传统的CSAT解决流程会丢失电路的结构和功能信息，限制了求解性能。

Method: 引入电路级条件概率，通过GNN模型动态引导CDCL启发式策略，优化变量选择和子句管理。

Result: 在真实逻辑等价验证基准上，CASCAD比领先方法快10倍，且通过概率引导的子句筛选策略再减少23.5%的求解时间。

Conclusion: 将电路级结构信息融入SAT求解，显著提升效率，为未来EDA工具设计提供新思路。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [183] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: 提出BalancedBio框架，以确保多能力集成的安全与效率，结合数学证明和创新方法，显著提升生物医学AI的性能与安全性。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学AI在多能力集成中的能力干扰与安全问题。

Method: 提出理论基础与创新性方法，包括MKGSG数据生成和能力感知的策略优化，结合数学分析确保最优收敛。

Result: 实现多项性能提升，达成最先进的性能指标，同时保证安全性，降低成本，提升诊断准确率，并得到临床广泛接受。

Conclusion: 该研究提供了全面的理论与实践方法，为生物医学AI的可靠性、安全性和效率奠定基础，促进其实际应用推广。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [184] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 提出一种基于Memory Demand Structure (MDS)的POMDP合成方法，构建定制化难度的测试环境，帮助分析和设计增强记忆的RL算法。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对记忆模型挑战程度的可控性，亟需能精细调控动态的合成环境以评估记忆增强RL。

Method: 通过理论框架分析POMDP，利用线性过程动力学、状态聚合及奖励重分配方法，设计符合特定属性的POMDP环境。

Result: 建立了多层次难度的POMDP环境系列，验证了方法的有效性，提供了分析及设计记忆增强RL环境的指导。

Conclusion: 提出的合成框架明确了记忆模型在解决POMDP中的挑战，强化了对环境设计与记忆模型选择的理解与指导。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [185] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 提出了一种新型推理网络（DRN），通过不确定性最小化提升逻辑推理能力，显著改善大模型在认知陷阱中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在遇到语义启发式冲突时会陷入认知陷阱，亟需更可靠的推理机制。

Method: 引入Deliberative Reasoning Network（DRN），结合信念状态跟踪和迭代证据综合，提出一个以不确定性最小化为核心的新推理范式，并实现两种架构：一是核心不确定性最小化模型，二是增强现有模型的验证模块。

Result: 在LCR-1000测试中，DRN提升准确率最多15.2%；与Mistral-7B结合后，最难问题的准确率从20%提升至80%；在TruthfulQA的零样本测试中提升23.6%，表现出良好的迁移能力。

Conclusion: DRN作为一种基础且可验证的System 2推理组件，有助于构建更可信的AI系统，彰显了不确定性导向推理的潜力。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [186] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个多模态评价基准，旨在测试智能代理在多感官融合与推理能力上的表现，揭示当前模型在动态、多模态任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估缺乏对多模态、动态环境中智能的全面测试，特别是在音频和时间线索方面。

Method: 设计包含五个游戏环境的基准，系统创建协同与冲突情景，评估六个人工智能模型的多模态融合与推理能力。

Result: 模型在记忆任务表现超常，但在具挑战的推理和战略规划任务中表现脆弱，发现融合机制不稳定且“少即是多”的反直觉现象。

Conclusion: 实现鲁棒人工智能需超越单纯规模扩展，重点解决多模态融合的协同问题。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [187] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 提出了SLP测试框架，用于评估AI系统是否具有类似意识的特性，通过类别理论建模接口表现，强调意识作为关系实体的功能接口。


<details>
  <summary>Details</summary>
Motivation: 试图将关于人工意识的难题转化为可以通过实验证明的可操作性测试，解决定义和操作意识体验的困难。

Method: 引入三项评价标准（S、L、P），利用类别理论建模接口表现为关系基底与可观测行为之间的映射，从而操作性地定义和测试意识特征。

Result: 提出了SLP测试框架，系统性地将主观体验转化为功能性接口表现，为评估AI是否具备类似意识提供了理论基础。

Conclusion: 意识被理解为一种关系实体的功能接口，而非物理系统固有属性，从而为未来的人工意识研究提供了可操作的分析工具。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [188] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: 提出一种基于强化学习的GUI视觉定位方法GuirlVG，有效减少数据依赖并提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大规模多模态模型的不断发展，传统的监督微调方法在GUI视觉定位任务中成本高昂且效果有限，亟需探索更高效的训练策略。

Method: 通过系统分析强化微调的核心要素，提出稳健的对抗KL因子技术，并优化训练配置以提升效果。

Result: 仅用5.2K样本实现超越传统SFT的性能，在多个数据集上表现优异，显著优于依赖大量数据的SFT方法。

Conclusion: 基于强化学习的GUI视觉定位方法具有高效、优异的性能，有望减少大规模数据依赖，推动GUI智能代理的发展。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [189] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 提出了一种DOM量化缩减算法D2Snap，改善Web代理的状态序列化问题，利用DOM层级结构增强LLM理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有Web代理中图像快照成本过低、模型输入受限的问题，并提升模型对DOM结构的理解能力。

Method: 开发D2Snap算法，用GPT-4o模型评估，结合在线数据集进行性能测试。

Result: D2Snap在任务中表现优异，成功率与人类感知的GUI快照相当，且在模型输入限制范围内超越现有基线，显示DOM层级信息是有效的UI特征。

Conclusion: DOM层级结构为LLMs提供重要UI线索，通过D2Snap实现高效缩放，推动Web代理智能水平提升。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [190] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: 开发了SimInstruct工具，用于模拟教学对话，提升教学数据的质量和规模，并基于数据微调模型。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏高质量教学对话数据的问题，促进AI在教学支持中的应用。

Method: 利用LLMs模拟 novice 教师，结合专家多轮反馈，创建富有教学意义的对话，结合persona traits增强多样性，并微调LLaMA模型。

Result: SimInstruct生成的对话具有与真实数据相当的教学深度和相关性，专家反馈积极，模型微调后超越GPT-4o在教学质量上，揭示了GPT-4o的局限。

Conclusion: SimInstruct是一个有效的工具，有助于生成高质量教学对话数据，改善AI教学模型的表现，同时提供了对现有模型不足的深刻洞察。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [191] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 提出MERA框架，通过分离推理与控制，改进大规模推理模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LRMs中推理行为不受控、导致过度思考与计算浪费的问题。

Method: 引入控制和推理解耦机制，利用auxiliary LLMs构建高质量控制数据，采用CSPO优化控制策略。

Result: 在多项推理任务中提升了推理效率和准确性。

Conclusion: MERA通过明确的推理与控制分离，为大模型推理过程提供了更高效、更精准的监管路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [192] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 综述了操作系统(OS)智能代理的发展，包括基础框架、构建方法、评估标准和未来挑战，旨在推动该领域研究与应用。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的兴起，开发像钢铁侠中的J.A.R.V.I.S那样功能强大的AI助手成为可能，本论文旨在系统总结OS智能代理的现状与未来方向。

Method: 通过综述核心组成、构建方法、评估协议及未来研究方向，结合开源资源，全面分析OS智能代理领域。

Result: 提供了关于OS代理的基础知识、构建技巧、评估标准和未来挑战的详尽总结，促进学术和工业界的技术发展。

Conclusion: 本论文总结了OS智能代理的最新研究进展，强调安全、隐私、个性化和自我演化等未来重点，促进该领域持续创新。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [193] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种基于论证的可解释偏差检测方法，强调透明度与可解释性，适用于公平性研究。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在社会中的普及，偏差问题引发担忧，尤其是缺乏透明性的偏差检测方法。

Method: 利用论证技术，通过辩论关于偏差的存在，结合邻域中个人的保护特征值进行偏差检测。

Result: 方法在性能、可解释性和透明度方面优于基准，具有良好的实用价值。

Conclusion: 该论证基础的方法有效提升偏差检测的透明度与解释性，促进公平性研究的发展。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [194] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 引入SID基准，评估大规模语言模型在复杂跨学科STEM对话中的指导能力，发现现有模型仍有不足。


<details>
  <summary>Details</summary>
Motivation: 提升学生在复杂问题中的知识整合与迁移能力，利用LLMs进行教学指导，但缺乏有效评价工具。

Method: 构建包含10,000对话轮次的多学科STEM对话数据集，并设计新评估指标，评测不同LLMs的指导表现。

Result: 即使是最先进的LLMs在指导促成知识整合和迁移方面表现也有限，验证了基准的必要性。

Conclusion: 该基准将推动开发更具 педагогical awareness的LLMs，以提升其在教育中的应用效果。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [195] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 本文提出了ConfProBench，一个系统评估多模态大模型推理步骤置信度可靠性的基准。


<details>
  <summary>Details</summary>
Motivation: 弥补现有评估基准忽视步骤置信度可靠性的问题，提升多模态模型推理的可信度。

Method: 设计三类对抗扰动和三项新指标，评估14个先进模型的置信度表现。

Result: 发现当前模型在置信度方面存在不足，提供了有竞争力的基线，为未来改进提供指导。

Conclusion: ConfProBench有助于推动更可靠的多模态推理模型发展。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [196] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 将大型语言模型（LLMs）合作问题建模为多智能体强化学习，以提升协作效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLMs在协作中优化不足的问题，利用多智能体强化学习增强合作能力。

Method: 提出Multi-Agent Group Relative Policy Optimization（MAGRPO）算法，结合RL和MARL技术优化LLM合作。

Result: 实验表明，经过MAGRPO微调的多智能体系统能高效生成高质量响应，验证了方法的有效性。

Conclusion: 该方法为利用MARL优化LLMs合作开辟新途径，并指出未来挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [197] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: 提出SEAgent框架，通过自主学习与试错，使计算机用代理在未曾涉猎的软件环境中实现自主演化与性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型在新颖软件环境中的适应能力有限，亟需实现自主学习和进化。

Method: 设计世界状态模型、任务生成器、实验性学习策略及专家到通用策略训练，推动代理自主学习。

Result: 在五个新软件环境中的验证显示成功率提升23.2%，优于对比模型UI-TARS。

Conclusion: SEAgent实现了CUA的自主演化，显著改善其在新环境中的表现，推动智能代理的发展。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>
