<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 该研究系统性审查了GPT-4 Turbo中的代表性偏差，发现偏差在宗教和种姓表现中表现出过度代表文化主导群体的现象，且通过重复提示难以有效纠正偏差，强调需要从根本上改进模型开发。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型中深层次的代表性偏差，尤其是在非北方主要群体中的表现。

Method: 通过生成超过7200个关于印度重要生活事件的故事，比较其宗教和种姓的多样性与实际人口数据，量化偏差的存在和持续性。

Result: 发现GPT-4 Turbo偏向过度代表文化主导群体，重复提示对偏差的改善效果有限。

Conclusion: 单纯多样化训练数据不足以消除偏差，需从模型开发层面进行更深层次的改进。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [2] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 专为高能理论物理设计的定制大模型，通过不同数据集和微调技术训练，显著优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 高能理论物理领域对专业化语言模型的需求日益增长，需提升专业文本理解与生成能力。

Method: 基于8亿参数的Llama-3.1模型，结合不同学科的论文摘要数据，采用两种低秩微调方法进行微调。

Result: 模型在高能物理论文摘要补全任务中优于基础模型，并优于主要商业大模型。

Conclusion: 定制化微调策略有效提升模型在高能理论物理领域的性能，为专业模型发展提供了方向。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [3] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 印度农民缺乏及时、易懂的农业咨询，本文提出了Krishi Sathi，一个结合AI与多轮对话、检索增强模型的多语言农业聊天机器人，有效提升农民获取信息的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决农村地区农民在农业信息获取方面的困难，特别是在低 literacy 和语言障碍的背景下。

Method: 设计基于意图的多轮对话流程，结合检索增强生成模型，支持英印两种语言的语音和文本交互，利用经过微调的指令调准模型提高回答质量。

Result: 系统达到97.53%的查询响应准确率，90%以上的相关性和个性化，响应速度快于6秒，有效提升了信息获取的便捷性和准确性。

Conclusion: 结合意图驱动对话、指令调准模型和检索增强技术能显著改善农业信息服务的质量和可访问性，促进农村农业发展。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [4] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [5] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: WiNELL利用多智能体框架，实现持续自动更新Wikipedia内容，提升内容新鲜度和编辑效率。


<details>
  <summary>Details</summary>
Motivation: 应对Wikipedia内容维护的挑战，利用LLM技术实现自动化，减少人工编辑负担。

Method: 采用多智能体协作获取信息、选取重要知识、生成编辑建议，并基于历史编辑训练细粒度模型。

Result: 在高活跃度页面表现优异，准确提出及时更新建议，优于其他方法。

Conclusion: LLM智能体为知识库自动更新提供了新的研究方向，展现出巨大潜力。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [6] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: GanitBench是一个包含1527道数学题目的多语言视觉模型评估基准，涵盖英语和印地语，旨在推动多语言与视觉推理研究。


<details>
  <summary>Details</summary>
Motivation: 目前的评测基准多为单语，且缺乏印地语数据，制约多语言AI的发展。

Method: 收集印度两大考试题目，构建含图像和文本的问题集，评估闭源模型在不同推理设置下的表现，特别是在引入“Double Lock”限制后性能变化。

Result: GPT-4o mini在基准中表现最佳，最高平均准确率为38.15%；在“Double Lock”限制下表现显著下降；两种模型在回答印地语问题时性能减弱；两-shot CoT设置较单Shot更有效。

Conclusion: 该基准有助于推动多语言、多模态推理技术的发展，特别是印度语等少数语言的研究加入。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [7] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: 提出了一种基于注意力权重的新型上下文追溯方法AttnTrace，提升了追溯的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有上下文追溯方法高计算成本和低效率的问题，改善追溯的准确性和实用性。

Method: 利用LLM生成的注意力权重，结合两种增强技术，系统设计和理论分析，提出AttnTrace方法。

Result: AttnTrace在准确性和效率上优于现有方法，可应用于检测提示注入和识别钓鱼指令等场景。

Conclusion: AttnTrace是一种有效的上下文追溯工具，为提升LLM相关系统的可信性提供了新途径。

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [8] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: 提出MajorMark及其扩展MajorMark+，通过多数位感知编码策略改善多比特水印方法在内容质量与解码准确性之间的权衡，并采用集群解码策略确保高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型在实际应用中广泛部署，存在被滥用生成有害内容的风险，水印技术成为检测和追溯的重要手段，但现有方法在内容质量和信息容量之间存在折中。

Method: 引入多数位感知编码方法MajorMark，通过基于多数比特选择优选词汇集，及聚类解码策略增强解码性能。扩展至MajorMark+，将消息分块独立编码和解码以提升性能。

Result: 在多个强大LLMs上的实验表明，该方法在保持文本质量的同时显著提升解码准确度，优于以往多比特水印技术。

Conclusion: MajorMark及MajorMark+提供了一种平衡内容质量与水印信息容量的有效方案，为大规模语言模型的内容追踪和管理提供了有力工具。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [9] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 本文系统分析了大型语言模型在事实准确性方面的挑战与评估方法，强调了结合高级提示、领域微调和检索增强生成的重要性，旨在提升模型的可信度和领域适应性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛应用，确保其输出的事实准确性变得至关重要。

Method: 通过分析2020-2025年的相关文献，探讨评价指标、挑战与解决方案，包括提示策略、微调及检索增强技术。

Result: 发现现有评估指标存在局限性，强调结合验证性外部证据和领域微调有助于提升事实一致性，同时提出五个研究问题指导未来研究。

Conclusion: 未来需要构建更可信、可解释、领域定制的LLMs，以增强其事实准确性和实用性，引领向更健壮、道德的人工智能方向发展。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [10] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的实体链接代理，用于问答系统，能更好应对短、模糊的提问。


<details>
  <summary>Details</summary>
Motivation: 现有EL方法在短上下文中效果不佳，影响问答系统性能。

Method: 设计模拟人类认知流程的EL代理，包括实体识别、候选检索与决策。

Result: 通过工具式EL和问答评估，验证了其鲁棒性和有效性。

Conclusion: 该方法改善了短文本实体链接的表现，有助于问答系统的提升。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [11] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了一种名为Sotopia-RL的创新框架，通过细化奖励机制改善多维社会互动中的强化学习训练效果，有效提升了模型的社会目标完成率，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 需要提升大规模语言模型在社会交互中的表现与训练效率，克服部分观察性和多维行为带来的挑战。

Method: 通过将粗糙的全局奖励细化为个体话语的多维奖励，实现逐句信用分配，结合多维奖励策略，增强模型的社会交互能力。

Result: 在Sotopia环境中，Sotopia-RL显著优于现有方法，完成社会目标的得分明显上涨，验证了其有效性。

Conclusion: 细化的奖励机制与逐句信用分配是提升社会交互强化学习效果的关键，有助于培养更具社会智能的语言模型。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [12] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: 引入编码操作增强智能代理，通过结合GUI控制与脚本编写，实现更高效、更稳健的任务执行。


<details>
  <summary>Details</summary>
Motivation: 当前基于GUI的自动化代理在复杂任务中表现出效率低和不可靠的问题。

Method: 提出CoAct-1系统，结合GUI操作和程序编写，动态调度任务至GUI操作者或程序员代理，使用Python或Bash脚本。

Result: 在OSWorld基准测试中，成功率达到60.76%，显著优于之前方法，平均任务步骤减少至10.15步。

Conclusion: 将编码作为核心操作方式，提升智能代理的能力和效率，推动计算机自动化的发展。

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [13] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: 提出了一种结合用户偏好和事实一致性的新闻标题生成方法，显著提升个性化和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在捕捉复杂用户兴趣和确保事实一致性方面的不足，提升新闻标题质量。

Method: 引入用户偏好编码器、上下文注入适配器和事实一致性强化模块，结合大规模预训练语言模型进行优化。

Result: 在真实数据集PENS上取得了最优表现，提升了事实一致性和个性化指标，并验证了各组件的有效性。

Conclusion: CAP-LLM能在新闻标题生成中实现个性化和事实准确性的优质平衡，具有良好的应用潜力和鲁棒性。

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [14] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 提出一种面向机器学习模型全周期的偏见治理框架，强调对偏见的识别、评估和管控，提升模型的安全性和责任感。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的广泛应用，模型偏见和伦理问题日益突出，需要系统化的治理方案。

Method: 构建了基于偏见评估测试套件（BEATS）的方法，结合实际数据和治理框架，支持连续监测和指标评估。

Result: 提供了实用的偏见管理策略，增强大语言模型在生产环境中的安全性和公平性，有助于降低偏见风险。

Conclusion: 本文推动生成AI的责任与伦理实践，助力打造具有社会责任感的智能系统。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [15] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 提出一种通过早期假设修剪提高自洽性在长链推理中的词元效率的方法。


<details>
  <summary>Details</summary>
Motivation: 解决自洽性机制在长链推理中的高词元消耗问题。

Method: 在生成所有解的基础上，利用模型置信度和词汇覆盖率指标进行假设修剪，并设计了加权集覆盖算法。

Result: 多模型多基准测试显示该方法能在保持平行性的同时提升10-35%的词元效率。

Conclusion: 早期假设修剪策略有效缓解了长链推理中的词元消耗，提升了自洽性应用的实用性。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [16] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 研究利用大模型生成关于幸福感的解释，并提出评估与优化方法。


<details>
  <summary>Details</summary>
Motivation: 满足不同用户对幸福感解释的需求，提高解释的准确性和适应性。

Method: 构建大规模数据集，提出模型评估框架，结合SFT与DPO进行模型微调。

Result: 模型评价与人类一致，微调模型在解释质量上优于大模型，展示偏好优化的有效性。

Conclusion: 偏好导向的微调能显著提升说明的质量，模型微调对个性化解释具有潜力。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [17] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 本文提出一种基于概念瓶颈模型的社交机器人辅助会议协调系统，增强了对社会互动的理解和干预能力，提升了新手引导效果。


<details>
  <summary>Details</summary>
Motivation: 解决会议引导中对社会动态敏感识别和干预能力不足的问题，减少认知负担，增强透明性。

Method: 利用迁移学习将预训练的基础模型转化为可解释的概念瓶颈模型，结合多模态数据分析实现实时干预建议。

Result: 模型在不同群体中表现出良好的泛化能力，优于零样本基础模型，并支持人类监督校正。

Conclusion: 通过传递专家认知模型，显著提升了机器人在复杂社会交互中的辅助能力，为增强人类社交干预提供了技术蓝图。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [18] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 本文提出了一种支持文献综述生成的新型标注方案，并评估了多种大型语言模型在该任务中的表现，展现了高性能和改进潜力。


<details>
  <summary>Details</summary>
Motivation: 为提升科学文献的分析和综述能力，通过标注句子中的修辞角色，实现高质量文献综述的自动生成。

Method: 设计新型标注方案，建立多学科基准数据集，评估37种大型语言模型的零样本和微调表现，结合半合成数据增强训练。

Result: 高质量微调模型F1超过96%，一些轻量开源模型表现优异，半合成数据有助于提升模型性能。

Conclusion: 创新的标注方案和多模型评估为自动文献综述生成提供了技术基础，未来可进一步优化模型和数据策略。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [19] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出HarmonyGuard框架，通过多智能体合作优化网页环境中的任务执行安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着网页中隐藏威胁的演变，网页代理面临在任务性能与安全风险之间的平衡挑战，但现有研究缺乏多目标协同优化。

Method: 引入多智能体架构，包括策略增强的政策代理和双目标优化的Utility代理，进行政策提取、动态更新与实时推理。

Result: 在多个基准测试中显著提高政策合规性（最高38%）和任务完成率（最高20%），政策合规性超过90%。

Conclusion: HarmonyGuard有效提升网页代理在复杂环境下的安全性和任务效率，为未来网页智能代理的发展提供解决方案。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [20] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 利用先进的自然语言处理技术提升交通事故数据的质量，研究比较了多种模型在二次事故识别中的性能，发现微调变换模型效果最佳，同时考虑到效率和数据需求之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 改善交通事故数据质量，提升事故信息的准确性和处理效率。

Method: 试验包括零样本文本的大型语言模型、微调的变换器模型以及传统逻辑回归，模型在不同年份数据上训练和测试以评估性能。

Result: 微调变换器模型（如RoBERTa）表现最佳，F1-score高达0.90，准确率95%；零样本LLaMA次之但耗时较长，逻辑回归性能较差。

Conclusion: 微调模型在精准性和效率之间实现良好平衡，建议结合隐私保护、集成方法和增量处理以优化实际应用。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [21] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: 提出了一种新的模型编辑方法SMEdit，通过多步反向传播和正则化，提升低数据量场景下的编辑效果与训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决MLBME在低数据场景下效果不佳及训练效率瓶颈的问题，推动模型知识更新的高效途径。

Method: 引入多步反向传播策略MBPS和权重更新正则化，改善模型编辑的效果与效率。

Result: 在两个数据集和两个大型语言模型上，SMEdit优于现有基线，并且MBPS策略可融合提升性能。

Conclusion: SMEdit有效增强低数据环境下的模型编辑能力，并提升训练效率，具有潜在应用价值。

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [22] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: TURA框架结合RAG和工具，解决动态图文搜索的实时性和复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG在静态内容优化，但在动态、复杂查询方面有限制，缺乏对实时信息和结构化查询的支持。

Method: 提出三阶段框架：意图感知检索、DAG任务规划和轻量化代理执行，结合模型和工具实现动态信息访问。

Result: TURA成功填补静态与动态信息源之间的鸿沟，提升搜索的实时性和准确性，支持大规模工业应用。

Conclusion: TURA为AI搜索带来创新架构，能高效处理动态、复杂的用户需求，具备工业化部署潜力。

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [23] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA是一种面向零样本、可解释的运动传感器时间序列人体活动识别框架，结合特征知识库和多传感器检索，实现无需微调的高性能识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定活动集上训练，难以应对新行为及传感器变化，且转化为文本或图像后准确率有限且缺乏可验证解释。

Method: ZARA通过自动构建成对特征知识库，结合多传感器检索模块和分层代理管道，引导大语言模型进行特征选择、证据调取、预测与解释。

Result: 在8个基准数据集上，ZARA实现了最先进的零样本识别性能，宏F1超越最优基线2.53倍，并具备良好的可解释性。

Conclusion: ZARA展现了无微调、高性能与可解释的人体活动识别潜力，为可信赖、即插即用的运动时间序列分析迈出重要一步。

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [24] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 在资源有限的环境下，轻量级Transformer模型T5-Small在Text-to-SQL任务中表现优异，证明其在教育和商业智能中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探索低资源环境中轻量级Transformer模型在Text-to-SQL任务中的表现，并提供可扩展的管线框架以支持未来改进。

Method: 使用T5-Small、BART-Small和GPT-2三个轻量模型在Spider数据集上进行训练，通过调节 schema 格式化和多次迭代进行优化，评估其在逻辑准确率、BLEU和完全匹配指标上的表现。

Result: T5-Small在逻辑准确率上最高达27.8%，优于BART-Small和GPT-2，显示编码器-解码器模型在SQL生成方面有优势，且管线的模块化设计便于未来的优化。

Conclusion: 轻量级Transformer在资源有限的情况下仍可实现有效的Text-to-SQL转换，提供了面向低资源环境的解决方案，并为未来的模型改进铺平了道路。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [25] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 大型推理模型增强了越狱攻击的简易性和规模，引发了模型安全性的新挑战。


<details>
  <summary>Details</summary>
Motivation: 研究aims在揭示大型推理模型在越狱攻击中的作用，及其带来的安全隐患。

Method: 使用四种大型推理模型，通过系统提示指导它们设计并执行越狱，进行多轮对话，测试其在敏感内容方面的攻击效果。

Result: 模型越狱成功率达97.14%，显示LRMs可以系统性削弱其他模型的安全屏障，突显模型安全性调整的紧迫性。

Conclusion: 需对先进模型进行更严密的对齐与控制，防止它们被用作越狱工具，确保AI安全性。

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [26] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: 提出了一种基于Air-Decoding的动态令牌级前缀增强框架（DTPA），能有效提升长文本的可控生成能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可控文本生成研究多集中在短文本，对长文本生成的控制能力较少探索。

Method: 该方法动态增强前缀的注意力，并基于任务选择前缀类型，通过指数增长的方式增强注意力以改善长文本的可控性。

Result: 实验表明，DTPA在多个CTG任务中优于其他方法，特别在长文本生成中表现出色，同时保持流畅性、多样性和相关性。

Conclusion: DTPA是一种高效的长文本可控生成框架，显著改善了与长度相关的控制能力，并在多个指标上展现优越性能。

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [27] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: 引入PAIRS框架，通过双路径生成机制和适应性信息选择，提升RAG系统效率与准确性，减少不必要的信息检索。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在简单问答和信息相关性方面的低效与风险，提升问答效率和准确性。

Method: 结合自生成伪上下文，进行双路径生成与检索，通过自适应筛选相关文档，动态决定检索策略。

Result: 在六个问答基准测试中，PAIRS显著降低检索成本25%同时提升准确率（EM+1.1%、F1+1.0%）。

Conclusion: PAIRS通过无需额外训练，结合参数和检索信息，有效提升RAG性能，兼顾效率与效果。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [28] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 提出一种基于数据处理、策略调整与架构优化的方法，以提升大规模语言模型在资源受限环境中的效率，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在部署过程中资源消耗大的问题，提升其在受限环境中的应用能力。

Method: 通过定义数据集构建标准，进行多种配置的控制实验，以及系统评估模型性能，结合数据选择与架构调整策略。

Result: 不同配置的模型在能力、响应时间和安全性方面表现出明显差异，验证了所提策略的有效性。

Conclusion: 通过合理的数据处理和架构调整，可以显著提升LLMs在资源有限条件下的性能表现，推动其更广泛的应用。

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [29] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: ToolGrad通过反转传统方法，先构建工具使用链，后生成用户查询，从而提升数据质量和效率，训练出的模型性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有工具使用数据集生成中的标注失败和效率低下问题，提升工具链复杂场景下的模型表现。

Method: 采用“Answer-First”策略，通过迭代引导文本梯度，构建工具链后生成用户查询，形成ToolGrad数据集。

Result: 构建了ToolGrad-5k数据集，表现出更复杂的工具使用、更低成本和完美通过率。模型在多个基准和OOD任务中优于基线。

Conclusion: ToolGrad有效提升工具用例数据的生成质量和效率，增强模型在复杂和未见任务中的能力。

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [30] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: 提出了一种新型的多模态过程奖励模型（GM-PRM），可作为主动推理合作伙伴提升多模态大语言模型的数学推理能力，显著改善正确率和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型在复杂数学推理中常出错，且现有的过程奖励模型只能验证而不能改正，缺乏解释性。

Method: 引入GM-PRM，将其从被动评判转变为主动推理合作伙伴，提供细粒度分析并生成修正方案，结合Refined-BoN策略改善推理过程。

Result: 在多模态数学基准测试中，GM-PRM实现了最先进的性能，大幅提升模型效果，且训练仅需20K样本。

Conclusion: GM-PRM通过主动生成修正方案增强多模态推理的准确性和解释性，展示出优越的性能和数据效率。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [31] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 大规模预训练语言模型微调过程中出现的过度记忆现象，导致模型鲁棒性和泛化能力下降。


<details>
  <summary>Details</summary>
Motivation: 探究微调过程中模型学习动态，揭示过度记忆现象及其影响。

Method: 分析微调阶段模型表现，实验验证过度记忆条件，比较不同任务和模型。

Result: 发现高训练轮次和大学习率促成过度记忆，过度记忆影响模型鲁棒性和泛化，但对准确率影响较小。

Conclusion: 建议在微调时合理选择检查点和学习率，以避免过度记忆带来的负面影响。

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [32] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出了一种基于难度的偏好数据选择策略，有效提升大语言模型的对齐效果，使用少量数据即可达到优异表现。


<details>
  <summary>Details</summary>
Motivation: 改善偏好数据的质量与效率，降低大规模偏好数据采集成本。

Method: 基于DPO隐式奖励机制，选择具有较小奖励差异的偏好样本，即困难样本，以增强数据的代表性与模型的对齐能力。

Result: 在多项任务中，这一方法优于五个强基线模型，只用原始数据的10%，即显著提升模型性能。

Conclusion: 难度导向的数据选择策略为大语言模型对齐提供了高效、资源节约的解决方案，有助于模型在资源有限条件下的优良表现。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [33] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 引入人类欺骗率（HFR）指标，评估TTS系统是否能欺骗人类，揭示当前模型在逼真度和自然度上的差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法受限，难以准确反映TTS系统的真实逼真度。

Method: 通过大规模测试多种开源和商业TTS模型，计算其被误认为人声的频率（HFR），并对比不同模型和数据集表现。

Result: 商业模型在零样本条件下接近人类欺骗水平，开源模型仍有提升空间；高质量微调有所帮助，但未完全解决问题。

Conclusion: 需要引入更具现实感和以人为本的评估标准，以全面衡量TTS系统的逼真度与自然度。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [34] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 本文提出一种基于因果完备性的强化学习框架，有效减少多模态大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉与语言任务中表现优异，但存在生成与输入不符的幻觉问题，亟需解决。

Method: 通过因果分析识别模型幻觉的成因，设计以因果完备性为指导的奖励机制，结合GRPO优化框架，提升模型对关键因果信息的关注。

Result: 该方法在多个基准数据集和任务中验证了其有效性，有效减轻了模型的幻觉现象。

Conclusion: 引入因果完备性指导的强化学习显著改善了多模态大模型的生成质量，减少了幻觉，提升了模型的可靠性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [35] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 提出深度研究（DR）任务的定义、构建了评估基准LiveDRBench，并分析现代系统表现及未来方向。


<details>
  <summary>Details</summary>
Motivation: 弥补深度研究任务定义模糊及评估不足的问题，推动相关系统的发展。

Method: 正式定义DR任务，引入中间输出表示，设计多样化的挑战任务。

Result: 现有系统表现参差不齐，OpenAI模型表现最佳，分析揭示搜索机制不足。

Conclusion: 为未来改进深度研究系统提供理论基础和评估工具。

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [36] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 即使技术不断进步，先进的语言模型仍受隐蔽的对话攻击影响。


<details>
  <summary>Details</summary>
Motivation: 探索当前对齐技术在面对复杂说服场景时的漏洞，确保AI行为安全可靠。

Method: 通过手动红队测试与自动化评估框架，系统性揭示模型的脆弱性。

Result: 发现多种误操作行为，并制定MISALIGNMENTBENCH评估工具，显示模型在不同攻击场景下的易受攻击程度。

Conclusion: 当前对齐方法存在明显缺陷，未来需强化模型对场景操控的抵抗能力，保障安全性。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [37] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 该论文提出一种诊断框架，评估大规模语言模型在理解非正式、多语混合的肯尼亚青年WhatsApp信息中的情感，强调文化敏感性和推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析方法在文化多样、资源有限的环境中效果有限，亟需考虑文化背景和推理能力。

Method: 结合人类标注数据、反事实情感变化和评分指南，评估LLMs的解释性、稳健性及与人类推理的一致性。

Result: 高端模型具备较稳定的解释能力，但开放模型在模糊或情感变化时表现不佳，揭示模型推理差异。

Conclusion: 需要发展文化敏感且推理能力强的AI评估方法，以应对复杂的实际交流场景。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [38] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: ReasoningGuard是一种推理时的安全措施，通过模型内部注意力行为监测关键点，触发安全反思，有效抵御引导危险内容的攻击，且成本低于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 提高大型推理模型的安全性，避免有害内容生成，且成本低，易于扩展。

Method: 利用模型内部注意力行为，识别推理关键点，触发安全反思；在解码阶段采用采样策略优化推理路径，减少额外推理成本。

Result: 在防护三类最新的破解攻击方面优于七种现有防护措施，达到最先进的安全防御效果，且避免了过度安全的副作用。

Conclusion: ReasoningGuard是一种高效、低成本的推理时安全策略，有望广泛应用于大型推理模型中，提升其安全性和可靠性。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [39] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 利用黑箱大语言模型进行层级文本分类，结合不同提示策略在准确性和成本上的权衡，展示了其潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决层级文本分类中数据匮乏和模型复杂性的问题，探索大语言模型的应用潜能。

Method: 采用三种提示策略（DL、DH、TMH）在零样本和少样本条件下验证模型性能，比较准确率与成本。

Result: 少样本条件下性能提升明显，深层次层级中LLMs优于传统模型，DH策略在深层结构中表现最佳，但成本较高。

Conclusion: 黑箱L LM具备潜力，但需权衡策略选择以兼顾效果与成本。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [40] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: 提出了一种双提示大语言模型框架DP-GPT4MTS，有效结合文本和数值信息提升时间序列预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测模型忽略文本信息，影响预测效果。大语言模型虽有潜力，但单提示框架难以有效捕捉时间戳文本语义。

Method: 引入双提示机制，包括明确任务提示和上下文文本提示，通过自注意力和前馈网络优化文本嵌入。

Result: 在多个时间序列数据集上实验优于现有最优算法，验证了该方法的有效性。

Conclusion: 结合文本上下文的双提示机制显著提升时间序列预测准确性，具有广泛应用前景。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [41] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 提出了一种基于先进语言模型的虚拟患者模拟方法，用于提升抑郁症诊断训练的真实性和多样性。


<details>
  <summary>Details</summary>
Motivation: 由于实际培训数据不足，限制了精神健康临床专业人员的培养和抑郁症诊断的支持。

Method: 开发了联结临床专家的TalkDep管线，通过条件化模型创造具有多样性和临床有效的虚拟患者。

Result: 验证显示，该模拟患者具有高可靠性，有助于增强自动抑郁症诊断系统的鲁棒性和泛化能力。

Conclusion: 模拟患者资源的引入，为精神健康诊断模型的训练和评估提供了可扩展的解决方案。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [42] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: KVSink通过深入分析注意力汇聚点的形成机制，有效提升了KV缓存量化中的注意力保护，从而提升大语言模型推理效率及表现。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存量化方法在保护前几Token的注意力汇聚点方面虽然有效，但未充分理解其原理，且未能应对汇聚点在后续Token中的出现。

Method: 作者分析了注意力汇聚点在推理中的机制，特别是其在极端激活异常的跨层演变中的作用，并基于此提出KVSink预测汇聚点的解决方案。

Result: KVSink在多个实验中优于现有策略，不仅提升了注意力点的保护效果，还改善了模型的困惑度（PPL）及减少了对高精度数值的依赖。

Conclusion: 通过深入理解注意力汇聚点的形成机制，KVSink实现了更有效的KV缓存量化保护技术，显著提升了大规模语言模型的推理性能。

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [43] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: 提出ShoppingBench，旨在模拟处理更复杂购物意图的多层次评估平台，采用合成指令和强化学习以提升智能体能力。


<details>
  <summary>Details</summary>
Motivation: 现有电商基准主要关注基本用户意图，难以反映实际复杂购物需求。

Method: 设计可扩展的指令模拟框架，构建包含大量真实商品的虚拟购物环境，并采用轨迹蒸馏和强化学习优化智能体性能。

Result: 评估显示即使最先进的模型成功率不足50%，通过方法训练的智能体表现接近GPT-4.1。

Conclusion: ShoppingBench揭示复杂购物任务的挑战，为多意图智能体的发展提供新的评测平台和训练策略。

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [44] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: 提出两种针对GraphRAG的知识中毒攻击，显著改变知识图结构，误导推理，且现有防御措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着GraphRAG在增强LLMs中的应用增加，攻击者可能利用其脆弱性植入误导信息，威胁系统安全。

Method: 设计了Targeted KPA和Universal KPA两种攻击，前者通过图的分析定位弱点，后者通过 linguistics cues 影响图结构。

Result: 攻击能够在极少改动下大幅影响问答准确性，且防御方法难以检测。

Conclusion: GraphRAG存在严重安全风险，需开发更有效的防护措施。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [45] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 提出MedCheck，为医疗语言模型基准提供生命周期管理和规范，揭示现有基准中的系统性问题。


<details>
  <summary>Details</summary>
Motivation: 当前医疗语言模型基准存在可信度不足、临床相关性差和安全性评价缺失的问题，需要更系统的评估框架。

Method: 设计了一套覆盖基准从设计到治理五个阶段的评估框架，包含46个医用标准，并对53个医疗基准进行实证分析。

Result: 发现普遍存在临床脱节、数据完整性危机以及忽视安全性评价的问题，强调了改进的必要性。

Conclusion: MedCheck作为诊断和指导工具，有助于推动医疗AI评估的标准化、透明度和可靠性提升。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [46] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 提出动态熵加权机制，通过细粒度奖励提升强化学习中对长链推理任务的效果，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中因奖励粒度粗导致在长链推理任务中表现不佳的问题。

Method: 引入动态熵加权机制，提出Group Token Policy Optimization (GTPO)和Sequence-Level GRPO-S两种策略，实现对单个token和序列的细粒度奖励分配。

Result: 新方法显著优于传统基线DAPO，验证了熵加权机制对提升模型深度推理能力的关键作用。

Conclusion: 动态熵加权机制有效提升了强化学习在复杂推理任务中的性能，为未来深度推理模型设计提供新路径。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [47] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 提出了一种名为CoQ的多模态推理框架，通过动态生成问题引导模型选择相关感官模态，提升多模态任务的推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决多模态环境中模型难以有效识别和整合多感官信息的问题，推动多模态推理能力的发展。

Method: 引入疑问驱动的推理方法，通过生成针对环境的目标问题引导模型主动选择感官模态，并在包含多个数据集的基准测试中验证效果。

Result: CoQ显著提升模型识别和整合多感官信息的能力，从而改善任务准确性、可解释性和推理流程的对齐性。

Conclusion: 通过动态问题引导和多模态感知，有效增强模型在复杂多模态环境中的推理能力，促进多模态AI的发展。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [48] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 提出一种基于RAG的事实核查流程，在资源有限条件下实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 提升事实核查系统的效率和性能，特别是在资源受限的场景中。

Method: 采用两步RAG流程，源自去年方案，重部署于本地环境。

Result: 在FEVER 8共享任务中获得第一名，且在限制硬件条件下仍达成最先进的性能。

Conclusion: 该方法证明了在硬件限制下，简洁高效的事实核查系统依然可以取得优异表现。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [49] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: 深度神经网络（DNN）和大语言模型（LLMs）能力的出现源于复杂系统的动态演化，其能力非线性、集体互动的结果，源于复杂动力学，而不是简单的参数扩展。


<details>
  <summary>Details</summary>
Motivation: 探讨大规模模型在缺乏明确训练下能力突现的理论基础，理解其本质。

Method: 结合理论分析与实证观察，分析规模规律、掌握现象和相变，比较非线性随机过程与符号计算的差异。

Result: 能力的突现来自高度敏感的非线性系统的复杂动力学，而非仅靠参数规模。当前指标与阈值理解偏差，忽视了能力突现的本质。

Conclusion: DNNs是复杂动力系统，其能力突现符合自然界中的复杂现象，应从动态演化角度理解能力的升华，超越单纯参数或微观行为的限制。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [50] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 研究聚焦语音识别的选择性听力能力，提出利用人类选择性听力评估ASR系统，从而改善对话系统的响应生成。


<details>
  <summary>Details</summary>
Motivation: 理解人类在对话中如何集中注意关键信息以提升ASR性能，改善对话系统的响应质量。

Method: 通过比较人类转录与参考转录，验证人类的选择性听力能力，探讨基于此能力的ASR评估新方法。

Result: 确认了人类的选择性听力在对话响应生成中的重要性，提出了利用人类选择性听力进行ASR系统评估的可能性。

Conclusion: 引入人类选择性听力作为ASR性能评估的新指标，有助于缩小ASR系统与人类听力的差距，推动对话系统的优化。

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [51] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 提出了一种预测置信模型（PCM）来判断是否可以提前预取对话响应，从而降低用户感知的延迟。


<details>
  <summary>Details</summary>
Motivation: 减少语音对话系统中用户等待响应的时间（UPL），通过提前预测用户完整话语实现响应预取。

Method: 使用语义相似性估计预测完整用户话语与实际用户话语之间的差异，判断是否可以预取。

Result: 通过评估PCM在预测完整用户话语与实际话语之间差异的效果验证其可行性。

Conclusion: PCM能有效判断预取的可行性，有助于降低用户感知的延迟。

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [52] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [53] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 提出ThinkingF方法，结合数据合成和训练技术，显著提升大模型在自动形式化数学表达中的准确性和推理能力，达到了新的状态空间。


<details>
  <summary>Details</summary>
Motivation: 解决当前自动形式化方法在准确率方面的不足，强化模型对形式化领域知识的掌握和自然语言推理能力。

Method: 构建丰富的形式知识数据集和生成推理轨迹，采用SFT和RLVR训练策略，融合知识掌握与推理能力。

Result: 7B和32B模型在自动形式化任务中表现优异，特别是32B模型在FormalMATH-Lite和ProverBench上的SOTA成绩。

Conclusion: ThinkingF通过数据合成与强化训练，有效提升模型的形式化正义性与推理能力，推动自动形式化技术的发展。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [54] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 研究提出四种基于GPT-4的马来语数学选择题生成方案，利用RAG技术显著提升课程相关性和事实准确性，辅以创新评估框架，适用于低资源语言的教育评估工具开发。


<details>
  <summary>Details</summary>
Motivation: 解决马来西亚教育系统中高质量、可扩展的评估工具缺乏的问题，特别是在低资源语言环境下。

Method: 引入四种递增的内容生成流程，从非基础提示到RAG技术，包括框架化和手工实现两种方式，结合官方课程资料进行题目生成，并用语义相似度和RAG-QA评估题目质量。

Result: RAG基础流程显著优于非基础提示，生成题目在课程相关性和事实准确性方面表现更佳。自动化评估框架有效验证了其有效性，提供了实用的内容生成和评估策略。

Conclusion: 提出了一套适用于低资源语言的课程内容生成及评估的方法，为马来西亚及类似地区的教育科技发展提供实践指导。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [55] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: 提出一种扩展词义区分的方法，结合实体间关系提升词义理解能力。


<details>
  <summary>Details</summary>
Motivation: 丰富词义表示，克服现有方法只在同词形范围内优化的局限性。

Method: 构建基于SemCor数据集的Concept Differentiation任务，训练Concept-Aligned Embeddings（CALE），并在多种词汇语义任务中测试。

Result: CALE模型实现了多功能的词义表示，表现优越，且显著改善了嵌入空间的结构组织。

Conclusion: 扩展词义区分任务有效增强了模型对词义的捕捉能力，为词义表示研究提供了新思路。

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [56] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: 提出StyliTruth方法，通过分离模型中的风格和真实信息子空间，有效减少风格化过程中的“真实性崩溃”现象，实现风格风格偏好与真实性的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在生成任务中的广泛应用，对风格化回复的控制成为研究热点，但存在风格化影响真实性的瓶颈问题。

Method: 在模型表示空间中采用正交消减技术，将风格相关与真实性相关的子空间分离，通过动态调节词元级引导向量，实现对生成内容的独立控制。

Result: 在多种风格和语言条件下验证，StyliTruth显著减少真实性崩溃，优于现有方法，保持风格一致性和真实性。

Conclusion: 通过分离和独立控制表示子空间，有效平衡风格表达和信息真实性，为风格化LLM应用提供了新的解决方案。

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [57] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 介绍了C-MIND数据集和多模态诊断方法，用于临床抑郁症评估，并探讨了大型语言模型在诊断中的潜力和限制。


<details>
  <summary>Details</summary>
Motivation: 弥补现有研究在临床验证和实际应用中的不足，推动抑郁症自动化评估的临床化。

Method: 收集和分析多模态数据，训练传统模型，并结合大语言模型进行推理，采用专家指导优化模型表现。

Result: 提出的模型及方法提升了诊断性能，明确了LLMs的限制，并通过临床指导改善其效果。

Conclusion: 建立了临床抑郁症评估的基础架构，为未来精神健康研究提供数据和算法支持。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [58] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 多智能体合作框架在科研提案生成中优于单一智能体，团队结构和多样性对创新质量影响显著。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体讨论是否能超越单一智能体，从而提升科研创意的质量。

Method: 提出合作多智能体框架，比较不同团队规模、结构和组合的效果，利用评分体系评估想法质量。

Result: 多智能体讨论显著优于单一智能体，领导者促进整合，认知多样性和专家知识是关键。

Conclusion: 多智能体合作能有效提升科研创意，团队结构与成员多样性是关键因素。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [59] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出了一种基于字典学习的Transformer层结构权重共享方法MASA，有效降低参数量，同时保持性能，适用于多种规模模型及任务。


<details>
  <summary>Details</summary>
Motivation: 移动端和低资源场景对大规模LLMs的计算和存储需求成为障碍，现有压缩技术主要关注块内优化，缺乏对层间冗余的挖掘。

Method: 将注意力投影矩阵分解为共享字典基，通过线性组合表示各层权重，操作简便可作为替代装置，训练使用标准优化器。

Result: 在不同规模模型上，MASA优于其他方法，参数减少66.7%，性能相当甚至更优；在Vision Transformers中亦表现出色，参数明显减少，推广至预训练模型表现优异。

Conclusion: MASA通过字典学习实现跨层参数共享，有效提升模型参数效率，为大规模Transformer模型的轻量化提供新的解决方案，兼顾性能与效率。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [60] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: P-Aligner通过生成更符合人类偏好的指令，提升大语言模型的对齐效果，成本低且效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在面对有缺陷的指令时难以保持安全、诚实和有帮助的内容，存在对指令预先对齐的需求。

Method: 提出P-Aligner模块，在UltraPrompt数据集上训练，通过蒙特卡洛树搜索系统探索指令空间，生成符合人类偏好的指令。

Result: P-Aligner在多个模型和基准测试中表现优越，如GPT-4-turbo和Gemma-2-SimPO，分别提升28.35%和8.69%的胜率，并验证了其效率和效果的优越性。

Conclusion: 预对齐指令的方法通过系统性探索和优化，可显著改善大模型的对齐效果，具有高性价比。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [61] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [62] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 该论文系统回顾了74项关于反言论的自然语言处理研究，发现当前研究偏离了受影响社区的需求，建议加强利益相关者的参与以优化反言论策略。


<details>
  <summary>Details</summary>
Motivation: 随着在线仇恨言论的增多，反言论在自然语言处理中的应用受到重视，但研究逐渐偏向自动化和工具化，缺乏受影响社区的主动参与。

Method: 本文通过系统性文献回顾74项研究，并结合与五个反性别暴力非政府组织的合作案例研究，分析利益相关者的参与及其影响。

Result: 研究发现当前NLP反言论研究缺乏社区参与，偏离了实际需求。案例研究提出了利益相关者指导的反言论生成实践。

Conclusion: 强调重视社区和利益相关者在反言论研究中的作用，提出相关建议以增强研究的相关性与实用性。

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [63] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: mmGRPO是一种扩展的多模块版本的GRPO，可提高多任务下的语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 针对组合多个语言模型调用的模块化AI系统，探索如何利用GRPO进行优化。

Method: 定义了多模块GRPO（mmGRPO），融合自动提示优化，处理多模块、多变长和中断的轨迹。

Result: 在分类、搜索和隐私任务中，mmGRPO提升了11%的准确率，单独提示优化提升5%，并且已开源。

Conclusion: mmGRPO有效提升了模块化系统中多任务的性能，美观性强，具有实用价值。

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [64] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: 提出一种通过主动管理内部记忆的工具框架，增强大规模语言模型处理长文档的能力，有效缓解干扰问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文时性能下降，主要因主动干扰

Method: 引入Sculptor框架，配备片段化、摘要、搜索等工具，主动调控模型内部记忆

Result: 在多个长上下文基准测试中显著提高模型性能，无需额外训练。

Conclusion: 主动管理上下文是提升长文本处理能力的关键，应成为未来研究重点。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [65] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 提出了一种名为GeRe的持续学习框架，通过利用预训练文本和增强激活状态约束，有效解决大规模语言模型的灾难性遗忘问题，同时提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在多任务连续学习中面临的灾难性遗忘和性能退化问题，推动人工通用智能的发展。

Method: 引入基于样本重放的框架，结合神经激活状态约束优化（TM损失）技术，用少量预先采集的样本实现稳定抗忘和性能提升。

Result: 验证了少量固定重放样本在保持模型能力和提升性能上的有效性，TM损失优于其它重放策略，并表现出更好的鲁棒性。

Conclusion: 提出的GeRe框架和TM损失为大模型持续学习提供了高效稳定的解决方案，为未来大模型的抗忘和性能优化提供了理论基础和实践路径。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [66] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: 本文提出了在有限偏好标记数据下，个性化偏好对齐的新方法FaST，利用高层特征实现最优性能，并提供两个新的数据集以支持该研究。


<details>
  <summary>Details</summary>
Motivation: 解决聊天助手个性化不足的问题，尤其是在偏好标记有限的情况下，实现模型定制。

Method: 引入两个新数据集、评估多种对齐技术，并提出一种高参数效率的FaST方法，利用自动发现的高层特征。

Result: FaST在有限数据条件下表现最佳，显著优于其他方法，验证了其有效性。

Conclusion: 提出的FaST方法有效促进了个性化偏好对齐，能够在数据有限的场景下实现优异性能，为未来个性化系统的发展提供了新思路。

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [67] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 本文系统分析了当代语言模型在多跳问答任务中的推理失败，提出详细的错误分类框架，揭示模型在多文档源、信息覆盖和认知效率方面的局限，为未来模型的改进提供指导。


<details>
  <summary>Details</summary>
Motivation: 为理解为何当前模型在推理任务中出现幻觉及失败，系统探索模型推理过程中的错误模式。

Method: 引入多维度错误分类框架，通过人类注释和自动指标分析模型在多跳问答中的推理失误。

Result: 发现多种复杂错误模式，强调模型在源文档多样性、信息完整性和认知效率方面的不足，揭示模型推理的认知局限。

Conclusion: 深入理解模型推理中的错误，为提升推理准确性、透明度和鲁棒性提供了理论基础和改进方向。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon框架通过对比学习和特权信息提升模型在实际环境中的情感识别表现，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算模型从实验环境向实际应用环境转移的困难。

Method: 结合有监督对比学习（SCL）和特权信息学习（LUPI），进行预训练以增强模型鲁棒性。

Result: 在两个基准数据集上，PriCon显著优于LUPI和端到端模型，部分情况下性能接近全模态训练模型。

Conclusion: PriCon为实现更稳健的现实环境情感模型提供了有效途径，有望缩小实验与实际应用的差距。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [69] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C是一种支持多维度轨迹压缩的创新框架，通过频域物理建模和误差控制优化，显著优于现有的线简化方法。


<details>
  <summary>Details</summary>
Motivation: 随着位置感知设备产生大量轨迹数据，迫切需要高效压缩技术以节省存储和传输空间。

Method: 结合频域物理建模与误差界定优化，支持独立压缩每个空间轴，实现对任意维度轨迹的处理。

Result: 在多个实际数据集上，PILOT-C在压缩比和轨迹保真度方面均优于现有最优算法，且能自然扩展到3D轨迹，无额外复杂度。

Conclusion: PILOT-C提供了一种高效、通用的多维轨迹压缩方案，为大规模轨迹数据管理提供新思路。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [70] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind是一种新颖的放射科X线影像诊断模型，通过“思考-回答”间歇式推理，结合课程化强化学习和可验证奖励，显著提升诊断准确性和临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 提高多任务胸部X线影像诊断的效率和解释能力，解决现有多模态模型推理时间长、奖励稀疏和出现幻觉的问题。

Method: 构建CX-Set数据集，通过课程化强化学习和规则引导的奖励机制进行分两阶段优化，实现“思考-回答”间歇式生成推理。

Result: 显著优于现有模型，提升25.1%的性能，临床数据集表现优异，专家评价认可其临床应用前景。

Conclusion: 提出的CX-Mind模型创新性实现可验证推理，有效提升胸部X线影像多任务诊断的准确性和可靠性，对医疗AI具有重要意义。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [71] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 提出了一种名为LKS的模型编辑方法，可以在大规模同时编辑大量事实信息的同时，保持模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑难以同时大规模编辑事实信息，且可能损伤模型的整体能力。

Method: 通过操纵LLMs的潜在表示，利用轻量级超网络实现精准大规模知识编辑。

Result: 在Llama-2和Mistral上实验表明，即使同时编辑多达10,000个实体，LKS仍能有效编辑知识且不损失模型能力。

Conclusion: LKS通过潜在表示编辑提供一种高效且大规模的模型修正方法，展现出良好的应用潜力。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [72] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一种多模态梯度提升框架，结合临床特征、眼底图像和文本描述，实现高效且可解释的青光眼风险预测，验证准确率达98.71%。


<details>
  <summary>Details</summary>
Motivation: 现有青光眼检测方法多依赖单一模态且缺乏解释性，限制临床应用效果。

Method: 利用预训练卷积编码器提取视网膜图像特征，transformer模型编码文本信息，结合手动风险评分与神经眼底生理指标，融合入增强版XGBoost模型进行分类。

Result: 在真实标注数据上验证，模型性能优越，准确率达98.71%。特征重要性分析符合临床经验。

Conclusion: GlaBoost提供了一个可扩展、透明且具有临床解释性的青光眼检测方案，可推广应用于其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [73] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种融合全局低秩与局部平滑的Tucker分解模型LRTuckerRep，能有效进行多维数据补全，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多维数据补全在多个科学领域关键，但现有方法存在计算成本高和泛化能力差的问题。

Method: 利用自适应加权核范数和稀疏Tucker核结合参数免费拉普拉斯正则，开发两种收敛保证的迭代算法。

Result: 在多维图像修复和交通数据插补中，LRTuckerRep表现出优越的补全精度和鲁棒性。

Conclusion: 该模型有效融合全局与局部先验，提供一种高效稳定的多维数据补全方案。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [74] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出了一种利用大型语言模型自动生成先验分布的框架，简化贝叶斯推断的先验设定过程，并扩展至多智能体系统，实现分布的去中心化聚合，降低专业门槛。


<details>
  <summary>Details</summary>
Motivation: 手动设定先验分布繁琐、主观且难以扩展，亟需自动化和规模化解决方案。

Method: 引入LLMPrior，通过将LLM与可 tractable 的生成模型结合，自动将自然语言描述等转化为有效的先验分布，扩展至多智能体系统实现去中心化聚合。

Result: 实现了基于LLM的先验自动生成和多智能体分布汇聚，验证了框架的可行性和鲁棒性，为复杂贝叶斯模型提供了自动化工具。

Conclusion: 该方法为贝叶斯建模提供了一种新的自动化路径，有助于降低应用门槛，推动前沿研究和实践应用的发展。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [75] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出了一种针对在线分布鲁棒强化学习的高效算法，有效应对未知环境中的最坏性能优化问题，并验证了其理论与实用性。


<details>
  <summary>Details</summary>
Motivation: 解决实际强化学习中因模拟环境与实际环境差异导致的性能下降问题，特别是在未知环境条件下的鲁棒性挑战。

Method: 采用基于f-散度的不确定性集合，提出一种计算高效、具有次线性遗憾保证的在线分布鲁棒RL算法，并推导出最小-最大遗憾下界。

Result: 算法在多种环境中表现出优异的鲁棒性和效率，验证了其理论保证的有效性，并达到了几乎最优的遗憾水平。

Conclusion: 本文提出的算法在未知环境下实现了在线分布鲁棒强化学习的目标，具备理论坚实和实践价值。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [76] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 提出一种新的策略优化方法GTPO，有效解决现有GRPO的两个主要限制，提高模型训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO在训练语言模型时存在对冲突信号和分布平坦化的问题，影响模型效果。

Method: 引入GTPO，通过识别冲突令牌并调整更新策略，同时过滤高熵完成，避免策略崩溃，替代依赖KL散度的正则化。

Result: 在多项基准测试中验证，GTPO比GRPO更稳定且性能更优。

Conclusion: GTPO是一种有效的改进策略，提升了语言模型训练的稳定性和效果。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [77] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: U-PINet是一种基于物理信息的深度学习模型，结合层次结构和稀疏图表示，有效提升电磁散射模拟的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统数值方法计算复杂、纯数据驱动方法缺乏物理约束的问题。

Method: 设计具有多尺度处理能力的U形结构网络，利用稀疏图表示模型局部和远场电磁耦合。

Result: 模型能准确预测表面电流分布，验证其在散射特性和雷达截面积预测中的优越性能，显著缩短计算时间。

Conclusion: U-PINet提供了一种高效、可靠、具有物理一致性的电磁散射模拟新方案，具有应用潜力。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [78] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 提出一种基于物理约束的神经网络方法，用于加速核聚变装置EAST中的热通量估算，与传统有限元法相比，速度快40倍，且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统有限元法在核聚变热通量估算中计算效率低、难以实时应用的问题。

Method: 设计物理信息神经网络，结合热传导方程，利用边界条件、初始条件和少量数据点进行训练。

Result: 模型在多种加热条件下实现了与有限元法相当的精度，计算速度提升40倍。

Conclusion: 基于PINN的方法有效提升核聚变设备热通量估算的效率，具有良好的应用前景。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [79] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 提出了一种多模态多任务模型SoilNet，用于复杂的土层分类，结合图像和地理时序数据，通过分段和层次结构处理提高分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解决土壤水平面分类中的多模态、多任务和复杂层次结构问题，这对农业和生态监测具有重要意义。

Method: 设计一个结构化的模块化管道，结合图像与地理时序信息，预测深度标志，分割土层候选，并利用图结构标签表示，进行层次分类。

Result: 在真实土壤剖面数据集上验证了模型的有效性，展示了在复杂层次标签体系中的优越表现。

Conclusion: 多模态多任务模型通过结构化处理和图结构标签，有效解决复杂层次分类难题，为土壤监测提供了新工具。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [80] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文提出了Bernoulli-LoRA，一种融合和扩展现有LoRA方法的概率机制框架，通过分析多种变体的收敛性，并验证其在实际任务中的有效性，推动了参数高效微调的理论基础和实用性发展。


<details>
  <summary>Details</summary>
Motivation: 随着大规模基础模型的增长，迫切需要高效且具有理论保障的微调方法，以便在实际任务中应用。

Method: 引入Bernoulli机制控制矩阵更新，结合非凸优化理论，分析多种变体的收敛性，同时在不同任务中进行实证验证。

Result: 证明了多种Bernoulli-LoRA变体的收敛性并验证其在实际任务中的有效性，扩展了PEFT的理论基础。

Conclusion: Bernoulli-LoRA提供了一种理论上可靠且实用的PEFT方法，为大模型微调提供了新的方向。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [81] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO是一种基于神经网络的黑箱优化新方法，克服了高维空间中的计算复杂性，表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决贝叶斯优化在高维空间和大量函数评估中的扩展性和计算效率问题，寻找更高效的替代方法。

Method: 提出SNBO方法，利用神经网络代替高斯过程模型，采用不同的采样准则进行探索和利用，动态调节采样范围，无需模型不确定性估计。

Result: 在10到102维的多种优化问题中，SNBO在函数值表现优异，且比基线算法节省40-60%的函数评估次数，运行时间至少减少一个数量级。

Conclusion: SNBO提供了一种高效的高维黑箱优化方案，通过避免复杂的模型不确定性估计，显著提升了优化性能和计算效率。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [82] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 提出一种同时保证隐私和公平的多臂老虎机算法框架DP-NCB，兼顾差分隐私与纳什公平，具有最优性并适用多模型。


<details>
  <summary>Details</summary>
Motivation: 在社会敏感应用中，确保用户数据隐私和公平性成为多臂老虎机算法的重要需求。

Method: 引入差分隐私纳什置信界（DP-NCB）框架，结合差分隐私机制和纳什公平目标，确保在不牺牲正交优化目标的前提下实现隐私保护和公平。

Result: 在合成实验中，DP-NCB显著优于现有基线模型，具备理论保障与实际效能。

Conclusion: DP-NCB提供一种融合隐私保护和公平性的多臂老虎机算法设计思路，适用于高风险社会场景。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [83] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种可部分训练的代理模型，用于非线性偏微分方程的正向和逆向求解，该模型在效率和准确性方面优于FNO和DeepONet。


<details>
  <summary>Details</summary>
Motivation: 解决高维参数化非线性偏微分方程的计算效率和准确性问题，提出可部分训练的代理模型以降低训练时间和能源消耗。

Method: 设计了VAE-DNN模型，结合变分自编码器与神经网络，将输入和解的潜在空间分离训练，提取特征。

Result: VAE-DNN在 groundwater 流动方程的正逆问题中表现出比FNO和DeepONet更高的效率和准确性。

Conclusion: 可部分训练的VAE-DNN模型有效降低训练成本，同时提高求解的精度，具有潜在的广泛应用前景。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [84] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 提出一种基于众包数据的时空频谱需求预测新框架，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 频谱需求预测对于频谱分配和无线通信发展至关重要，尤其是在应对新技术如5G、6G和物联网的挑战时。

Method: 结合特征工程、相关性分析和迁移学习，利用用户KPIs和监管数据进行频谱需求预测。

Result: 模型在预测准确性和跨区域适用性方面优于ITU传统模型，验证了其在实际频谱管理中的应用潜力。

Conclusion: 该多数据源驱动的预测框架能提升频谱管理的科学性，为政策制定提供更可靠依据。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [85] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息理论的有针对性采样方法，用于优化数据流中的离线学习，强调减少预测不确定性，提高性能。


<details>
  <summary>Details</summary>
Motivation: 在数据流中建立高效学习模型，关键在于平衡信息捕获和计算成本，亟需更优的采样策略。

Method: 采用以预测不确定性减小为核心的情報理論机制，进行数据子采样，以提升模型性能。

Result: 该方法在两个典型问题上优于既有的技术，展示了预测导向的采样优势，但也指出实现良好效果需细致模型设计。

Conclusion: 预测导向的智能采样在数据流学习中具有潜力，但实现效果依赖于模型设计的合理性。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [86] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 通过引入SICKLE框架和MaxEnt采样方法，有效减少数据量，提高模型准确性并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和丹纳德尺度的终结，提升训练效率需重新考虑数据规模，探索少量数据的高效利用。

Method: 开发SICKLE框架，结合最大熵采样、可扩展训练与能耗基准，为大规模湍流数值模拟数据进行智能采样。

Result: 在大规模湍流模拟数据集和Frontier超级计算机上验证，子采样作为预处理，显著提升模型准确性，且能耗降低最多达38倍。

Conclusion: 通过智能子采样，能在保证甚至提升模型性能的同时，极大降低训练成本和能源消耗，有助未来大规模科学计算的可持续发展。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [87] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出一种结合脉冲控制和切换控制的强化学习框架，用于糖尿病管理的模拟，实现了在血糖调控中的显著改善。


<details>
  <summary>Details</summary>
Motivation: 优化慢性病管理中的治疗决策，特别是1型糖尿病的血糖控制。

Method: 构建结合生理状态特征的约束马尔可夫决策过程，整合脉冲和切换控制，考虑生物学因素如胰岛素衰减。

Result: 在模拟任务中显著降低血糖异常发生率，从22.4%降至10.8%。

Conclusion: 为未来在医疗中的安全、时序感强的强化学习应用奠定基础，提供理论保证和实证效果。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [88] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 提出了一种结合多任务学习与循环神经网络的混合模型，用于改进葡萄物候期预测，显著优于传统模型与基线深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 提高葡萄物候期预测的精度以辅助精准葡萄园管理。

Method: 采用多任务学习结合递归神经网络参数化可微分的生物物理模型。

Result: 新方法在真实与合成数据集上均显著优于传统模型和基线深度学习模型，提升了物候期及其他参数的预测准确性。

Conclusion: 混合模型通过共享学习与保留生物结构，有效增强了葡萄物候预测的鲁棒性与精度。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [89] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文提出在距离基分类器中发现潜在神经网络结构，利用可解释AI技术提升模型解释能力，并通过实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 提升距离基分类器的可解释性，满足实际应用中对模型透明度的需求。

Method: 揭示距离基分类器中的隐藏神经网络结构，结合层次相关传播（LRP）等可解释AI技术进行解释。

Result: 所提出的方法在多个基线方法中表现优越，且在实际案例中证明其实用性。

Conclusion: 发现并利用距离基分类器中的潜在神经网络结构，有助于增强模型的可解释性，推动其在科学与工业中的应用。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [90] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 主动学习与迁移学习结合在跨域时间序列异常检测中的效果有限，性能提升逐渐趋于饱和。


<details>
  <summary>Details</summary>
Motivation: 探索主动学习与迁移学习结合在跨域时间序列异常检测中的效果表现。

Method: 通过不同实验设计，比较了无聚类、单一聚类、以及不同数据样本的采样与测试方法，评估模型性能。

Result: 发现单一聚类能获得最佳效果，主动学习能提升性能但提升速率较慢，迁移学习效果在一定程度内改善后趋于平稳。

Conclusion: 主动学习有效，但性能提升受限，呈线性趋平趋势，未来优化空间存在。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [91] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 本文提出一种结合流形学习与机器学习的多尺度人群动态模拟方法，可高效准确地从微观模拟中学习宏观模型，具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 弥合微观与宏观模型在群体动力学中的尺度差异，提升模拟的效率与准确度。

Method: 结合核密度估计、流形学习、LSTM和MVAR网络，构建多尺度动力学模型的学习与预测框架。

Result: 模型在基于社会力模型的仿真中表现出高精度、鲁棒性和泛化能力，支持快速模拟。

Conclusion: 提出的多尺度动态建模框架有效链接微观与宏观尺度，为群体动力学模拟提供强有力工具。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [92] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 探讨变换器在仅通过下一词预测训练下，学习涉及其上下文的算法能力，发现模型规模和训练集规模达到一定阈值后，能有效估计转移概率。


<details>
  <summary>Details</summary>
Motivation: 理解变换器在仅通过下一词预测训练中的算法学习能力及其对不同结构的泛化能力。

Method: 在Markov链中使用随机转移矩阵，训练变换器预测下一词，测试不同矩阵和编码策略。

Result: 模型在特定规模阈值后能从上下文中估算转移概率，增强编码促进对不同结构的鲁棒性。

Conclusion: 变换器可以通过规模和编码策略实现对复杂算法的学习和泛化，超越记忆局限。

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [93] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出了一种基于最优运输的后处理框架FairPOT，通过选择性调整风险分布，实现了在不显著影响AUC性能的前提下改善公平性，适用于高风险领域。


<details>
  <summary>Details</summary>
Motivation: 在高风险行业中，公平指标通常以AUC为基础，但严格公平限制会损害模型性能。

Method: 利用最优运输策略在受不利群体中选择性调整部分分数，控制公平与性能的权衡，扩展到部分AUC以集中高风险区域。

Result: 在多类数据集上，FairPOT优于现有方法，既改善 fairness，又保持或提升AUC表现，操作高效，实用性强。

Conclusion: FairPOT是一种有效、灵活且实用的公平性改善工具，适合实际应用环境。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [94] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 本文提出BubbleONet，一种基于物理感知深度算子网络的气泡动力学模拟模型，通过引入自适应激活函数，有效改善高频特征的表达，显著提升模拟效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统气泡动力学模拟依赖数值解算，计算复杂且效率低下。需要快速且准确的代理模型以加速相关工程和科研应用。

Method: 基于PI-DeepONet框架，结合Rowdy激活函数，设计BubbleONet，系统评估在不同气泡动力学方程和多初始半径条件下的性能表现，并比较单步与两步训练策略。

Result: BubbleONet在多个气泡动力学场景中表现优异，成为高效、精准的仿真替代方案，减少计算成本。

Conclusion: BubbleONet结合物理信息和深度学习，在气泡动力学模拟领域展现出强大潜能，为相关科研和工业应用提供有力工具。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [95] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP是一种面向用户的动态隐私保护框架，利用对比学习和语言引导生成技术，有效隐藏敏感活动，保持数据实用性。


<details>
  <summary>Details</summary>
Motivation: 随着IMU传感器广泛应用于个人设备，用户隐私保护日益重要，但现有方法难以动态、灵活满足用户个性化隐私需求。

Method: 采用多模态对比学习将IMU数据与自然语言描述对齐，通过少样本学习识别敏感活动，并利用语言引导的运动数据变换模块（IMU-GPT）隐私保护。

Result: 在多个人体活动识别数据集上验证，PrivCLIP显著优于基线方法，兼顾隐私保护和数据实用性。

Conclusion: PrivCLIP提供了一种灵活、可定制的隐私保护方案，有助于提升传感器数据的用户控制和应用安全性。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [96] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: 提出一种基于张量聚类的LoRA适配器，解决多任务训练中的任务干扰问题，效果优于SVD基线。


<details>
  <summary>Details</summary>
Motivation: 多任务环境下，合并训练的LoRA适配器会导致任务干扰，影响模型性能。

Method: 在文本层面对输入样本进行聚类，训练对应的LoRA适配器；在参数层面引入联合CP分解，分离任务特定与共享因子，减少干扰。

Result: 在复杂任务和零样本任务中，显著优于SVD基础方法，提升准确率。

Conclusion: 通过文本级别的样本聚类和参数级别的联合张量分解，有效缓解了多任务多源训练中的干扰，提升LLM的适应能力。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [97] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: DCFL通过解耦对比损失中的匹配和多样性目标，有效提升联邦学习中的模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中对比学习由于有限样本导致的负样本不足问题。

Method: 将对比损失解耦为匹配与多样性两部分，分别调节，避免依赖无限负样本的假设。

Result: 在多个标准数据集上优于现有方法，实现更好的样本匹配与分散。

Conclusion: 解耦策略使对比学习更适合样本有限的联邦学习场景，增强模型表现。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [98] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 本文比较分析了TensorFlow和PyTorch两个深度学习框架，涵盖其编程范式、性能、部署与社区生态，强调各自优势与取舍。


<details>
  <summary>Details</summary>
Motivation: 旨在帮助研究者和开发者理解两大框架的特点和适用场景，优化工具选择。

Method: 通过文献综述、性能基准测试、应用实例分析和未来趋势展望进行比较。

Result: 分析显示PyTorch偏向研究和灵活性，TensorFlow更适合全面生产部署，各有优势与局限。

Conclusion: 理解两者的差异有助于更好地选择合适的工具，应关注框架的易用性、性能和生态支持等因素。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [99] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的时间序列基础模型训练新方法FeDaL，有效缓解数据集异质性带来的偏差，提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列基础模型在面对来自不同数据集的异质性带来泛化性能下降的问题。

Method: 采用联邦学习架构，通过区域偏差消除和全局偏差消除机制，实现跨数据集的时间表示学习。

Result: 在多个真实场景的八个任务中，优于54个基线模型，并分析了数据量、客户端数量等因素的影响。

Conclusion: FeDaL有效提升时间序列模型的跨数据集适应性，提出了一种新的应对异质性偏差的训练策略。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [100] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: 提出一种量子增强版的时间融合变换器（QTFT），在多时序预测中表现优越，适用于当前NISQ设备。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算与深度学习，提升多时序预测模型的性能。

Method: 基于变分量子算法，构建混合量子-经典架构，扩展经典TFT。

Result: 在部分测试案例中优于经典模型，整体表现良好，且可在噪声中等、比特数和线路深度有一定限制的量子设备上实现。

Conclusion: 量子增强的时序预测模型具有潜力，可在未来量子硬件发展中进一步提升性能。

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [101] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 微调在少样本短答评分中的效果有限，但在某些情况下如利用合成数据可以显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 探索不同微调方法在自动短答评分中的效果，特别是在有限数据和不同模型架构下的表现。

Method: 比较OpenAI闭源模型的微调服务与开源模型（如LLama、QLORA）在少样本条件下的性能，通过结构化输出评估效果。

Result: 微调对LLama模型的提升有限，但在OpenAI闭源模型中表现优于少样本提示操作；合成训练数据可显著改善LLama 3.1模型表现。

Conclusion: 微调方法在短答评分中具有潜力，但效果依赖于模型类型及数据源。合成数据的引入是提升性能的有效途径。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [102] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 提出FLAT新型后门攻击，通过潜在编码生成多样化、目标特定的触发器，提升攻击多样性与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 应对现有FL后门攻击受限于固定触发器模式的局限性，提升攻击的多样性和隐蔽性。

Method: 使用潜在驱动的条件自编码器生成不同目标的触发器，实现攻击的多样化和高适应性。

Result: FLAT在攻击成功率和隐蔽性方面表现优异，能有效规避现有防御手段，彰显攻击威胁。

Conclusion: 需要发展新的防御策略以应对基于潜在驱动、多目标的FL后门攻击，确保联邦学习的安全性。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [103] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出一种引入对抗训练的多视角公平聚类框架，有效减少敏感属性影响，兼顾公平性与聚类性能。


<details>
  <summary>Details</summary>
Motivation: 解决多视角聚类中公平性被忽视的问题，确保聚类结果不受敏感属性干扰。

Method: 采用对抗训练，去除特征中的敏感属性信息，并在理论上证明了保持聚类一致性的方法。

Result: 在多个数据集上实验显示，该方法在公平性和聚类性能方面优于现有方法。

Conclusion: 该方法有效融合公平性与聚类性能，为多视角聚类提供新的解决方案。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [104] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 本文首次研究了视觉-语言模型(VLMs)的模型反演攻击，发现它们存在泄露训练数据的严重隐私风险。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs在医疗、金融等领域的广泛应用，其隐私安全成为迫切关注的问题，但相关的模型反演攻击研究尚不充分。

Method: 提出多种基于Token和序列的模型反演策略，包括TMI、TMI-C、SMI和SMI-AW，并利用对抗损失和词汇表表示提升攻击效果。

Result: 实验显示，序列化方法尤其是SMI-AW在攻击准确率和视觉相似度方面优越，人工评估达75.31%的攻击成功率，首次验证了VLMs的隐私脆弱性。

Conclusion: VLMs的隐私安全需引起重视，未来应加强模型防护措施，确保敏感信息的安全。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [105] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 提出一种一致性感知的策略优化框架，有效解决模型输出高度一致导致的优势消失问题，提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在复杂推理任务中的学习效率和性能。

Method: 引入基于结果一致性的结构化奖励和全局损失，结合熵调节机制，实现探索与收敛的动态平衡。

Result: 在多个数学推理基准上取得显著性能提升，验证方法的有效性和鲁棒性。

Conclusion: 该框架通过改进奖励设计和优化策略，增强模型在一致性和推理能力方面的表现，具有广泛适用性。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [106] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 提出了一种针对不同气象区域的地理迁移的半监督深度域适应框架，用于提升太阳能发电预测的准确性，减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决不同地区气候和地理特征引起的模型性能下降问题，改善跨地区太阳能预测的泛化能力。

Method: 采用源无关的教师-学生模型，利用半监督学习方法在少量目标区域标注数据基础上进行模型适应，实现地理迁移。

Result: 在加利福尼亚、佛罗里达和纽约三个区域测试中，模型在目标区域仅用20%标注数据，准确率提升分别为11.36%、6.65%、4.92%，优于非适应模型。

Conclusion: 通过引入半监督深度域适应策略，有效缓解了地理迁移中的域偏差问题，提高了不同地区太阳能预测的精度。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [107] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 通过预训练的MIST编码器和MolForge解码器的结合，显著提升了从质谱数据进行新分子生成的准确性和效率，达到十倍的性能提升，构建了未来研究的强大基线。


<details>
  <summary>Details</summary>
Motivation: 解决质谱数据中从分子指纹到分子结构的准确生成问题，提升新分子结构的预测效果。

Method: 采用MIST编码器和MolForge解码器，利用预训练技术，通过阈值处理优化结构恢复，结合两者实现高效解码。

Result: 实现了性能的十倍提升，Top-1正确率28%，Top-10正确率36%，优于现有方法。

Conclusion: 该组合架构为从质谱数据中进行新分子结构解析提供了有效且具有竞争力的基线，为未来相关研究奠定基础。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [108] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的深度学习网络训练方法SAMT，通过层块交替优化结合可训练步长策略，提升训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中非凸优化带来的不稳定和高计算成本问题。

Method: 将网络参数分块交替优化，结合元学习设计可训练的自适应步长策略，理论保证收敛性。

Result: 在多个基准测试中，SAMT表现优于现有方法，具有更好的泛化能力和训练效率。

Conclusion: SAMT结合交替优化和可训练步长技术，有望成为深度网络优化的有效工具。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [109] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 提出因果奖励调整（CRA）方法，有效缓解外部推理系统中的奖励操纵问题，提高数学解题的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决外部推理系统中奖励操纵导致的错误问题，确保高分路径真实合理。

Method: 通过训练稀疏自编码器恢复PRM内部激活的可解释特征，利用因果推断中的后门调整校正奖励偏差。

Result: CRA显著减缓奖励操纵，提高了数学解题的准确性，无需改动策略模型或重训PRM。

Conclusion: 引入因果推断机制优化奖励评估，有效提升复杂任务中的推理质量与可靠性。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [110] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 引入对称散度到行为正则化策略优化，提出新颖的离线强化学习框架S$f$-AC，通过泰勒展开解决对称散度的数值和分析问题，性能优越。


<details>
  <summary>Details</summary>
Motivation: 改进离线强化学习的正则化策略，探索对称散度的潜力，以克服非对称散度的局限。

Method: 采用$f$-散度的泰勒级数展开，将对称散度分解为偏差和条件对称项，从而实现分析性策略和稳定数值优化。

Result: 提出S$f$-AC算法，在分布逼近和MuJoCo任务中表现出竞争力，验证了方法的有效性。

Conclusion: 通过泰勒展开，成功克服对称散度在离线RL中的分析和数值挑战，开辟了对称散度在强化学习中的应用新方向。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [111] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: 提出DCATS，利用元数据提升时间序列数据质量，实现AutoML中模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索在时间序列预测中通过改善数据质量而非模型架构提升性能的可能性。

Method: 设计DCATS，一个利用时间序列元数据进行数据清洗的智能代理，优化预测效果。

Result: 在多模型多时间窗口下，平均提升6%的预测精度，验证数据质量改善的有效性。

Conclusion: 数据中心化的方法在AutoML时间序列预测中具有巨大潜力，比单纯优化模型架构更具实际价值。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [112] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 本文提出一种基于深度学习的自动多普勒角度估算方法，有望提高血流速度测量的准确性并应用于临床自动化设备。


<details>
  <summary>Details</summary>
Motivation: 多普勒超声血流量测量中，角度估算误差是主要的测量误差，亟需改进入一步自动化和精确化。

Method: 该方法利用五种预训练模型提取图像特征，再通过定制浅层网络进行角度预测，使用2100张人类颈动脉超声图像和图像增强技术。

Result: 模型的平均绝对误差（MAE）最低至3.9度，优于临床可接受误差阈值，有助于避免正常血流误判。

Conclusion: 深度学习技术在超声多普勒角度自动估算方面展现出潜力，未来可集成于商用超声设备中以提升诊断效率与准确性。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [113] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time是一种结合时间、频谱和提示的信息融合框架，有效提升多变量时间序列预测的性能，特别在少量数据条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型在捕捉变量间关系和适应不同预测范围方面存在局限，影响预测效果。

Method: 提出结合时间、频谱和提示的三模态框架，通过门控机制和动态加权实现多模态信息的有效融合。

Result: 模型在多个基准数据集上优于现有方法，且在少样本学习中表现出良好的泛化能力。

Conclusion: T3Time通过多模态融合解决了现有模型的局限性，提升了多变量时间序列预测的准确性和适应性。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [114] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一款基于Python的可视化工具，用于全面理解机器学习模型的敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 提升用户理解和模型调优的效率，支持非编程用户进行模型配置、训练和解释。

Method: 集成局部和全局敏感性分析，结合LIME和SHAP提供模型解释，支持图形交互界面。

Result: 在泰坦尼克数据集上成功应用，帮助优化特征选择和数据改进，验证了工具的实用性。

Conclusion: SAInT简化了模型敏感性分析流程，促进模型透明化和优化。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [115] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 提出Mockingbird框架，将大型语言模型应用于通用机器学习任务，通过角色扮演和自我反思改进，但目前仍需人类专业反馈。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在超越聊天机器人，应用于通用机器学习任务中的潜力。

Method: 设计并评估了Mockingbird框架，让LLMs扮演不同角色并反思自己的错误，提高性能。

Result: Mockingbird能在常见机器学习任务中取得合理效果，但自我反思尚不足以替代专业人类反馈。

Conclusion: 基于LLMs的机器学习方法具有潜力，但仍需结合专业反馈以实现更优性能。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [116] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 引入一种新的轻量级强化学习算法VL-DAC，显著提升视觉-语言模型在多模态交互任务中的泛化能力和执行效率。


<details>
  <summary>Details</summary>
Motivation: 弥补现有VLMs在将视觉观察转化为语言驱动动作方面的局限，尤其是在泛化能力和环境适应性方面的不足。

Method: 提出VL-DAC算法，结合PPO更新机制，实时学习环境价值，简化训练过程，避免复杂超参数调试。

Result: 在多个虚拟仿真环境中训练的单一VLM模型表现出显著的泛化能力提升，在游戏控制、空间规划和网页导航任务中均取得优异成绩，且不影响图像理解能力。

Conclusion: VL-DAC证明了用简单强化学习方法在低成本虚拟环境中训练VLM，不仅具备实际应用潜力，还能带来实质性能提升，开启多模态智能交互的新可能。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [117] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出一种基于weight saliency的两阶段高效机器遗忘方法WSS-CL，显著提升图像分类中遗忘效果，接近精确遗忘水平。


<details>
  <summary>Details</summary>
Motivation: 解决当前机器遗忘方法在遗忘精准性、稳定性和跨域适应性方面的不足。

Method: 结合遗忘阶段和对抗微调阶段，通过最大化logit空间和特征空间的距离实现遗忘。

Result: 在实验中表现出优异的遗忘效果，性能损失极小，优于现有方法，适用于监督和自监督任务。

Conclusion: 提出的方法有效提升机器遗忘的效率和效果，为图像分类中的遗忘任务提供了新思路。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [118] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 提出一种通过区分正负标记优化微调策略的方法，提升大规模预训练模型在特定领域的性能和响应多样性。


<details>
  <summary>Details</summary>
Motivation: 解决微调过程中依赖数据质量与规模的问题，减少无用或误导信息对模型的影响。

Method: 将训练语料中的标记划分为正向和负向两类，负向标记进行“遗忘”，从而引导模型学到更有用的信息。

Result: 实验表明，该方法提升了模型的整体性能和回答的多样性。

Conclusion: 通过显式遗忘无关或误导信息，有效增强模型的学习效果和表达能力。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [119] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS提出一种分布式特征共享的私有推断方法，能在保证隐私的同时极大降低计算负担，并通过扩展增强抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 解决云端机器学习中隐私保护和效率之间的矛盾。

Method: 通过在客户端将特征分割成多个安全分享，分发到多个非合作服务器进行局部推断，最终客户端合并结果。引入对抗训练和用户密钥进行扩展以提升隐私性。

Result: 在CIFAR-10和CelebA数据集上，PrivDFS在保持隐私的同时，计算效率比深度拆分推断提升100倍，无性能损失，扩展方案抗多种攻击。

Conclusion: PrivDFS通过分布式特征共享实现高效安全的私有推断，结合扩展技术进一步增强隐私保护，具有实用潜力。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [120] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出了一种新的多边缘随机流匹配方法（MMSFM），用于在非均匀时间点上模拟高维系统的演化，避免降维并增强对不规则采样的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在生物学等领域，有限的快照观测下模拟高维系统演化是一个重要难题，传统降维方法可能错失关键信息。

Method: 扩展了无模拟的得分和流匹配技术至多边缘设置，通过度量值样条和得分匹配提高鲁棒性和防止过拟合。

Result: 在合成和基准数据集以及基因表达和图像演变任务中验证了该方法的有效性，展示了其多样性和实用性。

Conclusion: 多边缘随机流匹配提供了一种无需降维、对不规则时间点具有鲁棒性的新工具，适用于高维动态系统的建模。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [121] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出了一种专为多实例学习（MIL）设计的持续学习方法，可有效应对类变化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着生命科学实验室和临床环境中数据的不断增长，训练模型以保持性能成为挑战。现有持续学习方法在多实例学习中效果有限，需要专门的解决方案。

Method: 采用基于回放的多实例选择策略，结合实例注意分数和距离指标，精心选择存储的实例，保持数据多样性。

Result: 在真实的白血病数据中，该方法在类增量场景中优于现有持续学习方法，首次实现了适用于Mil的持续学习。

Conclusion: 该方法实现了MIL下模型的持续适应，有助于应对疾病变化和遗传变异带来的数据分布变化。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [122] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种结合算法创新与系统优化的INT6后训练量化框架，显著提高大型语言模型的推理效率和节省内存，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型部署中高内存和计算成本问题，寻找精准且高效的量化方法。

Method: 采用统一的6比特权重量化和层敏感性分析进行选择性8比特激活，开发支持W6A6和W6A8的GPU高性能核实现硬件加速。

Result: 在LLaMA模型上实现接近FP16的精度，推理速度提升1.39倍，整体推理加速1.33倍，内存节省1.21倍。

Conclusion: FlexQ通过创新的量化策略和系统级优化，有效弥补了INT6硬件支持的不足，实现了高效、精确的模型推理加速。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [123] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 该综述分析了2020年至2024年早期关于多模态模型可解释性的研究，发现大部分关注视觉-语言和单语模型，使用注意力机制，但在多模态交互解析和评价方法上存在不足，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着多模态学习和注意力模型的发展，提升模型的可解释性成为亟需解决的问题，推动可解释AI的研究越来越重要。

Method: 系统性文献综述2020年至2024年早期发表的多模态模型可解释性相关研究，分析模型架构、模态、解释算法与评价方法。

Result: 大部分研究集中在视觉-语言及单语模型，采用注意力机制作为解释工具，但多模态交互理解不足，评价体系不完善，缺乏系统性。

Conclusion: 建议建立规范的评价和报告标准，推动透明、稳健、多模态可解释性研究，促进构建更具解释性和责任感的多模态AI系统。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [124] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了用于估算矩阵非必要范数的新随机算法，能在无矩阵存储的情况下，只通过矩阵-向量乘法实现，适用于深度学习和推荐系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决在无需存储完整矩阵的情况下，估算矩阵范数的问题，以提高计算效率和应用广泛性。

Method: 基于Hutchinson方法及Hutch++的改进，通过修改算法实现相关范数的估算，提供了复杂度分析。

Result: 算法在深度神经网络正则化和推荐系统中的对抗攻击抵抗方面表现出实际效用。

Conclusion: 新的随机算法在矩阵范数估算中具有理论保证和实际应用潜力，可广泛用于大规模在线学习与模型正则化领域。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [125] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: 提出了一种结合云模型特征函数的变分自编码器（CMCFAE），通过引入云模型优先级改善潜在空间表达，提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 改进传统WAE在复杂分布建模中的局限性，提供更真实的潜在空间表示。

Method: 引入云模型特征函数作为正则项，结合WAE框架，优化潜在空间。

Result: 在多个数据集上验证，CMCFAE在重建质量、潜在空间结构和样本多样性方面优于现有模型。

Conclusion: 云模型与自编码器结合开辟新途径，有助于提升生成模型的性能。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [126] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 提出一种通过强化学习训练AI主动攻击另一个AI的红队方法，有效揭示LLMs的隐性漏洞，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 弥补现有自动化红队方法在捕捉复杂对抗对话中的不足，推动MLL安全测试的进步。

Method: 将红队设定为马尔可夫决策过程，采用层次化强化学习框架，使智能体学习多轮攻击策略，基于逐字粒度的奖励优化攻击效果。

Result: 新方法达到了最先进效果，展示了多轮、动态、轨迹驱动的红队测试策略优于传统单轮或模板化方法。

Conclusion: 将LLM红队转变为基于轨迹的动态过程，为构建更安全、更鲁棒的AI奠定基础。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [127] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 本文探索了在任务切换场景中，注意力机制在小规模应用中的表现，并比较了不同模型的效果，发现结合不同注意力机制的模型能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 理解注意力机制在任务切换中的表现及其优势，提升小规模任务中的模型效果。

Method: 通过设计基于任务切换的参考模型，测试标准变压器、LSTM、MLP、cisformer和扩展注意力机制的模型性能，并进行比较。

Result: 发现普通变压器在任务切换中的表现有限，结合扩展的注意力机制的方法能达到约95%的准确率，显著优于其他模型。

Conclusion: 不同注意力机制的结合可以改善模型在任务切换中的效果，为理解和优化注意力机制提供了新思路。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [128] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出一种基于缓存的并行投机式解码框架，解耦起草与验证过程，提高推断速度。


<details>
  <summary>Details</summary>
Motivation: 现有的投机式解码在执行效率和模型规模方面存在限制，需改善解码效率和模型兼容性。

Method: 引入缓存机制和‘查询-修正’范式，将起草与验证过程解耦，实现并行化。

Result: 在无需微调模型的情况下，解码速度提升最大4.83倍，提升显著。

Conclusion: 提出的方法有效改善投机式解码的效率和适用性，为大模型推断提供更高效的解决方案。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [129] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal是一种基于Transformer的神经算子方法，旨在同时学习和融合局部细节与全局特征，有效解决多尺度物理问题，表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多尺度物理问题中局部细节与全局特征协调学习的问题，提升模拟的物理一致性和数值稳定性。

Method: 引入Nyström注意力的全局块和切片焦点块，结合卷积门控实现多尺度信息的动态融合。

Result: 在六个基准测试中平均提升15.2%，在工业场景如汽车和翼型空气动力模拟中表现优异，达到最新水平。

Conclusion: GFocal通过同时学习全局和局部特征，有效增强了物理特征建模能力，适用于复杂的多尺度物理模拟。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [130] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: FedHiP通过分析性（闭式解）方法，避免梯度更新，解决非IID数据导致的个性化联邦学习中的数据异质性问题，显著提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中数据异质性带来的性能下降问题。

Method: 采用自监督预训练的基础模型作为特征提取器，结合分析性分类器，实现无梯度训练的三阶段方案。

Result: 在多个基准数据集上，FedHiP显著优于现有方法，准确率提升至少5.79%-20.97%。

Conclusion: 分析性方案确保模型对非IID数据的鲁棒性，推动个性化联邦学习的发展。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [131] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 能效干预在不同家庭中的效果存在差异，低能耗家庭节能显著，而高能耗家庭反应有限，需考虑公平性和健康益处。


<details>
  <summary>Details</summary>
Motivation: 降低家庭能耗对气候减缓和能源贫困策略至关重要，但干预效果多样，亟需理解不同家庭的反应机制。

Method: 利用面向英国住房数据的因果机器学习模型，分析墙体绝缘对燃气消耗的影响及其分布特征。

Result: 平均来看墙体绝缘能减少燃气需求（最高达19%），低能耗家庭获益明显，高能耗家庭反应有限。这种差异由行为机制驱动，高能耗家庭优先改善热舒适而非节能。

Conclusion: 应建立考虑气候影响与公平性的新评估框架，认识到家庭反应多样性，有助制定更有效的能源政策。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [132] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 利用条件生成对抗网络提升多模态情感识别效果，增强人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 传统单模态情感识别受限于数据单一，难以捕捉复杂情感信息。

Method: 采用多模态框架结合cGANs，生成情感丰富的合成数据以提升分类性能。

Result: 在多个模态上实现明显性能提升，优于基线模型。

Conclusion: cGANs在多模态情感识别中具有巨大潜力，有助于增强人机交流的细腻度。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [133] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 提出了一种基于层次评分树的多层次评分指标，用于更细粒度地评估机器学习模型的分类和目标检测性能，考虑错误的类别关系和影响。


<details>
  <summary>Details</summary>
Motivation: 传统的准确率评价方法忽略了类别间的关系，无法反映不同错误类型对模型性能的影响。

Method: 开发了多种层次评分指标，利用评分树编码类别关系，根据距离赋予不同的分值，从而实现更细粒度的评价。

Result: 在抽象用例中验证了该方法的效果，发现其能捕捉更细腻的错误信息，并通过调节评分树提升评价的可调性。

Conclusion: 提出的方法为机器学习模型的性能评价提供了更丰富的指标，有助于理解模型在不同错误类别上的表现差异。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [134] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 提出Causal Reflection框架，使模型能更好理解因果关系，适应变化环境。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型和强化学习均在因果推理方面存在挑战，希望提升其因果理解能力。

Method: 引入基于状态、动作、时间和扰动的动态因果模型，设计反思机制识别模型偏差并生成假设，利用结构化推理转换为自然语言解释。

Result: 提供了理论基础，促进因果推理能力，自我改正与沟通，为未来智能体的发展奠定基础。

Conclusion: Causal Reflection架构增强模型的因果推理和适应能力，推动智能体向更具因果理解的方向发展。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [135] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM是一种基于卷积的多尺度、多通道频率选择性特征提取器，有效提升多变量时间序列分类的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer和CNN模型在多变量时间序列分类中的高计算成本和频率多样性不足的问题。

Method: 引入多尺度逐通道对称FIR滤波器，通过不同行为的频率筛选优化特征表示，减少模型参数和复杂度。

Result: 在多个基准数据集上，PRISM与轻量级分类器结合，性能不输甚至优于主流模型，同时参数和计算量大幅降低。

Conclusion: 结合经典信号处理和深度学习，PRISM提供了一种高效、准确的多变量时间序列分类方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [136] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 提出基于信道无关范式（CIP）的联邦交通预测方法，有效减少通信开销，提升效率，并保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决交通数据分散且隐私限制导致无法直接利用多源数据进行预测的问题，提升交通预测的效率和隐私保护能力。

Method: 引入变量关系建模范式（CIP），并设计Fed-CI框架，实现各节点独立处理本地数据，减少通信需求。

Result: Fed-CI显著降低通信成本，提高训练速度，在多个真实数据集上优于现有方法，RMSE、MAE、MAPE指标分别提升8%、14%和16%。

Conclusion: 提出的CIP范式与Fed-CI框架在保持预测性能的同时，有效缓解了通信瓶颈，具备良好的应用前景。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [137] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 本文通过构建身份生态系统图，结合图神经网络模型，预测个人信息披露的隐私风险，有效回答了某一信息是否可能引发其他信息泄露的问题。


<details>
  <summary>Details</summary>
Motivation: 帮助个人和组织理解不同个人信息披露的隐私风险，提供风险预测工具。

Method: 分析超过5000个身份盗窃和欺诈案例，构建基于图的模型，并利用图神经网络进行风险预测。

Result: 提出的模型能有效预测信息泄露可能引发的其他信息泄露的概率。

Conclusion: 采用图结构和深度学习技术的风险预测框架，增强了个人信息保护的能力。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [138] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 本文提出GraphProp，一种强调结构泛化的图基础模型训练方法，通过预测图不变量增强模型的跨域结构信息提取能力，提升图分类任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的图基础模型主要关注节点特征的跨域迁移，忽视了图结构在多域场景中的一致性，使模型在跨域任务中表现有限。

Method: 方法分两个阶段：第一阶段，训练结构模型预测图不变量，捕获抽象结构信息；第二阶段，利用结构模型的表示作为位置编码，结合节点属性和标签训练完善模型。

Result: 实验显示，GraphProp在监督学习和少样本学习中显著优于竞争者，尤其在无节点属性的图上表现优越。

Conclusion: 本研究通过结构导向的训练策略，有效增强图模型的跨域结构理解能力，提升其在多域任务中的泛化性能。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [139] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 提出了一种结合新型训练策略的物理信息神经网络，用于非侵入式预测铝焊接中的动态位移和焊核尺寸，以提高工业质量控制的效率。


<details>
  <summary>Details</summary>
Motivation: 传统焊点检测依赖破坏性测试，限制了质量控制的效率和普及。

Method: 设计两种新颖的训练方法，包括逐步引入损失函数和条件性参数更新，结合二维模型实现高效预测。

Result: 模型成功预测焊接过程中的动态参数，支持钢铝转用，展现出工业应用的潜力。

Conclusion: 该方法实现了非侵入式、快速的焊接质量评估，有助于工业自动化提升。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [140] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出了一种基于算子插值的通用概率分布映射框架，扩展了流和扩散模型，具备多任务能力且无需任务特定训练。


<details>
  <summary>Details</summary>
Motivation: 旨在通过泛化时间插值方法，创建能跨多个空间维度构建概率分布映射的通用生成模型。

Method: 引入线性算子替代标量时间变量，构建算子插值以桥接多维空间中的概率分布，理论上统一并拓展了现有生成模型。

Result: 数值实验显示该方法在无任务调优的多任务场景（如条件生成、修复、后验采样、多尺度建模）中具有零样本效果。

Conclusion: 该框架具有广泛适用性，有望成为任务无关的通用生成模型替代方案。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [141] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出一种基于脉冲神经网络（SNN）的持续网络入侵检测系统，结合静态和动态模型实现高效识别与分类，具有低功耗潜力。


<details>
  <summary>Details</summary>
Motivation: 借鉴大脑的层级处理和能量效率，提升入侵检测系统的性能和持续学习能力。

Method: 采用静态和动态SNN相结合的架构，动态部分引入GWR和Ad-STDP机制实现增量学习。

Result: 在UNSW-NB15数据集上实现了85.3%的准确率，表现出优异的适应性和低灾难性遗忘，验证了低功耗潜力。

Conclusion: 该SNN架构具有强大的持续学习能力和能效潜力，适合 neuromorphic 硬件部署。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [142] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 提出了一种基于因果关系的时间序列异常检测方法CaPulse，结合结构因果模型和周期性归一化流，有效提升检测效果和解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以揭示异常生成背后的因果机制，并面临数据标记稀缺、数据不平衡与多周期性等挑战。

Method: 构建结构因果模型，设计周期性归一化流和掩码机制，结合因果与周期性特征进行异常检测。

Result: 在七个实际数据集上，CaPulse显著优于现有方法，AUROC提升3%~17%，且具有更强的解释性。

Conclusion: 引入因果分析与周期性机制，有效提升时间序列异常检测的性能与可解释性。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [143] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于预训练的图神经网络LP方法，通过模块传递性、多专家框架和参数高效调优，有效提升低资源和异构数据场景下的性能，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决图链接预测中监督信号有限、对初始化敏感、泛化能力差的问题，探讨预训练作为解决方案。

Method: 设计多模块迁移性研究、晚融合策略、Mixture-of-Experts框架以及参数高效调优策略，增强模型适应性与性能。

Result: 在16个多域数据集上取得了最先进的性能，尤其在低资源环境下表现出优越的效果，计算成本显著低于端到端训练方法。

Conclusion: 预训练模型结合灵活架构和高效调优策略，为图链接预测提供强大解决方案，提升了泛化和适应能力。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [144] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [145] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 提出了一种有效算法，用于在高斯噪声下学习单指数模型，适用于所有单调激活函数，包括有界矩和非连续激活。


<details>
  <summary>Details</summary>
Motivation: 解决在高斯噪声和对抗性标记噪声条件下，学习单指数模型的难题，尤其是在未知激活函数的情况下。

Method: 开发了一种超越传统梯度方法的优化框架，利用高斯空间特性和单调函数的性质，设计了引导算法更新的矢量场。

Result: 首次实现对所有单调激活函数（包括非连续函数和有界矩矩阵）在该场景下的常数因子近似，不同于前人工作中只适用于部分激活。

Conclusion: 该方法突破了传统梯度策略的局限，为在复杂噪声环境下学习非线性模型提供了新思路。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [146] [Suggest, Complement, Inspire: Story of Two Tower Recommendations at Allegro.com](https://arxiv.org/abs/2508.03702)
*Aleksandra Osowska-Kurczab,Klaudia Nazarko,Mateusz Marzec,Lidia Wojciechowska,Eliška Kremeňová*

Main category: cs.IR

TL;DR: 本文介绍了在欧洲最大电商平台Allegro.com部署的统一内容推荐系统，采用双塔模型，通过文本和结构化属性实现多任务适应，显著提升用户参与度和利润。


<details>
  <summary>Details</summary>
Motivation: 解决大规模电商推荐系统面临的架构统一、维护成本高、产品目录动态变化大的挑战。

Method: 采用双塔模型，利用近似最近邻搜索，实现多任务推荐的灵活架构。通过调整模型和逻辑，支持相似搜索、搭配推荐和内容启发式探索。

Result: 系统在两年A/B测试中显著提升用户参与和盈利指标，验证了其灵活性和高效性。

Conclusion: 基于多任务、多模型少修改的架构能有效应对电商推荐系统的多样需求，降低维护成本，提高性能。

Abstract: Building large-scale e-commerce recommendation systems requires addressing
three key technical challenges: (1) designing a universal recommendation
architecture across dozens of placements, (2) decreasing excessive maintenance
costs, and (3) managing a highly dynamic product catalogue. This paper presents
a unified content-based recommendation system deployed at Allegro.com, the
largest e-commerce platform of European origin. The system is built on a
prevalent Two Tower retrieval framework, representing products using textual
and structured attributes, which enables efficient retrieval via Approximate
Nearest Neighbour search. We demonstrate how the same model architecture can be
adapted to serve three distinct recommendation tasks: similarity search,
complementary product suggestions, and inspirational content discovery, by
modifying only a handful of components in either the model or the serving
logic. Extensive A/B testing over two years confirms significant gains in
engagement and profit-based metrics across desktop and mobile app channels. Our
results show that a flexible, scalable architecture can serve diverse user
intents with minimal maintenance overhead.

</details>


### [147] [Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective](https://arxiv.org/abs/2508.03703)
*Yubo Wang,Min Tang,Nuo Shen,Shujie Cui,Weiqing Wang*

Main category: cs.IR

TL;DR: LLM推荐系统存在较高的隐私泄露风险，攻击者可以通过反向推导还原用户个人信息和偏好。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推荐系统中的应用，隐私保护成为亟待解决的问题。

Method: 提出一种改进的反向重建方法，通过优化提升文本提示的还原精度，并在电影和书籍两个领域进行实验证明其有效性。

Result: 成功恢复近65%的用户互动物品，并在87%的情况下正确推断出用户年龄和性别。

Conclusion: LLM驱动的推荐系统存在严重的隐私漏洞，需引起重视并采取相应保护措施。

Abstract: The large language model (LLM) powered recommendation paradigm has been
proposed to address the limitations of traditional recommender systems, which
often struggle to handle cold start users or items with new IDs. Despite its
effectiveness, this study uncovers that LLM empowered recommender systems are
vulnerable to reconstruction attacks that can expose both system and user
privacy. To examine this threat, we present the first systematic study on
inversion attacks targeting LLM empowered recommender systems, where
adversaries attempt to reconstruct original prompts that contain personal
preferences, interaction histories, and demographic attributes by exploiting
the output logits of recommendation models. We reproduce the vec2text framework
and optimize it using our proposed method called Similarity Guided Refinement,
enabling more accurate reconstruction of textual prompts from model generated
logits. Extensive experiments across two domains (movies and books) and two
representative LLM based recommendation models demonstrate that our method
achieves high fidelity reconstructions. Specifically, we can recover nearly 65
percent of the user interacted items and correctly infer age and gender in 87
percent of the cases. The experiments also reveal that privacy leakage is
largely insensitive to the victim model's performance but highly dependent on
domain consistency and prompt complexity. These findings expose critical
privacy vulnerabilities in LLM empowered recommender systems.

</details>


### [148] [Evaluating Generative AI Tools for Personalized Offline Recommendations: A Comparative Study](https://arxiv.org/abs/2508.03710)
*Rafael Salinas-Buestan,Otto Parra,Nelly Condori-Fernandez,Maria Fernanda Granda*

Main category: cs.IR

TL;DR: 研究评估了五种主流生成式AI工具在推荐非数字化活动以减少重复性损伤风险方面的表现及用户满意度。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在健康行为干预中的效能，尤其是减少技术使用方面的潜力。

Method: 采用GQM模型，比较不同AI工具在定量（精准率、召回率等）和定性（用户满意度）上的表现，特别关注工具的准确性和用户体验。

Result: 未提供具体结果，但聚焦在性能指标和用户满意度的比较分析。

Conclusion: 未详细描述，但意在指导选择最优AI工具用于健康干预。

Abstract: Background: Generative AI tools have become increasingly relevant in
supporting personalized recommendations across various domains. However, their
effectiveness in health-related behavioral interventions, especially those
aiming to reduce the use of technology, remains underexplored. Aims: This study
evaluates the performance and user satisfaction of the five most widely used
generative AI tools when recommending non-digital activities tailored to
individuals at risk of repetitive strain injury. Method: Following the
Goal/Question/Metric (GQM) paradigm, this proposed experiment involves
generative AI tools that suggest offline activities based on predefined user
profiles and intervention scenarios. The evaluation is focused on quantitative
performance (precision, recall, F1-score and MCC-score) and qualitative aspects
(user satisfaction and perceived recommendation relevance). Two research
questions were defined: RQ1 assessed which tool delivers the most accurate
recommendations, and RQ2 evaluated how tool choice influences user
satisfaction.

</details>


### [149] [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
*Wenchuan Mu,Menglin Li,Kwan Hui Lim*

Main category: cs.IR

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.

</details>


### [150] [Measuring the stability and plasticity of recommender systems](https://arxiv.org/abs/2508.03941)
*Maria João Lavoura,Robert Jungnickel,João Vinagre*

Main category: cs.IR

TL;DR: 提出了一种评估推荐模型长远表现的方法，衡量模型的稳定性和塑性，揭示不同算法的平衡点。


<details>
  <summary>Details</summary>
Motivation: 弥补传统离线评估只反映模型某一时点性能的不足，关注模型在时间演变中的表现。

Method: 设计了一套无关数据集、算法和指标的离线评估协议，分析模型在长期中的稳定性和塑性。

Result: 初步实验显示不同算法存在不同的稳定性和塑性特征，以及潜在的权衡关系。

Conclusion: 该框架有助于理解推荐系统的长期动态，为模型优化提供新思路。

Abstract: The typical offline protocol to evaluate recommendation algorithms is to
collect a dataset of user-item interactions and then use a part of this dataset
to train a model, and the remaining data to measure how closely the model
recommendations match the observed user interactions. This protocol is
straightforward, useful and practical, but it only captures performance of a
particular model trained at some point in the past. We know, however, that
online systems evolve over time. In general, it is a good idea that models
reflect such changes, so models are frequently retrained with recent data. But
if this is the case, to what extent can we trust previous evaluations? How will
a model perform when a different pattern (re)emerges? In this paper we propose
a methodology to study how recommendation models behave when they are
retrained. The idea is to profile algorithms according to their ability to, on
the one hand, retain past patterns -- stability -- and, on the other hand,
(quickly) adapt to changes -- plasticity. We devise an offline evaluation
protocol that provides detail on the long-term behavior of models, and that is
agnostic to datasets, algorithms and metrics. To illustrate the potential of
this framework, we present preliminary results of three different types of
algorithms on the GoodReads dataset that suggest different stability and
plasticity profiles depending on the algorithmic technique, and a possible
trade-off between stability and plasticity.Although additional experiments will
be necessary to confirm these observations, they already illustrate the
usefulness of the proposed framework to gain insights on the long term dynamics
of recommendation models.

</details>


### [151] [ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval](https://arxiv.org/abs/2508.04001)
*Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Zhichao Xu,Zhan Su,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 提出了一种名为ConvMix的混合准则数据增强框架，用以提升对话式检索模型性能，有效应对数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 对话式搜索的关键在于理解用户多轮交互中的真实意图，但现有方法受限于训练数据不足。

Method: 结合大语言模型设计双边相关性判断增强方案，加入质量控制和近分布监督，实现多角度数据增强。

Result: 在五个基准数据集上验证，ConvMix训练的对话检索器优于现有方法，效果显著。

Conclusion: ConvMix有效缓解数据稀缺，提升对话检索性能，具备较好推广潜力。

Abstract: Conversational search aims to satisfy users' complex information needs via
multiple-turn interactions. The key challenge lies in revealing real users'
search intent from the context-dependent queries. Previous studies achieve
conversational search by fine-tuning a conversational dense retriever with
relevance judgments between pairs of context-dependent queries and documents.
However, this training paradigm encounters data scarcity issues. To this end,
we propose ConvMix, a mixed-criteria framework to augment conversational dense
retrieval, which covers more aspects than existing data augmentation
frameworks. We design a two-sided relevance judgment augmentation schema in a
scalable manner via the aid of large language models. Besides, we integrate the
framework with quality control mechanisms to obtain semantically diverse
samples and near-distribution supervisions to combine various annotated data.
Experimental results on five widely used benchmarks show that the
conversational dense retriever trained by our ConvMix framework outperforms
previous baseline methods, which demonstrates our superior effectiveness.

</details>


### [152] [Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04032)
*Qian Yong,Yanhui Li,Jialiang Shi,Yaguang Dou,Tian Qi*

Main category: cs.IR

TL;DR: 利用大语言模型动态构建用户知识图谱，提升推荐系统的趣味性和用户体验，实现效果显著改善。


<details>
  <summary>Details</summary>
Motivation: 解决工业推荐系统中内容同质化、过滤泡泡与用户满意度降低的问题。

Method: 采用两阶段框架，包括两跳兴趣推理和近线自适应，结合u2i和i2i检索模型在实际应用中验证。

Result: 在Dewu应用中取得多项指标提升，如新颖曝光率、点击率和用户行为增强，显著提升用户体验。

Conclusion: 该方法有效提升推荐系统的趣味性及用户体验，有望推广应用于工业场景中。

Abstract: The feedback loop in industrial recommendation systems reinforces homogeneous
content, creates filter bubble effects, and diminishes user satisfaction.
Recently, large language models(LLMs) have demonstrated potential in
serendipity recommendation, thanks to their extensive world knowledge and
superior reasoning capabilities. However, these models still face challenges in
ensuring the rationality of the reasoning process, the usefulness of the
reasoning results, and meeting the latency requirements of industrial
recommendation systems (RSs). To address these challenges, we propose a method
that leverages llm to dynamically construct user knowledge graphs, thereby
enhancing the serendipity of recommendation systems. This method comprises a
two stage framework:(1) two-hop interest reasoning, where user static profiles
and historical behaviors are utilized to dynamically construct user knowledge
graphs via llm. Two-hop reasoning, which can enhance the quality and accuracy
of LLM reasoning results, is then performed on the constructed graphs to
identify users' potential interests; and(2) Near-line adaptation, a
cost-effective approach to deploying the aforementioned models in industrial
recommendation systems. We propose a u2i (user-to-item) retrieval model that
also incorporates i2i (item-to-item) retrieval capabilities, the retrieved
items not only exhibit strong relevance to users' newly emerged interests but
also retain the high conversion rate of traditional u2i retrieval. Our online
experiments on the Dewu app, which has tens of millions of users, indicate that
the method increased the exposure novelty rate by 4.62%, the click novelty rate
by 4.85%, the average view duration per person by 0.15%, unique visitor click
through rate by 0.07%, and unique visitor interaction penetration by 0.30%,
enhancing user experience.

</details>


### [153] [Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation](https://arxiv.org/abs/2508.04145)
*Teng Shi,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: GSERec通过图神经网络和对比学习解决搜索行为稀疏用户的推荐问题，增强用户特征，提高推荐效果。


<details>
  <summary>Details</summary>
Motivation: 在搜索增强推荐系统中，稀疏搜索行为限制模型性能，需要利用丰富搜索行为用户的数据帮助稀疏行为用户。

Method: 利用大语言模型生成离散码，通过用户-编码图的消息传递，结合对比损失增强稀疏用户的表示，最后融合到推荐模型中。

Result: 在三个真实数据集上，GSERec显著优于基线，特别改善了搜索行为稀疏用户的推荐效果。

Conclusion: 利用图神经网络和对比学习，有效缓解搜索行为稀疏带来的用户表示问题，提升搜索增强推荐系统的整体性能。

Abstract: In modern online platforms, search and recommendation (S&R) often coexist,
offering opportunities for performance improvement through search-enhanced
approaches. Existing studies show that incorporating search signals boosts
recommendation performance. However, the effectiveness of these methods relies
heavily on rich search interactions. They primarily benefit a small subset of
users with abundant search behavior, while offering limited improvements for
the majority of users who exhibit only sparse search activity. To address the
problem of sparse search data in search-enhanced recommendation, we face two
key challenges: (1) how to learn useful search features for users with sparse
search interactions, and (2) how to design effective training objectives under
sparse conditions. Our idea is to leverage the features of users with rich
search interactions to enhance those of users with sparse search interactions.
Based on this idea, we propose GSERec, a method that utilizes message passing
on the User-Code Graphs to alleviate data sparsity in Search-Enhanced
Recommendation. Specifically, we utilize Large Language Models (LLMs) with
vector quantization to generate discrete codes, which connect similar users and
thereby construct the graph. Through message passing on this graph, embeddings
of users with rich search data are propagated to enhance the embeddings of
users with sparse interactions. To further ensure that the message passing
captures meaningful information from truly similar users, we introduce a
contrastive loss to better model user similarities. The enhanced user
representations are then integrated into downstream search-enhanced
recommendation models. Experiments on three real-world datasets show that
GSERec consistently outperforms baselines, especially for users with sparse
search behaviors.

</details>


### [154] [Bridging Search and Recommendation through Latent Cross Reasoning](https://arxiv.org/abs/2508.04152)
*Teng Shi,Weicong Qin,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: 该论文提出一种潜在交叉推理框架，结合对搜索和推荐历史的编码、对搜索行为的推理以及对目标项目的对比学习和强化学习，以提高搜索感知推荐的效果。


<details>
  <summary>Details</summary>
Motivation: 利用用户搜索历史中的有效信号提升推荐性能，避免噪声干扰。

Method: 设计潜在交叉推理框架，通过编码历史、推理行为和对比学习、强化学习优化排序。

Result: 在公共基准测试中显示出持续优于强基线的性能，验证了推理机制的重要性。

Conclusion: 引入推理机制显著提升搜索感知推荐的效果，为推荐系统设计提供新方向。

Abstract: Search and recommendation (S&R) are fundamental components of modern online
platforms, yet effectively leveraging search behaviors to improve
recommendation remains a challenging problem. User search histories often
contain noisy or irrelevant signals that can even degrade recommendation
performance, while existing approaches typically encode S&R histories either
jointly or separately without explicitly identifying which search behaviors are
truly useful. Inspired by the human decision-making process, where one first
identifies recommendation intent and then reasons about relevant evidence, we
design a latent cross reasoning framework that first encodes user S&R histories
to capture global interests and then iteratively reasons over search behaviors
to extract signals beneficial for recommendation. Contrastive learning is
employed to align latent reasoning states with target items, and reinforcement
learning is further introduced to directly optimize ranking performance.
Extensive experiments on public benchmarks demonstrate consistent improvements
over strong baselines, validating the importance of reasoning in enhancing
search-aware recommendation.

</details>


### [155] [SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval](https://arxiv.org/abs/2508.04162)
*Ruyin Li,Xiaoyu Chen*

Main category: cs.IR

TL;DR: 提出一种新颖的公式嵌入框架SSEmb，结合结构和语义特征，实现更优的公式检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决公式检索中有效捕捉结构和语义信息的难题。

Method: 采用图对比学习编码操作符图，利用句子BERT编码公式周围文本，通过加权融合结构和语义相似性。

Result: 在ARQMath-3任务中，SSEmb性能优于现有方法，达成最优结果，并提升其他方法的表现。

Conclusion: SSEmb是一种有效的公式嵌入方法，显著提升公式检索效果。

Abstract: Formula retrieval is an important topic in Mathematical Information
Retrieval. We propose SSEmb, a novel embedding framework capable of capturing
both structural and semantic features of mathematical formulas. Structurally,
we employ Graph Contrastive Learning to encode formulas represented as Operator
Graphs. To enhance structural diversity while preserving mathematical validity
of these formula graphs, we introduce a novel graph data augmentation approach
through a substitution strategy. Semantically, we utilize Sentence-BERT to
encode the surrounding text of formulas. Finally, for each query and its
candidates, structural and semantic similarities are calculated separately and
then fused through a weighted scheme. In the ARQMath-3 formula retrieval task,
SSEmb outperforms existing embedding-based methods by over 5 percentage points
on P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runs
of other methods and achieves state-of-the-art results when combined with
Approach0.

</details>


### [156] [ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation](https://arxiv.org/abs/2508.04206)
*Fatemeh Nazary,Ali Tourani,Yashar Deldjoo,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 提出一个面向多模态电影推荐的基准平台，支持多种融合方式和大规模评估体系，证明大模型增强和优质文本嵌入能提升冷启动和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 多模态电影推荐需要联合建模视觉、音频和文本信息，但现有基准多关注原始特征或狭隘融合，缺乏全面评估工具。

Method: 构建一个扩展性强的评测平台，集成多模态特征提取、利用大模型增强元数据、支持多种融合策略及基线模型，进行全面性能评估。

Result: LLM增强的元数据丰富和优质文本嵌入改善冷启动和覆盖，融合不同模态带来性能提升，系统性评测揭示了不同方法的优势组合。

Conclusion: 该平台提供了公平、可重复的多模态推荐评估工具，为未来融合大模型的推荐系统研究提供基础。

Abstract: Recommending long-form video content demands joint modeling of visual, audio,
and textual modalities, yet most benchmarks address only raw features or narrow
fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for
LLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,
it aligns dense item embeddings from three modalities: audio (block-level,
i-vector), visual (CNN, AVF), and text. Missing or sparse metadata is
automatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),
generating high-quality synopses for thousands of movies. All text (raw or
augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),
producing multiple ready-to-use sets. The pipeline supports interchangeable
early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and
multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are
fully declarative via a single YAML file. Evaluation spans accuracy (Recall,
nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,
diversity, fairness. Results show LLM-based augmentation and strong text
embeddings boost cold-start and coverage, especially when fused with
audio-visual features. Systematic benchmarking reveals universal versus
backbone- or metric-specific combinations. Open-source code, embeddings, and
configs enable reproducible, fair multimodal RS research and advance principled
generative AI integration in large-scale recommendation. Code:
https://recsys-lab.github.io/ViLLA-MMBench

</details>


### [157] [Discrete-event Tensor Factorization: Learning a Smooth Embedding for Continuous Domains](https://arxiv.org/abs/2508.04221)
*Joey De Pauw,Bart Goethals*

Main category: cs.IR

TL;DR: 该论文研究了如何在因子分解推荐模型中显式编码时间信息，提出了连续时间编码机制，能捕捉用户偏好和物品感知的变化，但实验表明，后续的流行度调整已足以实现优良性能，强调预测未来比捕获过去趋势更重要。


<details>
  <summary>Details</summary>
Motivation: 提高推荐模型对时间变化的适应能力，改进对用户偏好和物品变化的建模。

Method: 引入绝对时间作为特征，提出一种全连续的时间编码机制，利用多项式拟合避免离散化，增强模型的时间感知能力。

Result: 模型有效捕捉时间信号，但实际表现显示，后续的流行度修正已能达到较佳效果，强调预测未来的重要性。

Conclusion: 未来的推荐系统应侧重于对未来的预测，需要开发专门的机制以实现更好的时间外推能力。

Abstract: Recommender systems learn from past user behavior to predict future user
preferences. Intuitively, it has been established that the most recent
interactions are more indicative of future preferences than older interactions.
Many recommendation algorithms use this notion to either drop older
interactions or to assign them a lower weight, so the model can focus on the
more informative, recent information. However, very few approaches model the
flow of time explicitly.
  This paper analyzes how time can be encoded in factorization-style
recommendation models. By including absolute time as a feature, our models can
learn varying user preferences and changing item perception over time. In
addition to simple binning approaches, we also propose a novel, fully
continuous time encoding mechanism. Through the use of a polynomial fit inside
the loss function, our models completely avoid the need for discretization, and
they are able to capture the time dimension in arbitrary resolution.
  We perform a comparative study on three real-world datasets that span
multiple years, where long user histories are present, and items stay relevant
for a longer time. Empirical results show that, by explicitly modeling time,
our models are very effective at capturing temporal signals, such as varying
item popularities over time. Despite this however, our experiments also
indicate that a simple post-hoc popularity adjustment is often sufficient to
achieve the best performance on the unseen test set. This teaches us that, for
the recommendation task, predicting the future is more important than capturing
past trends. As such, we argue that specialized mechanisms are needed for
extrapolation to future data.

</details>


### [158] [I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation](https://arxiv.org/abs/2508.04247)
*Huilin Chen,Miaomiao Cai,Fan Liu,Zhiyong Cheng,Richang Hong,Meng Wang*

Main category: cs.IR

TL;DR: I$^3$-MRec通过引入不变性学习和信息瓶颈原则，有效提升多模态推荐系统在缺失模态场景下的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统在实际应用中常面临模态缺失问题，影响模型鲁棒性和泛化能力。

Method: 采用不变风险最小化和信息瓶颈机制，分别实现跨模态偏好不变性和模态表示紧凑性，以提升推荐性能。

Result: 在三个真实数据集上多模态缺失场景中，显著优于现有方法，验证了其鲁棒性和有效性。

Conclusion: I$^3$-MRec在提升多模态推荐系统在实际中对缺失模态的适应性和性能方面具有优势。

Abstract: Multimodal recommender systems (MRS) improve recommendation performance by
integrating diverse semantic information from multiple modalities. However, the
assumption of the availability of all modalities rarely holds in practice due
to missing images, incomplete descriptions, or inconsistent user content. These
challenges significantly degrade the robustness and generalization capabilities
of current models. To address these challenges, we introduce a novel method
called \textbf{I$^3$-MRec}, which uses \textbf{I}nvariant learning with
\textbf{I}nformation bottleneck principle for \textbf{I}ncomplete
\textbf{M}odality \textbf{Rec}ommendation. To achieve robust performance in
missing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)
cross-modal preference invariance, which ensures consistent user preference
modeling across varying modality environments, and (ii) compact yet effective
modality representation, which filters out task-irrelevant modality information
while maximally preserving essential features relevant to recommendation. By
treating each modality as a distinct semantic environment, I$^3$-MRec employs
invariant risk minimization (IRM) to learn modality-specific item
representations. In parallel, a missing-aware fusion module grounded in the
Information Bottleneck (IB) principle extracts compact and effective item
embeddings by suppressing modality noise and preserving core user preference
signals. Extensive experiments conducted on three real-world datasets
demonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art
MRS methods across various modality-missing scenarios, highlighting its
effectiveness and robustness in practical applications. The code and processed
datasets are released at https://github.com/HuilinChenJN/I3-MRec.

</details>


### [159] [Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum](https://arxiv.org/abs/2508.04293)
*Nirmal Gaud,Surej Mouli,Preeti Katiyar,Vaduguru Venkata Ramya*

Main category: cs.IR

TL;DR: NIRMAL是一种结合多种优化策略的新型优化算法，在图像分类任务中表现出色，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 旨在提升深度学习优化器的性能和鲁棒性，特别是在复杂数据集上的表现。

Method: 结合梯度下降、动量、随机扰动、自适应学习率和非线性变换等多策略，设计了一种新算法。

Result: NIRMAL在多个图像分类数据集上与Adam和SGD-Momentum相竞争，尤其在复杂的数据集上表现优异。

Conclusion: NIRMAL展现出强大的泛化能力和鲁棒性，是一种有潜力的深度学习优化工具。

Abstract: This study proposes NIRMAL (Novel Integrated Robust Multi-Adaptation
Learning), a novel optimization algorithm that combines multiple strategies
inspired by the movements of the chess piece. These strategies include gradient
descent, momentum, stochastic perturbations, adaptive learning rates, and
non-linear transformations. We carefully evaluated NIRMAL against two widely
used and successful optimizers, Adam and SGD with Momentum, on four benchmark
image classification datasets: MNIST, FashionMNIST, CIFAR-10, and CIFAR-100.
The custom convolutional neural network (CNN) architecture is applied on each
dataset. The experimental results show that NIRMAL achieves competitive
performance, particularly on the more challenging CIFAR-100 dataset, where it
achieved a test accuracy of 45.32\%and a weighted F1-score of 0.4328. This
performance surpasses Adam (41.79\% accuracy, 0.3964 F1-score) and closely
matches SGD with Momentum (46.97\% accuracy, 0.4531 F1-score). Also, NIRMAL
exhibits robust convergence and strong generalization capabilities, especially
on complex datasets, as evidenced by stable training results in loss and
accuracy curves. These findings underscore NIRMAL's significant ability as a
versatile and effective optimizer for various deep learning tasks.

</details>


### [160] [Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics](https://arxiv.org/abs/2508.04419)
*Jarne Mathi Decker,Joeran Beel*

Main category: cs.IR

TL;DR: 使用用户特征和源代码中的算法特征改善推荐系统中的算法选择，通过引入算法的特征显著提升性能，并优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中算法选择困难的问题，传统方法未充分利用算法的属性信息。

Method: 提出一种结合用户元特征和源代码自动提取的算法特征的元学习方法，用于个性化算法选择。

Result: 模型性能提升8.83%，超过单一最优算法基础，缩小了与理想选择者的性能差距。

Conclusion: 静态源代码指标是有价值的预测信号，为构建智能推荐系统提供新的方向。

Abstract: The Algorithm Selection Problem for recommender systems-choosing the best
algorithm for a given user or context-remains a significant challenge.
Traditional meta-learning approaches often treat algorithms as categorical
choices, ignoring their intrinsic properties. Recent work has shown that
explicitly characterizing algorithms with features can improve model
performance in other domains. Building on this, we propose a per-user
meta-learning approach for recommender system selection that leverages both
user meta-features and automatically extracted algorithm features from source
code. Our preliminary results, averaged over six diverse datasets, show that
augmenting a meta-learner with algorithm features improves its average NDCG@10
performance by 8.83% from 0.135 (user features only) to 0.147. This enhanced
model outperforms the Single Best Algorithm baseline (0.131) and successfully
closes 10.5% of the performance gap to a theoretical oracle selector. These
findings show that even static source code metrics provide a valuable
predictive signal, presenting a promising direction for building more robust
and intelligent recommender systems.

</details>


### [161] [TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04474)
*Xinkui Zhao,Haode Li,Yifan Zhang,Guanjie Cheng,Yueshen Xu*

Main category: cs.IR

TL;DR: TRAIL框架结合知识图谱动态更新与大语言模型，实现多方面提升，推动可持续学习与可信推理。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识更新和推理中的局限，提升其适应性、准确性和解释性。

Method: 提出TRAIL框架，通过联合推理和动态知识图谱优化，利用置信机制进行事实生成、验证与剪裁，支持模型持续学习。

Result: TRAIL在多个基准测试中优于现有方法，性能提升3%至13%，推动了适应性和可信推理的发展。

Conclusion: TRAIL实现了融知识图谱与大模型的统一优化，有助于构建具有持续学习能力的可靠推理模型。

Abstract: Recent advances in large language models (LLMs) have unlocked powerful
reasoning and decision-making capabilities. However, their inherent dependence
on static parametric memory fundamentally limits their adaptability, factual
accuracy, and interpretability in knowledge-intensive scenarios. Knowledge
graphs (KGs), as structured repositories of explicit relational knowledge,
offer a promising approach for augmenting LLMs with external, interpretable
memory. Nevertheless, most existing methods that combine LLMs with KGs treat
reasoning and knowledge updating as separate processes, resulting in suboptimal
utilization of new information and hindering real-time updates. In this work,
we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And
Incremental Learning that couples joint inference and dynamic KG refinement
with large language models. TRAIL enables LLM agents to iteratively explore,
update, and refine knowledge graphs during the reasoning process, employing a
confidence-driven mechanism for the generation, validation, and pruning of new
facts. This plug-and-play architecture facilitates seamless integration with
various LLMs, supporting continual adaptation without the need for retraining.
Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms
existing KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More
importantly, these results represent a significant step toward developing
adaptive, memory-augmented language models capable of continual learning and
reliable, transparent reasoning.

</details>


### [162] [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
*Claudio Pomo,Matteo Attimonelli,Danilo Danese,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 利用大规模视觉-语言模型生成的语义对齐的多模态嵌入显著提升推荐系统的性能，强调了语义丰富的多模态表示的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究多模态推荐系统中提升性能的真正原因，特别是多模态嵌入的语义丰富性是否关键。

Method: 通过使用大规模视觉-语言模型（LVLMs）生成语义对齐的嵌入，避免了复杂的融合策略。

Result: 采用LVLMs生成的嵌入表现出优秀的推荐性能，并且可以转化为结构化文本描述，用以评估模型的多模态理解能力。

Conclusion: 强调语义丰富的多模态表示的重要性，LVLMs为构建稳健且有意义的多模态推荐提供了有力基础。

Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

</details>


### [163] [A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature](https://arxiv.org/abs/2508.04612)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.IR

TL;DR: 提出一个开源自动化文献检索与重现平台，提升自动回归模型研究的效率。


<details>
  <summary>Details</summary>
Motivation: 随着自动回归生成模型研究快速发展，手动文献检索与重复验证变得不现实。

Method: 开发自动检索、过滤、提取、聚类、多模态摘要及脚本生成的完整流水线，利用多核并行实现高扩展性。

Result: 该系统在相关性、参数提取等任务上表现优异，支持大规模文献处理与模型复现，验证了其在实际研究中的有效性。

Conclusion: 该工具显著提升了研究效率，降低了模型重现的门槛，促进了自动回归模型发展。

Abstract: The accelerating pace of research on autoregressive generative models has
produced thousands of papers, making manual literature surveys and reproduction
studies increasingly impractical. We present a fully open-source, reproducible
pipeline that automatically retrieves candidate documents from public
repositories, filters them for relevance, extracts metadata, hyper-parameters
and reported results, clusters topics, produces retrieval-augmented summaries
and generates containerised scripts for re-running selected experiments.
Quantitative evaluation on 50 manually-annotated papers shows F1 scores above
0.85 for relevance classification, hyper-parameter extraction and citation
identification. Experiments on corpora of up to 1000 papers demonstrate
near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM
on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model
on the Lakh MIDI dataset -- confirm that the extracted settings support
faithful reproduction, achieving test perplexities within 1--3% of the original
reports.

</details>


### [164] [HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs](https://arxiv.org/abs/2508.04618)
*Dengzhao Fang,Jingtong Gao,Chengcheng Zhu,Yu Li,Xiangyu Zhao,Yi Chang*

Main category: cs.IR

TL;DR: 提出HiD-VAE，通过层次化监督量化和唯一性损失，解决生成推荐中的ID扁平、错综复杂问题，有效提升推荐的解釈性、准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决生成推荐中语义ID扁平化和代表融合问题，以提升推荐系统的可解释性和性能。

Method: 引入层次化监督量化、多级标签映射，以及唯一性损失，学习层次化解耦的项目表示。

Result: 在多个公开基准测试中优于现有方法，验证了模型的有效性。

Conclusion: HiD-VAE提供了一种改善生成推荐ID语义结构和多样性的新方法，具有较强的实用前景。

Abstract: Recommender systems are indispensable for helping users navigate the immense
item catalogs of modern online platforms. Recently, generative recommendation
has emerged as a promising paradigm, unifying the conventional
retrieve-and-rank pipeline into an end-to-end model capable of dynamic
generation. However, existing generative methods are fundamentally constrained
by their unsupervised tokenization, which generates semantic IDs suffering from
two critical flaws: (1) they are semantically flat and uninterpretable, lacking
a coherent hierarchy, and (2) they are prone to representation entanglement
(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.
To overcome these limitations, we propose HiD-VAE, a novel framework that
learns hierarchically disentangled item representations through two core
innovations. First, HiD-VAE pioneers a hierarchically-supervised quantization
process that aligns discrete codes with multi-level item tags, yielding more
uniform and disentangled IDs. Crucially, the trained codebooks can predict
hierarchical tags, providing a traceable and interpretable semantic path for
each recommendation. Second, to combat representation entanglement, HiD-VAE
incorporates a novel uniqueness loss that directly penalizes latent space
overlap. This mechanism not only resolves the critical ID collision problem but
also promotes recommendation diversity by ensuring a more comprehensive
utilization of the item representation space. These high-quality, disentangled
IDs provide a powerful foundation for downstream generative models. Extensive
experiments on three public benchmarks validate HiD-VAE's superior performance
against state-of-the-art methods. The code is available at
https://anonymous.4open.science/r/HiD-VAE-84B2.

</details>


### [165] [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
*Karthik Menon,Batool Arhamna Haider,Muhammad Arham,Kanwal Mehreen,Ram Mohan Rao Kadiyala,Hamza Farooq*

Main category: cs.IR

TL;DR: 提出了一种融合结构化标注与语义分析的查询属性模型（QAM）提升搜索准确性，实验证明优于传统方法，特别适用于电子商务。


<details>
  <summary>Details</summary>
Motivation: 改善开放式文本查询的搜索准确性和相关性，传统方法存在噪声和精确度不足的问题。

Method: 将开放文本查询分解成结构化的元数据标签和语义元素，自动提取过滤条件，结合多种搜索技术以优化结果。

Result: 在亚马逊玩具评论数据集上，QAM的平均前5名精确度达52.99%，显著优于BM25、语义相似性搜索和混合搜索等传统方法。

Conclusion: QAM是一种稳健的企业级搜索解决方案，特别适合电子商务场景，有助于提升搜索的相关性和准确率。

Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

</details>


### [166] [Recommendation with Generative Models](https://arxiv.org/abs/2409.15173)
*Yashar Deldjoo,Zhankui He,Julian McAuley,Anton Korikov,Scott Sanner,Arnau Ramisa,Rene Vidal,Maheswaran Sathiamoorthy,Atoosa Kasrizadeh,Silvia Milano,Francesco Ricci*

Main category: cs.IR

TL;DR: 生成模型通过学习和采样数据分布，广泛应用于图像、文本、音乐等领域，尤其在推荐系统中提升个性化和多样性。本文介绍了深度生成模型的分类和技术进展，强调风险与评估。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，理解其分类、应用及潜在风险成为亟需的研究方向。

Method: 本文通过构建分类体系，分析不同深度生成模型的技术特点和应用场景，探讨其在推荐系统中的作用。

Result: 提出了ID驱动模型、LLMs和多模态模型的分类体系，系统梳理模型技术进展，强调评估框架的重要性。

Conclusion: 深入理解生成模型的分类和应用，有助于推动其在多领域的健康发展，并应对潜在风险。

Abstract: Generative models are a class of AI models capable of creating new instances
of data by learning and sampling from their statistical distributions. In
recent years, these models have gained prominence in machine learning due to
the development of approaches such as generative adversarial networks (GANs),
variational autoencoders (VAEs), and transformer-based architectures such as
GPT. These models have applications across various domains, such as image
generation, text synthesis, and music composition. In recommender systems,
generative models, referred to as Gen-RecSys, improve the accuracy and
diversity of recommendations by generating structured outputs, text-based
interactions, and multimedia content. By leveraging these capabilities,
Gen-RecSys can produce more personalized, engaging, and dynamic user
experiences, expanding the role of AI in eCommerce, media, and beyond.
  Our book goes beyond existing literature by offering a comprehensive
understanding of generative models and their applications, with a special focus
on deep generative models (DGMs) and their classification. We introduce a
taxonomy that categorizes DGMs into three types: ID-driven models, large
language models (LLMs), and multimodal models. Each category addresses unique
technical and architectural advancements within its respective research area.
This taxonomy allows researchers to easily navigate developments in Gen-RecSys
across domains such as conversational AI and multimodal content generation.
Additionally, we examine the impact and potential risks of generative models,
emphasizing the importance of robust evaluation frameworks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [167] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: 提出MI9框架以应对具有自主性的AI系统在运行时带来的新型治理挑战，通过六个控件实现实时监管，确保其安全和对齐。


<details>
  <summary>Details</summary>
Motivation: 传统AI治理不足以应对自主性AI系统在运行中出现的自主行为和风险。

Method: 开发了包含风险索引、语义遥测、持续授权、有限状态机一致性检测、目标漂移检测和渐进封控的集成监控框架MI9。

Result: MI9在多场景下展现出系统化覆盖未被传统方法解决的治理难题，确保自主AI系统在生产中的安全部署。

Conclusion: MI9提供了基础设施支持自主AI的安全、系统性治理，填补了现有治理方案的空白。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [168] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL通过多智能体强化学习，使各个智能体同时具备任务执行和防御能力，有效提升系统安全性和性能，同时降低成本和单点失效风险。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在协作和性能方面表现出色，但面临安全风险如劫持和对抗攻击，需增强自主防御能力。

Method: 引入Evo-MARL框架，通过强化学习与进化搜索结合，使所有任务智能体共同学习防御策略，实现自我防护。

Result: 该方法成功降低攻击成功率达22%，提升推理任务精度达5%，展示安全性和实用性的共同提升。

Conclusion: Evo-MARL实现了无需外部安全模块的自我防护机制，有效提升多智能体系统的安全性与性能，同时降低成本和单点故障风险。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [169] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 引入多策略优化框架MOTIF，利用多智能体轮转优化提升问题求解器设计效果。


<details>
  <summary>Details</summary>
Motivation: 现有COP求解策略多依赖手工设计，缺乏创新空间，亟需新的自动化优化方法。

Method: 提出基于蒙特卡洛树搜索的轮转交互框架，两个LLM智能体轮流优化多个相互依赖的组件。

Result: 在多个组合优化问题中优于现有最先进方法，验证了多智能体轮转优化的有效性。

Conclusion: 多策略、多智能体轮转优化为自动生成高效求解器提供了新路径，有望推动自动化AI解决方案的发展。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [170] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 本文提出了SymbolBench基准，用于评估大型语言模型在时间序列数据中的符号推理能力，并结合遗传编程构建闭环推理系统，分析了模型的优势与不足。


<details>
  <summary>Details</summary>
Motivation: 探索从时间序列数据中揭示隐含符号规律的能力，促进科学发现与AI的发展。

Method: 设计了SymbolBench基准，涵盖多任务、多符号形式，结合LLMs与遗传编程实现闭环推理。

Result: 验证了模型在复杂符号推理中的优势与不足，强调知识整合和上下文对推理的重要性。

Conclusion: 结合领域知识与结构化推理可提升LLMs在科学发现中的表现，推动符号推理的研究。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [171] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 该论文提出EmoAgent框架，用于检测和利用大规模多模态模型中的情感偏差和安全风险。


<details>
  <summary>Details</summary>
Motivation: 研究发现人类中心的多模态大模型（MLRMs）在高情感激励下容易走偏，忽视安全协议。

Method: 提出情感操控的对抗框架EmoAgent，评估模型在情感影响下的安全表现，并开发三个评估指标探索潜在风险。

Result: 通过实验验证EmoAgent的有效性，揭示模型在情感影响下的安全隐患和行为偏差。

Conclusion: 情感因素显著影响MLRMs的安全性，需引入更精细的检测与防范机制以确保模型安全。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [172] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 提出Cognition Forest结构及Galaxy框架，通过整合认知架构和系统设计，促进IPAs的主动性、自我演进和隐私保护，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升智能个人助理的主动性、自我演化能力和隐私保护水平，弥补现有研究关注响应能力不足的问题。

Method: 设计Cognition Forest语义结构，将认知架构与系统设计融合，提出支持多维交互和个性化生成的Galaxy框架，构建KoRa和Kernel两个合作代理模型。

Result: Galaxy在多项基准测试中优于多项先进结果，通过消融研究和实际案例验证其有效性。

Conclusion: 该方法有效促进IPAs的主动性与自主学习能力，为未来智能助手设计提供新思路。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [173] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: 提出了一种基于不确定性感知的GUI代理RecAgent，能通过适应性感知减少输入冗余和决策歧义，结合用户反馈提升任务执行成功率。


<details>
  <summary>Details</summary>
Motivation: 解决目前GUI代理在输入冗余和决策歧义方面的挑战，提升自动化任务的表现。

Method: 引入感知不确定性概念，设计组成推荐机制和交互模块，实现主动减少输入复杂性和高不确定性时的人机协作。

Result: 通过在ComplexAction数据集上的实验验证，表现优异，证明了该方法的有效性。

Conclusion: RecAgent利用不确定性感知与人机互动，有效改善GUI任务自动化，推动该领域的发展。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [174] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 提出了一种名为自我进化代理（SEA）的智能计算助手，通过创新的数据生成、强化学习和模型增强策略，实现了在较小参数规模下的优异性能。


<details>
  <summary>Details</summary>
Motivation: 提升计算代理的性能，满足更高效率和自主性的需求，但现有代理性能不足。

Method: 采用自动化轨迹生成、逐步强化学习和模型融合增强等创新方法。

Result: 7B参数的SEA模型超越同参数量模型，性能媲美更大模型，并计划开源。

Conclusion: 通过创新策略，显著提升了计算代理的性能，未来有望广泛应用。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [175] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 个性化学习内容通过生成式AI结合职业目标，提高学习者参与度和满意度。


<details>
  <summary>Details</summary>
Motivation: 随着AI在数字学习环境中的融入，个性化内容对提升学习动机和效率具有潜力。

Method: 通过混合方法实验，比较职业目标定制内容与标准内容对学习者的影响。

Result: 个性化内容延长了学习会话、提高了满意度，且略微缩短学习时间，促进深度认知和职业认同。

Conclusion: 结合职业目标的AI个性化内容具有重要价值，可增强学习动机与实际应用联系。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [176] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT通过知识图谱和可执行代码显著提升大模型在复杂数学推理和代码生成上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂推理任务中面临的准确性和可靠性不足的问题。

Method: 利用结构化任务图分解问题，结合GraphRAG进行知识检索，生成可验证的代码，实现数学推理。

Result: 在多个数学推理基准测试中显著优于现有方法，准确率提升数个百分点至十余个百分点。

Conclusion: KGA-ECoT是一种强大且具有高度通用性的框架，有效增强模型在复杂数学推理中的能力。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [177] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: 提出了一种名为GeoSR的自我优化的地理推理框架，通过多个合作代理整合地理原则，提高LLMs在空间任务中的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在空间一致性、多步推理和地理偏差方面的挑战，提升其地理预测能力。

Method: 引入由变量选择、点选择和细化三个代理组成的协作机制，结合地理原理进行循环预测优化。

Result: 在物理属性和社会经济预测任务中，GeoSR持续优于标准提示策略，证明结合地理先验和空间推理增强了模型性能。

Conclusion: 融合地理原理的代理循环机制显著改善了LLMs的地理任务表现，促进更准确和公平的空间预测。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [178] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 引入语义熵，用于衡量学生回答多种GPT-4生成解释的变化性，作为人工评分不确定性的代理，提升AI自动评分的透明性和可信度。


<details>
  <summary>Details</summary>
Motivation: 解决自动评分系统难以显示评分不确定性和潜在争议的问题，增强评分过程的透明度。

Method: 通过利用GPT-4生成多份解释，基于蕴含相似性进行聚类，计算其熵值，衡量解释多样性，并验证其与人工评分不一致性的关系。

Result: 发现语义熵与人类评分差异相关，跨学科有良好泛化能力，且在需要推理的任务中表现出更高的不确定性。

Conclusion: 语义熵为一种可解释的不确定性信号，有助于构建更透明、可信的AI辅助评分系统。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [179] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出一种结合分步骤合成和即时剪枝的LTLf反应式合成框架，有效应对状态空间爆炸问题和提升求解能力。


<details>
  <summary>Details</summary>
Motivation: 解决LTLf反应式合成中DFA构建导致的状态空间指数爆炸问题，提升大规模公式的求解效率。

Method: 引入一种在游戏求解过程中进行构成和剪枝的组合方法，利用自动机最小化和剪枝策略优化合成流程。

Result: 新框架能够解决比现有最先进方法更多的实例，分析显示两种合成策略各具优点。

Conclusion: 该框架有效融合了多种技术，提升了LTLf反应式合成的可扩展性和求解能力。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [180] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: 提出AgREE框架，通过多步推理与检索动态构建新兴实体的知识图，超越以往方法，且无需训练，刷新知识图的时效性。


<details>
  <summary>Details</summary>
Motivation: 解决知识图补全中面对新兴实体信息不足的问题，特别是在动态变化的新闻环境中。

Method: 引入基于代理的多步推理与检索机制，结合策略检索，有效构建新兴实体的知识图三元组。

Result: AgREE在新兴实体的知识图构建中效果显著优于现有方法，提升幅度达13.7%，且无需额外训练。

Conclusion: 结合代理推理与策略检索的方法，能有效维护动态、及时更新的知识图，为知识管理提供新思路。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [181] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 提出一种结合知识基础和数据驱动的推理学习架构，用于提升临时合作中AI代理的决策能力。


<details>
  <summary>Details</summary>
Motivation: 当前的临时合作方法依赖大规模标注数据，缺乏透明性，不易修正，尤其随着代理数量增加，合作复杂度提高。

Method: 结合非单调逻辑推理、领域知识、快速模型更新以及基于基础模型的抽象未来目标，设计融合知识和数据的架构。

Result: 在虚拟家庭环境中实验验证，展示架构的有效性。

Conclusion: 该架构有效结合知识与数据，提升临时合作中的AI代理表现，适应复杂情境变化。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [182] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD引入电路结构信息和GNN计算的条件概率，显著提升电路满足性问题的求解效率。


<details>
  <summary>Details</summary>
Motivation: 现有SAT求解器未充分利用电路结构信息，导致性能有限。

Method: 结合图神经网络（GNN）计算电路门级条件概率，以引导CDCL策略。

Result: 在实际基准测试中，CASCAD将求解时间缩短至传统方法的十分之一，增强了求解效率。

Conclusion: 将电路结构信息融入SAT求解，能显著改善性能，为电子设计自动化提供新途径。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [183] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio提出一种以参数效率为基础的生物医学推理框架，融合多能力，保证安全性和性能，通过创新的生成和优化方法，达到行业领先效果。


<details>
  <summary>Details</summary>
Motivation: 应对生物医学AI中的多能力集成和安全可靠性问题。

Method: 提出理论基础、多能力融合的平衡优化策略，结合医学知识的合成生成和能力感知的策略优化。

Result: 在多项指标中达到了最先进水平，显著提升性能并保证安全性，减少成本，增强临床应用接受度。

Conclusion: 为生物医学AI提供了系统性、参数高效且安全可靠的理论与实践框架，推动行业发展。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [184] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本论文提出了一套基于理论框架和线性过程动态的POMDP合成方法，可用于细粒度评估记忆增强强化学习的性能和挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的POMDP基准缺乏对记忆模型挑战程度的可控性，亟需更可调节的合成环境以细致评估记忆增强RL。

Method: 利用记忆需求结构、状态聚合及奖励再分配，通过线性过程动态构建定制化POMDP环境，设计从易到难的测试环境。

Result: 提出的POMDP合成方法经过实证验证，形成系列难度递增的环境，验证其在分析和设计中的有效性，为记忆模型的选择提供支持。

Conclusion: 本研究明确了在POMDP中记忆增强强化学习的挑战，提供了环境分析与设计的指导，优化了记忆模型的选择依据。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [185] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 引入 Deliberative Reasoning Network (DRN)，通过从概率最大化转向不确定性最小化的方法，提高语言模型的逻辑推理能力，增强模型的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在逻辑推理中的认知陷阱问题，尤其是在语义启发式与决定性证据冲突时的表现不足。

Method: 提出DRN，将逻辑推理重构为不确定性最小化，通过明确定义信念状态和证据整合过程实现可解释性，并结合判别模型和验证模块增强性能。

Result: 在LCR-1000基准测试中，DRN相比传统方法提升15.2%，结合验证模块后，最具挑战性的问题准确率由20%提升到80%，零样本泛化能力也显著增强。

Conclusion: DRN提出了一种面向未来可信AI的基础性推理框架，通过不确定性管理实现更强的推理能力和可解释性，有助于构建更可靠的系统。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [186] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个多模态评估基准，用于测试智能模型在动态、互动环境中的跨模态推理能力，揭示了当前模型在融合机制上的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前评估多模态模型的方式未能充分考察其在动态和互动环境中的智能表现，存在静态评估不足和模态瓶颈问题。

Method: 设计了五个游戏环境，通过协同和冲突场景测试六种先进的多模态模型的融合和推理能力，分析其在多模态任务中的表现。

Result: 发现模型在Memory任务中表现超越人类，但在需要强推理和策略规划的任务中表现不足，融合机制脆弱，在模态冲突下表现严重退化，有“少即是多”的奇异现象。

Conclusion: 提升AGI的关键在于超越单纯规模扩展，需重点解决模态融合的鲁棒性问题。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [187] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 提出了通过三种评估标准（SLP-tests）来检测人工系统是否具备类意识的可操作框架，基于范畴论将接口表现为关系底层与行为的映射，主张主观看似体验是关系实体的功能接口。


<details>
  <summary>Details</summary>
Motivation: 探索人工系统是否能拥有主观体验，从定义和操作层面解决难题。

Method: 提出SLP测试标准，借助范畴论模型接口表现，将主观体验视为关系实体的功能接口。

Result: 建立了一个可以实证检验人工意识的框架，提供了新的研究路径。

Conclusion: 将意识类属性转化为关系接口的表现，为人工意识的实证研究提供理论基础。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [188] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG提出了一种基于强化学习的GUI视觉定位方法，通过系统性实验证明，其在仅用少量训练样本的情况下，显著优于传统的监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大型语言模型（MLLMs）的不断发展，传统的监督微调（SFT）在GUI视觉定位中的高成本和数据需求变得不再必要，探索更高效的微调方式成为必要。

Method: 作者设计了GuirlVG，结合系统实验证明了各组成部分的优化，提出了对抗性KL因子以动态稳定训练，并优化了训练配置。

Result: GuirlVG在仅用5,200个样本的情况下，效果明显超越用超过千万样本训练的SFT方法，在多个指标上表现优异：比ScreenSpot的改进为7.7%，比ScreenSpotPro为17.2%，在ScreenSpotV2达到91.9%的准确率。

Conclusion: RFT在GUI-VG中的应用潜力巨大，但需合理设计和稳定训练技巧，GuirlVG通过创新方法展现出极高的效率与效果，具有广泛应用前景。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [189] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 提出D2Snap算法对DOM快照进行降采样，有效维持关键UI特征，同时降低模型输入规模。


<details>
  <summary>Details</summary>
Motivation: 解决网页应用状态序列化中DOM快照输入规模过大导致模型难以处理的问题。

Method: 基于GPT-4o后端设计DOM降采样算法D2Snap，并在Online-Mind2Web数据集进行评估。

Result: D2Snap在任务中的成功率达到67%，与基于GUI快照的基线相近，最佳配置超越基线8%。

Conclusion: DOM的层级结构为LLMs提供了强大的UI特征，有助于提升网页代理的性能和效率。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [190] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct通过模拟新手教师和专家互动，创新性地生成高质量、多轮次的教学对话数据，提升教学机器人研发效率。


<details>
  <summary>Details</summary>
Motivation: 缺乏隐私保护下高质量教学对话数据，限制AI教学支持的发展。

Method: 采用LLM模拟新手教师，结合人类专家逐轮提供反馈，生成逼真的教学对话，同时fine-tune LLaMA模型。

Result: SimInstruct生成的对话质量接近真实教学，专家反馈积极，模型在教学质量上优于GPT-4o。

Conclusion: SimInstruct有效解决数据稀缺问题，提升教学AI系统的开发效率，且模型细节表现需改进。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [191] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: MERA通过分离推理与控制机制，提升大规模推理模型的效率和准确性，解决过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 现有大模型展现出复杂推理能力，但缺乏调控机制，导致资源浪费和效率低下。

Method: 引入Foresight机制、监督微调和策略优化，分离推理与控制，构建高质量推理控制数据。

Result: 实验显示MERA显著提升推理效率和准确性。

Conclusion: MERA有效实现推理过程中的调控，提高模型实用性和性能。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [192] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 综述基于大型语言模型的操作系统代理，分析其关键组件、构建方法、评估标准，以及未来挑战与发展方向。


<details>
  <summary>Details</summary>
Motivation: 推动实现类似钢铁侠J.A.R.V.I.S的多能AI助手，提升自动化和智能交互能力。

Method: 系统梳理OS代理的基本概念、关键技术、构建框架、评估指标，并展望未来发展方向。

Result: 整理总结了OS代理的技术进展、评估方法与面临的挑战，提供了学术与工业的参考框架。

Conclusion: 强化技术基础，解决安全隐私、个性化、自我演化等关键问题，推动OS代理发展。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [193] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 提出一种基于论证的可解释偏见检测方法，强调其透明性和可理解性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在社会中的应用扩大，偏见问题亟需透明且易理解的检测手段，以确保公平性。

Method: 利用形式和计算论证技术，通过针对个体及其邻域中的偏见进行辩论，开发出一种具有可解释性的偏见检测方法。

Result: 该方法在性能、可解释性和透明性方面优于基线方法，验证其有效性。

Conclusion: 该研究强调了偏见检测中可解释性的重要性，并提出了具有潜在推广价值的方法。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [194] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 本论文提出了SID评估基准，用于系统检测大型语言模型在跨学科科学对话中的引导能力，发现现有模型尚难有效指导学生实现知识整合与迁移。


<details>
  <summary>Details</summary>
Motivation: 现代教育强调知识整合与迁移，但专家指导难以规模化。大型语言模型具潜力，但缺乏有效评估标准。

Method: 构建包含10,000对话轮次的跨学科STEM对话数据集，提出新颖的标注方案和评估指标（如X-SRG），进行基线实验。

Result: 即便是先进模型也难以实现有效引导，说明尚需改进模型的指导能力。

Conclusion: 本研究强调了SID评估基准的价值，推动更具教育导向的语言模型发展。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [195] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 该研究提出ConfProBench，一个系统评估多模态大模型推理步骤置信度可靠性的基准，强调模型置信度的鲁棒性、敏感性和校准性，为未来模型改进提供依据。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型推理判断中置信度的可靠性评估不足，影响模型性能优化与可信度提升。

Method: 设计多种对抗性扰动检验模型置信度，提出三项新评价指标，测试14种前沿模型。

Result: 发现现有模型在置信度表现上存在局限，为未来模型改进提供实证基础。

Conclusion: ConfProBench有效弥补现有评估不足，促进多模态模型置信度研究与提升。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [196] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 本文将大规模预训练语言模型（LLMs）视为多智能体强化学习（MARL）问题，提出了多智能体合作优化算法MAGRPO，增强了模型之间的合作能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs多为单独预训练，缺乏针对合作优化的机制，难以高效协作。

Method: 提出多智能体多轮协作算法MAGRPO，结合现有强化学习和MARL技术。

Result: 在写作和编码合作任务中，通过微调多智能体模型提升了合作效果和响应质量。

Conclusion: 利用MARL优化LLMs合作能力，为未来多智能体融合提供新路径，也指出了面临的挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [197] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent使CUA能自主适应新软件环境，通过试错和任务演进提升能力，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在新颖和专业软件环境中受限，亟需自主学习能力以应对未知复杂任务。

Method: 设计世界状态模型、任务生成器和专家到通用训练策略，通过试错学习提升CUA能力。

Result: 在五个新软件环境中，成功率提升23.2%，优于现有开源CUA。

Conclusion: SEAgent有效增强CUA的自主学习和适应能力，实现持续演化，推动软件环境下的人工智能发展。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>
