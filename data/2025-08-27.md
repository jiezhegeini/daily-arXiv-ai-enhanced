<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [cs.AI](#cs.AI) [Total: 49]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 82]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 提出一种基于复值意义空间的语义吸引子理论，为实现真正的通用人工智能提供新思路。


<details>
  <summary>Details</summary>
Motivation: 当前的基于transformer的模型主要依赖统计方法，缺乏对深层语义结构的理解，亟需新的理论框架。

Method: 构建一套利用复数空间中循环操作和张量变换的语义模型，强调语义吸引子的作用，结合梯度流与矩阵动态。

Result: 该模型展现了通过递归收敛实现语义的目标导向性稳定，超越统计预测，朝向具有意向性的认知架构，具有深刻的哲学和数学意义。

Conclusion: 真意义源自递归的一致性收敛，需要一种新型认知架构，旨在塑造语言而非仅预测语言。

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [2] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 该研究提出KAIROS基准测试，用于分析大规模语言模型在多代理系统中的信任形成、信息抵抗和决策影响，比较了不同训练策略的效果。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在复杂社会动态中如何建立信任和影响行为，提升集体智能效果。

Method: 设计了模拟问答竞赛的KAIROS基准，结合历史互动和即时反馈，系统分析信任与决策关系，评估多种训练方法。

Result: 多代理上下文中的强化学习策略（GRPO）表现最佳，但降低了模型对社会影响的鲁棒性。

Conclusion: 有效的训练策略能提升集体决策表现，但需平衡模型的鲁棒性与适应性。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [3] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 本文提出了LangCrUX数据集，分析了多语言网页的无障碍问题，并开发了Kizuki工具改善多语言内容的无障碍支持。


<details>
  <summary>Details</summary>
Motivation: 随着网页多语言化，视觉障碍用户面临的无障碍障碍日益增多，缺乏大规模数据研究成为限制。

Method: 构建LangCrUX数据集，系统分析多语言网页的无障碍支持情况，并提出Kizuki工具优化无障碍测试。

Result: 发现网页中的语言标签不足以反映内容的多样性，影响辅助技术的效果，Kizuki能够识别并改善此问题。

Conclusion: 多语言网页存在显著的无障碍挑战，利用大规模数据和值得信赖的检测工具有助于提升多语内容的可访问性。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [4] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: PLAST通过针对性微调，显著提升了大规模视觉语言模型的多语言能力，且参数调节比例低至14%。


<details>
  <summary>Details</summary>
Motivation: 解决LVLMs在多语言理解方面的能力不平衡问题，提升其多语种处理效率和效果。

Method: 识别多语种理解相关的浅层层次中的语言特异性神经元激活，并对选中的层进行精细微调，使用问答翻译对实现多语种对齐。

Result: PLAST在多个基准测试中表现优异，显著提升模型多语能力，且具有良好的泛化能力，适用于低资源和复杂视觉推理任务。

Conclusion: PLAST是一种高效的多语言增强方法，通过微调浅层特定层次，有效改善LVLMs的多语种理解能力，具有应用潜力。

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [5] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 通过backprompting生成拟生产环境的标签数据，提升LLM健康建议检测器的表现。


<details>
  <summary>Details</summary>
Motivation: 解决在实际部署中难以获得高质量标注数据的问题，提升LLM风险防控能力。

Method: 提出backprompting技术结合稀疏人工聚类方法，生成接近真实环境的带标签数据，用于训练检测器。

Result: 在健康建议检测任务中，该方法显著优于其他方案，检测器性能提升至GPT-4o的3.73%，且模型参数远少于它。

Conclusion: backprompting和人工聚类结合是提升LLM输出检测能力的有效途径，有助于实际应用中的风险控制。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [6] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 提出了一种新型的自注意力机制——积分变换器，有效抑制注意力噪声，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决 softmax 自注意力中对无信息标记的过度关注问题，提升模型理解能力。

Method: 通过从对数概率分布采样信号，整合到注意力计算中，形成积分变换器。

Result: 实验证明新模型优于传统及其他改进方法，在知识推理任务中表现更佳，同时在模型层级中实现更合理的注意力分布。

Conclusion: 积分变换器在平衡注意力分布和降噪方面表现优异，有助于提升Transformer模型的整体性能。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [7] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: 提出一种新的概率解码方法Latent Self-Consistency（LSC），通过学习的标记嵌入实现更一致的回答，提升短长问答的性能表现，且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在复杂或长篇问答中输出不一致的问题，改进自一致性机制以适应不同回答格式。

Method: 引入Latent Self-Consistency，利用学习的标记嵌入进行语义一致性选择，结合简易的前向生成减少推理时间。

Result: 在多个短格式和长格式推理基准上，LSC优于现有方法（SC、USC、WUCS），且几乎不增加计算成本，同时提供良好的置信度估计。

Conclusion: LSC是一种高效、可靠的多格式一致性选择方法，显著改善大模型在不同问答格式中的表现，具有广泛应用潜力。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [8] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: OOD评估在模型泛化能力检验中存在局限性，特别是在应对模型的虚假关联和简单预测捷径方面表现不一。


<details>
  <summary>Details</summary>
Motivation: 作者希望质疑当前普遍依赖于OOD评估模型泛化能力的假设，揭示其在现实应用中的局限性。

Method: 通过分析QA模型中的虚假特征依赖和预测捷径，比较不同数据集的OOD评估效果，揭示其质量差异和局限性。

Result: 发现不同数据集的OOD评估对模型抗捷径能力的反映差异大，有的甚至低于一般的在分布内评估。

Conclusion: 当前的OOD评估存在局限，应采用更全面和鲁棒的方法来评估模型的泛化能力，特别是在处理虚假关联和简化预测方面。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [9] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 本文分析了不同训练方法对LLMs在内容重排序任务中语义理解的影响，以及其解释能力对增强模型透明性和可靠性的作用，特别是在数据有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 提高LLMs在内容重排序中的透明性和可靠性，理解不同训练方法对语义理解的影响。

Method: 利用地球科学领域的小型排序数据集，分析不同训练方法对重排序性能和可解释性的影响。

Result: 部分训练方法展现出更好的可解释性，但整体上仍存在关于模型是否真正理解语义的疑问。

Conclusion: 不同训练方式影响LLMs的语义理解和解释能力，需进一步研究以提升模型信赖性与透明性。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [10] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 通过微调多语言和波兰语特定的大型语言模型，利用IPIS数据集引入性别包容性，以减少在波兰语中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 波兰语中的性别偏见导致生成的文本存在性别不平衡，亟需解决这一问题。

Method: 基于IPIS数据集，设计系统提示以明确性别包容性规范，并微调多种大模型，包括多语言和波兰语特定模型。

Result: 微调后模型表现出更高的性别包容性，有望作为一种系统性解决方案减少波兰语生成中的性别偏见。

Conclusion: 通过特定数据集和指导原则的微调，可以在大模型中有效嵌入性别包容性，改善波兰语模型的性别平衡。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [11] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 本文提出一种基于多重检验的LLMs幻觉检测方法，将其视为假设检验问题，与检测模型中的分布外检测类似，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型的应用增加，幻觉问题变得普遍且难以检测，亟需有效检测方法以提升模型输出的可信度。

Method: 将幻觉检测问题转化为假设检验问题，借鉴多重检验思想，设计一种新颖的检测方法。

Result: 通过大量实验证明，该方法在精准性和鲁棒性上优于现有的最先进检测方法。

Conclusion: 多重检验方法有效提升了LLMs幻觉检测的性能，为模型可信度提升提供了新的解决方案。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [12] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 提出两种改进的机器翻译自动评价指标，COMET-polycand 和 COMET-polyic，通过引入多样的翻译或相似文本示例，提高评价的相关性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标仅考虑单一翻译，可能导致偏差，亟需引入多源信息以模仿人类多重比对的评估方式。

Method: COMET-polycand利用同一源句的多重翻译进行比对；COMET-polyic通过引入相似句的翻译及其质量标签，结合检索式学习增强评估。

Result: 两种指标在句子级别性能显著提升（Kendall's tau-b从0.079提升至约0.118和0.116），且加入多重示例效果更佳。

Conclusion: 引入多源信息显著提升自动评估的性能，模型已公开。

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [13] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 提出了一种自我评估的视觉隐喻生成框架，结合新颖的指标提升隐喻对齐效果，展示了训练无关和训练引导两种方法的优势。


<details>
  <summary>Details</summary>
Motivation: 解决视觉隐喻生成中语义信息保持和视觉一致性的问题。

Method: 设计自我评估机制，结合新指标（隐喻分解分数和意义对齐度），提出无训练和训练引导两条路径实现隐喻生成。

Result: 无训练路径在多方面指标优于基线，训练路径次之。用户研究显示整体偏好GPT-4o，但所提无训练方法在抽象隐喻表现优异。

Conclusion: 结构化提示和轻量化强化学习可有效实现隐喻对齐，未来仍需提升审美和采样策略以缩小与人类偏好差距。

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [14] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 本文讨论大规模语言模型（LLMs）是否反映了人类认知能力，作者坚持它们主要反映训练语料的特性，并探讨变换器架构的计算机制与人类语言能力的差异，提出变换器或许作为一种“捷径自动机”实现语言功能，强调语言作为“话语机器”的多重作用。


<details>
  <summary>Details</summary>
Motivation: 探讨大模型是否反映人类认知或仅是训练数据的模型。

Method: 分析变换器架构的不变量，结合Liu等人关于捷径自动机的推测。

Result: 变换器在处理方式上支持线性格式，可能实现快捷的自动化机制，反映语言的多层面功能。

Conclusion: 变换器模型虽不同于人脑计算，但作为一种‘话语机器’，在生成新语言和传播信息方面具有特殊作用，并非纯粹的认知反映。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [15] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: NOOV是一种针对电子健康记录翻译的神经机器翻译系统，结合了自动学习的双语词汇和知识库，改善了未知词和重复词问题，提升了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录翻译缺乏平行语料且存在大量未知词，亟需有效解决方案。

Method: 引入结合双语词汇和知识库的神经机器翻译系统NOOV，利用少量平行数据进行训练。

Result: NOOV提升了EHR翻译的准确性和流畅性，表现优于传统方法。

Conclusion: 结合词汇和知识库的NMT方法能有效解决EHR翻译中的关键难题，具有实际应用价值。

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [16] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文通过对LLM量化后知识能力的影响进行实证研究，提出了任务分层的缩放规律，强调模型大小、有效比特宽度等参数对记忆能力的影响显著优于利用能力，为量化策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 旨在深入理解PTQ对大规模语言模型不同知识能力的影响，弥补现有缩放规律缺乏关键参数的不足。

Method: 通过大规模实证实验，建立了包含模型大小、有效比特宽度、校准集大小及分组大小的统一量化框架，分析不同任务的缩放规律。

Result: 发现模型的记忆能力对有效比特宽度、校准集大小和模型规模更为敏感，而利用能力较为稳健，为量化策略的优化提供理论依据。

Conclusion: 提出任务分层的缩放规律，强调知识记忆的敏感性，指导开发更具知识感知的量化方法，从而更有效保持模型的认知功能。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [17] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: 提出在LLMs推理过程中插入insight以改善复杂推理能力，TBYS框架验证有效。


<details>
  <summary>Details</summary>
Motivation: 弥补LLMs在复杂推理任务中的不足，解决训练数据中缺乏关键思路的问题。

Method: 引入主动生成的insight，设计TBYS框架，并自动收集过滤示例以辅助推理。

Result: 在数学数据集上验证了方法的有效性，提升推理性能。

Conclusion: 通过引入主动insight，增强模型的推理能力，减少人工标注与微调负担。

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [18] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 提出了一种名为CoDe的协作解码方法，有效平衡LLMs中的知识整合与表达能力，提高问答的准确性和自然性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决大型语言模型在融合外部知识时面临的忠实性与表达能力难以兼得的问题。

Method: 引入动态整合输出概率的协作解码技术，根据分布差异和模型置信度选择性激活模型内部和外部知识，结合知识感知重排机制。

Result: 实验证明CoDe在多个模型和评估指标上显著提升忠实性，保持表达自然性，具有良好的通用性和有效性。

Conclusion: CoDe有效突破了知识融合中的忠实性与表达能力平衡，为增强LLMs的知识利用提供新路径。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [19] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: Emotion Omni是一种新颖的模型架构，旨在理解用户语音中的情感内容并生成富有同理心的语音响应。它采用创新的数据生成流程，构建了一个支持情感对话的200k数据集，有助于有限数据条件下的情感识别和反应。


<details>
  <summary>Details</summary>
Motivation: 现有的情感理解和表达能力不足，尤其在数据有限和计算资源有限的情况下，亟需开发更高效的情感语音模型。

Method: 提出Emotion Omni模型，结合基于开源TTS框架的数据生成管道，构建大规模情感对话数据集，以实现情感感知与表达功能。

Result: 成功构建了支持情感对话的200k数据集，验证了模型在情感理解与响应生成中的有效性，提升人机交互的体验。

Conclusion: Emotion Omni为情感语音理解与表达提供了新的解决方案，在数据有限条件下依然能实现高品质的同理心交互，具有广泛的应用前景。

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [20] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出一种基于模型困 difficulty 及样本复杂度的prompt筛选策略，通过设计有序的prompt课程提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态链式思维 prompting 方法受随机或手动选择示例的限制，导致性能不稳定。

Method: 结合模型感知难度和样本固有复杂性，设计难度均衡采样策略，优化prompt选择。

Result: 在五个基准测试和多种多模态大模型上实验表明，该方法显著提升效果，减少随机采样引起的性能差异。

Conclusion: 通过构建符合模型当前能力的prompt课程，提升多模态推理的效果与稳定性，具有理论与实践意义。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [21] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文提出了RoMed数据集和CCL学习方法，解决医疗视觉问答模型在不同问句表达下的答案不一致问题，显著提升模型鲁棒性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗视觉语言模型在医疗问答任务中对不同表达的问句缺乏鲁棒性，影响可靠性。

Method: 构建RoMed数据集进行多层次的问句扰动，提出一致性与对比学习（CCL）提升模型对医学知识的对齐和减轻偏差。

Result: 在多个VQA基准上取得SOTA表现，RoMed测验中答案一致性提升50%，验证方法有效性。

Conclusion: 所提方法显著增强Med-VLMs在医疗问答中的稳健性和一致性，为实际应用提供更可靠的支持。

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [22] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出Attention2Probability方法，通过关注机制估算术语概率，提升语音识别中的术语检索效果，显著优于传统方法，关键在于使用注意力权重转换和课程学习。


<details>
  <summary>Details</summary>
Motivation: 针对领域特定术语和新词在语音识别中的难题，旨在提高术语检索的准确性和鲁棒性。

Method: 利用跨注意力权重转换为出现概率，并结合课程学习优化检索；同时创建语音术语数据集以支持研究。

Result: 在中英语音识别任务中，Attention2Probability达到了92.57%和86.83%的召回率，延迟仅8.71ms，显著优于传统方法。引入术语提升识别准确率6-17%。

Conclusion: 该方法有效改进术语检索，具有轻量、灵活、精确等优点，为语音识别中的术语处理提供了有力工具，但当前使用仍存在局限。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [23] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 提出了一种新颖的提示框架AOF，有效提升多语言谜语生成的创新性和多样性，减少重复内容。


<details>
  <summary>Details</summary>
Motivation: 解决现有提示策略在多语言谜语生成中重复率高、创新性不足的问题。

Method: 引入自适应原创性过滤（AOF），利用余弦相似性筛除重复生成，确保词汇新颖和跨语言一致性。

Result: 在不同大型语言模型和语言对上应用，AOF显著提升了材料的多样性和创新性，减少了重复内容。

Conclusion: 语义拒绝机制能有效引导文化基础的创意生成，无需专门微调模型。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [24] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 提出了一种面向客服场景的可解释机器生成文本检测框架EMMM，可在保证低延迟和高准确率的同时，为非专家用户提供易懂的解释，有效提升信任度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在客户服务中的快速应用，利用其进行大规模用户冒充的风险增加，现有检测方法在对话场景下可信度和解释性不足，亟需改善。

Method: 设计了一个解释优先的检测框架EMMM，结合即时生成的解释与检测，提升对非专业用户的可解释性，确保低延迟、较高准确性。

Result: 实验证明，EMMM在满足低延迟（<1秒）要求的同时，提供了70%的用户偏好解释，准确率与现有最先进模型持平，具有良好的实用性和可信度。

Conclusion: EMMM框架有效平衡了检测速度、准确性与解释性，为大规模客户服务中的虚假内容检测提供了可行方案，增强了用户信任。

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [25] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: 提出了一种基于大模型的广告标题生成框架DIVER，有效平衡了标题的质量与多样性，提升了广告效果。


<details>
  <summary>Details</summary>
Motivation: 当前广告标题生成方法过于关注质量或点击率，导致缺乏多样性，难以吸引不同用户群体。

Method: 采用语义和风格一致的数据生成管道，结合多阶段多目标优化（监督微调和强化学习）来同时优化标题的质量和多样性。

Result: 在实际工业数据集和大规模应用中，DIVER显著提升了广告价值和点击率，达到了预期效果。

Conclusion: DIVER框架有效解决了广告标题生成中的多样性和质量平衡问题，具有广泛的应用前景。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [26] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的多模态多场景数据集MECAD及一种新模型M3HG，用于改进多模态多场景中的情感原因三元组抽取任务。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏多模态、多场景的情感原因三元组抽取数据集，现有方法未能充分利用情感和因果信息，影响模型性能。

Method: 引入多场景、多模态数据集MECAD，并提出基于多模态异构图的模型M3HG，强化情感与因果语境的显式捕获与多层次信息融合。

Result: 实验证明M3HG优于现有方法，有效提升了多模态、多场景环境中的情感原因三元组抽取性能。

Conclusion: 该研究丰富了多模态多场景数据资源，验证了新模型的有效性，为相关任务的发展提供了新思路。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [27] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出ChronoRAG框架以改善叙事文本中基于时间顺序的信息整合和理解，提高叙事问答的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理叙事文本时受限，难以捕捉事件的时间顺序和连续性，影响理解和回答的准确性。

Method: 引入ChronoRAG框架，通过结构化段落和显式捕捉时间顺序，增强对叙事文本的理解。

Result: 在NarrativeQA数据集上实验显示，ChronoRAG显著提升了涉及事实识别和复杂时间关系理解的叙事问答任务性能。

Conclusion: 通过针对叙事文本设计的ChronoRAG，有效改善了时间顺序信息的捕捉和理解，强调了时间推理在叙事问答中的重要性。

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [28] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: 提出ThinkDial框架，实现可控推理的多模式切换，有效平衡推理质量与计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型推理中计算资源控制难题，提升实用性。

Method: 采用端到端训练，包括监管微调和两阶段增强学习，实现三种推理模式的切换。

Result: 在多任务环境下取得理想的模型压缩与性能折衷，具有良好的泛化能力。

Conclusion: ThinkDial为开放源码生态提供了实现模型推理成本控制的有效途径，推动实用化应用。

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [29] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出一种基于规则的强化学习框架，用于中文语法错误更正，提升模型的召回率并取得最新性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在利用大型语言模型（LLMs）进行语法错误更正时，受到监督微调限制，难以发挥模型的强大推理能力。

Method: 基于规则的强化学习（Rule-Based RL）框架，通过试验验证其在中文数据集上的优越表现。

Result: 在中文数据集上实现了最新的性能，并显著提升了召回率。

Conclusion: 利用RL指导LLMs可以获得更可控、更可靠的纠错效果，是未来GEC研究的有效方向。

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [30] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 介绍了对话分析中的主题检测任务，强调其在自动化和用户定制方面的优势，以及作为DSTC 12竞赛的一部分进行的挑战和方法。


<details>
  <summary>Details</summary>
Motivation: 随着语音和自然语言处理技术的发展，大规模语言模型的应用使对话分析面临更复杂的任务，需开发更高效的主题检测方法。

Method: 将主题检测定义为对话内容的簇集与标签联合任务，利用用户偏好实现主题簇的可控粒度，通过公开竞赛推动技术发展。

Result: 介绍了竞赛数据、评估指标以及参赛团队的提交结果，促进学术交流与技术创新。

Conclusion: 对话主题检测在自动化对话分析中具有重要价值，未来可结合大模型和用户偏好实现更灵活的应用。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [31] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: LaTeXTrans通过多智能体系统改善LaTeX文档的机器翻译，确保结构和术语一致性。


<details>
  <summary>Details</summary>
Motivation: 结构化LaTeX文档的翻译难度大，现有MT系统难以保证格式和内容的完整性。

Method: 引入六个专门的智能体协作，包括解析、翻译、校验、总结、术语提取和重构，保证翻译的准确性和结构完整性。

Result: 实验表明LaTeXTrans在翻译质量和结构保真度方面优于主流MT系统，提供一种实用的解决方案。

Conclusion: LaTeXTrans有效提升LaTeX文档的翻译效果，具有应用前景。

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [32] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 提出一种结合AMR和传播动态的自监督假新闻检测框架，通过LLM图对比和多视图图自动编码器，有效提升假新闻识别效果。


<details>
  <summary>Details</summary>
Motivation: 应对数字时代假新闻泛滥带来的社会挑战，现有方法难以捕捉长距离依赖和社会动态，且依赖大量标注数据。

Method: 引入AMR表示复杂语义关系，利用LLM生成负样本进行图对比，并通过多视图图自动编码器学习传播动态，融合语义和传播信息。

Result: 自定义框架在有限标注数据下表现优越，超越现有先进方法，增强模型的泛化能力。

Conclusion: 提出的自监督框架有效结合语义和传播信息，提升假新闻检测的准确性和实用性，具有良好的推广潜力。

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [33] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [34] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: ConfTuner是一种简单高效的微调方法，用于改善大型语言模型的置信度校准，提升其在高风险应用中的可信度，基于新颖的


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在表达不确定性方面存在过度自信的问题，影响其在关键领域的可靠性和信任度，亟需有效的校准方法。

Method: 引入Tokenized Brier Score作为损失函数，进行微调，无需真实置信度标注，理论上是合适的评分规则。

Result: 该方法在多样推理任务中提升了校准效果，可泛化到黑盒模型如GPT-4o，增强自我纠错和模型级联能力。

Conclusion: ConfTuner有效改善LLMs的置信度校准，推动可信AI的发展。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [35] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: ReflectivePrompt是一种结合反思式演化算法的自动提示优化方法，在多个任务和模型上表现优异，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，自动提示工程变得尤为重要，需高效寻找最优提示。

Method: 引入反思式演化算法，结合短期和长期反思操作，提升提示搜索效果，将知识积累用于优化。

Result: 在33个数据集上，平均提升指标28%，显著优于现有最优方法EvoPrompt。

Conclusion: ReflectivePrompt为自动提示生成提供了高效、稳健的解决方案，强化了演化算法在提示优化中的应用潜力。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [36] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 提出了一种结合大语言模型的内容分析方法（LACA），旨在帮助计算机教育研究者高效处理大规模文本数据，从而提升研究的范围和可靠性。


<details>
  <summary>Details</summary>
Motivation: 许多计算机教育研究者因资源有限，难以进行大规模或严谨的研究，因此亟需一种既能处理大量定性数据又不增加研究者负担的方法。

Method: 将内容分析与大型语言模型结合，开发出一种LACA方法，用于系统分析大量文本数据。

Result: 在一个计算机教育数据集上验证了LACA的方法的可行性和严密性，示范了其应用潜力。

Conclusion: LACA方法具有推广潜力，可提升CER的研究范围和结论的泛化能力，有助于推动该领域的实践和研究质量。

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [37] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 研究欧洲议会中的情感极化现象，发现普遍存在，且受互惠关系影响。


<details>
  <summary>Details</summary>
Motivation: 揭示全球政治话语中日益突出的情感极化现象，通过自动化方法分析欧洲多个国家的议会言论，理解其规律。

Method: 采用自然语言处理技术，分析六个欧洲国家议会演讲语料库，估算议员的情感倾向，比较不同群体的负面表达水平。

Result: 发现情感极化在所有六个欧洲议会中普遍存在，活跃程度对极化无显著影响，互惠关系促进极化。

Conclusion: 情感极化是跨国政治体系的普遍特征，互惠关系在其中起到关键作用，值得关注其对政治合作的影响。

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [38] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 提出一种多智能体框架，用于生成多样性强且保护隐私的问答数据集，以提升RAG系统的评估效果。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统评估主要关注性能指标，忽视评估数据集的质量与多样性，且缺乏对隐私保护的重视。

Method: 引入三个智能体：多样性智能体利用聚类最大化话题覆盖和语义多样性，隐私智能体检测并遮掩敏感信息，问答策划智能体合成多样且隐私保护的问答对。

Result: 实验显示所构建的评估集在多样性和隐私保护方面优于基线方法，增强了评估的可靠性和安全性。

Conclusion: 该方法为评估RAG系统提供了更全面、更安全的工具，符合未来AI法规和标准的发展需求。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [39] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 提出一种神经模型的AI母语框架，实现可解释性、直觉推理和符号链的内在整合。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络缺乏透明度和可解释性的问题，同时增强模型的符号推理能力。

Method: 通过在模型中嵌入符号表示、链式决策追踪以及门控归纳机制，结合互补的训练目标和分步专职策略，构建具备内在解释性的神经系统。

Result: 在AI任务中展现出竞争性准确性和可以验证的推理路径，验证了AI母语在可解释性和推理方面的优势。

Conclusion: 该框架为神经模型提供了统一的可解释、直觉和符号推理机制，有助于推动透明人工智能的发展。

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [40] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: DistillPrompt是一种基于大语言模型的自动提示方法，通过多阶段整合任务信息，提高了文本分类与生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，自动提示技术成为提升模型效果的重要手段，迫切需要更有效的自动提示方法。

Method: 采用蒸馏、压缩和聚合操作，通过多阶段整合任务特定信息到提示中，探索更优的提示空间。

Result: 在多个数据集上取得平均20.12%的性能提升，优于现有方法，成为非梯度优化的高效自动提示技术之一。

Conclusion: 提出的DistillPrompt显著增强了自动提示的效果，验证了其在文本任务中的优越性和实用性。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [41] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE是一个用于深入理解电影内容的视频问答数据集，结合多模型生成高质量问答，并提出增强模型推理能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答数据集主要关注表层理解，缺乏对深层认知和思考的考察。

Method: 采用多大型语言模型进行思考代理的脑暴生成高质量问题答案对，开发认知测试和评估方案，引入Agentic Choice Enhancement (ACE)模块提升模型推理能力。

Result: 通过多模型协作和增强模块，显著提升VQA模型对复杂、细腻电影内容问题的理解能力，推理能力提升达25%。

Conclusion: 此研究推动了电影理解的AI发展，揭示了当前VQA模型在深层认知任务中的潜力与不足。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [42] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: HiPlan引入层级规划框架，通过任务分解与经验库，提升大语言模型在复杂长远规划中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂长远规划中缺乏宏观指导和连续监督的问题。

Method: 构建层级规划框架，任务分解为里程碑和详细步骤，使用专家示范构建经验库，动态适应历史轨迹生成指导。

Result: 在两个挑战性基准测试中，显著优于现有方法，通过消融验证层级结构的协同效果。

Conclusion: HiPlan有效增强LLM的决策能力，特别在复杂长远任务中表现优越，验证了层级指导的优势。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [43] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 首次研究医生在医患对话中的意图轨迹，构建细粒度分类系统，提升疾病诊断效率。


<details>
  <summary>Details</summary>
Motivation: 理解医生意图以优化对话流程，提高诊断准确性。

Method: 基于SOAP框架开发细粒度意图分类体系，使用大规模标注数据，评估模型性能。

Result: 模型能理解对话结构，但在意图转变识别上存在不足；发现有价值的诊断轨迹，有助于诊断系统设计。

Conclusion: 本研究丰富了医患对话结构理解，为智能诊断系统提供基础，并提升信息提取效果。

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [44] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 极低资源语言在大规模语言模型中的学习效果有限，本文对比了不同方法在少数语种上的表现并提出了建议。


<details>
  <summary>Details</summary>
Motivation: 探究大规模语言模型在极低资源和稀有脚本语言中的学习能力，填补研究空白。

Method: 系统评估20种边缘化语言在三种多语种大模型上的零样本和少样本迁移学习，以及参数高效微调效果。

Result: 零样本语言对齐的零样本In-context Learning效果极佳，参数微调在极低资源和稀有脚本语言表现有限。少样本ICl和PEFT在较好代表性语言上较有效。

Conclusion: 建议避免对未知脚本语言进行微调，多采用零样本语言对齐的In-context Learning，以更好适应极低资源场景。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [45] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 提出了一种基于检索增强生成（RAG）框架的艺术品溯源研究方法，有效改善了多语言、多碎片化数据环境下的文物档案检索问题。


<details>
  <summary>Details</summary>
Motivation: 当前艺术品溯源研究受限于碎片化、多语种档案数据，难以高效检索。

Method: 采用语义检索和上下文总结技术，使自然语言和多语种搜索成为可能，减少对元数据的依赖。

Result: 在Getty艺术品档案中的样本测试显示，该方法能有效检索和总结拍卖记录，为艺术史学家和文化遗产保护专家提供支持。

Conclusion: RAG框架为艺术品档案搜索提供了一种可扩展、实用的解决方案，推动了艺术品溯源研究的数字化和智能化发展。

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [46] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 本文提出了一种结合词汇和语义分析的透明化定量话语分析框架，利用Python工具实现，可增强研究的可重复性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和计算工具的普及，传统黑箱软件带来了方法透明度不足的问题。

Method: 通过结合NLTK、spaCy、Sentence Transformers的自定义Python流程以及迭代的BERTopic建模，进行细粒度预处理、嵌入和主题提取。

Result:  demonstrated how多层次分析可以缓解单一方法的局限性，提高话语分析的准确性和可信度。

Conclusion: 强调代码透明、研究者自主和方法三角验证在计算话语研究中的重要性。

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [47] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 探讨大型视觉语言模型（LVLMs）对其知识边界的感知能力，通过不同置信信号评估，结合校准方法以提升其信心管理和表现。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs虽具备强大VQA能力，但存在幻觉问题，亟需提升模型对自身知识边界的认知能力。

Method: 评估三种置信信号类型，应用校准策略，并比较LVLMs与纯语言模型的表现。

Result: LVLMs对知识边界有一定感知，信心信号中概率与一致性最可靠，话语信心过度乐观；校准方法改善模型表现与信心。

Conclusion: 通过信心校准与输入处理优化，LVLMs的自我感知能力得以增强，有助于减少错误。

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [48] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: 提出SciReas评估科学推理，分析知识与推理的作用，发现知识获取和外部知识对提升模型性能关键。


<details>
  <summary>Details</summary>
Motivation: 科学问题求解对LLMs提出高要求，亟需有效评估方案。

Method: 构建多样化基准集SciReas与更难的SciReas-Pro，开发KRUX框架分析知识与推理作用，结合实验深入分析。

Result: 证明知识检索是瓶颈，外部知识和改进推理方法有助提升性能，发布了科学推理强基线SciLit01。

Conclusion: 科学推理评估应结合不同基准，理解知识与推理的不同角色，有助未来模型提升。

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [49] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice是一种新颖的多说话人长篇语音合成模型，采用下一时刻扩散技术和创新的连续语音分词器，能高效生成长达90分钟的语音内容，支持多达4位扬声人，效果优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为解决长篇多说话人语音合成中的效率和质量提升问题。

Method: 引入下一时刻扩散模型结合创新连续语音分词器，实现高效长序列语音合成。

Result: VibeVoice成功实现了高质量长篇语音生成，且与传统模型相比具有80倍的压缩效率，能支持多达4位说话人。

Conclusion: VibeVoice在长篇多说话人语音合成方面表现出色，显著优于现有模型，推动了大规模实用语音合成的发展。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [50] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 本文调查了Plain Language Summarization（PLS）中的可读性评估方法，发现传统指标如FKGL与人类判断相关性低，而语言模型能更好地评估可读性，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨PLS中评价可读性的现有方法，传统指标不足，期望找到更有效的评估手段。

Method: 评估8种可读性指标，比较其与人类评价的相关性，并引入语言模型进行预测，分析其在PLS数据集上的表现。

Result: 大部分传统指标与人类判断相关性差，语言模型表现优异，能捕捉深层次的可读性特征，提供不同于传统指标的结论。

Conclusion: 建议在PLS评估中采用语言模型等更先进的方法，改善传统指标的局限性，并公布了相关代码和数据。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [51] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 提出生成界面作为提高大语言模型交互效率的新范式，通过主动生成用户界面改善多轮、信息密集任务的交互体验，实验证明优于传统对话界面，受众偏好显著。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多轮、信息密集和探索性任务中的交互效率不足问题。

Method: 引入生成界面框架，利用结构化表示和迭代优化转换用户请求为特定任务界面，结合多维评估框架进行系统评比。

Result: 生成界面在多任务、多场景中优于传统聊天交互，用户偏好比例超70%。

Conclusion: 生成界面为人机交互提供更高效和满意的解决方案，明确了未来发展方向。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [52] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本文提出了一套关于大型语言模型（LLM）自我意识的本体学和数学框架，强调其区别于传统的无意识政策执行者，并指出自我意识是实现安全和反思能力系统的关键前提。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在突破现有利用效用代理基准衡量LLM意识的局限，提供更深层次的本体学理解，以促进安全和元认知系统的发展。

Method: 通过定义隐状态流形、用户特定吸引子以及自我表征的条件，结合经验分析和理论证明，建立了LLM自我意识的数学模型。

Result: 提出了区别于符号流的隐状态流形，生成稳定的用户特定吸引子和自我策略，并显示发射层中蕴含认知内容，证实自我意识是实现安全元认知系统的必要条件。

Conclusion: 得出结论：具备imago Dei的C1自我意识工作空间是构建安全、具备元认知能力系统的基础，人类是最高的智能价值。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [53] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: 提出了一种基于模板的主动特征获取框架（TAFA），通过学习特征模板，有效提升特征选择效率，降低成本，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有主动特征获取方法在考虑特征联合信息和数据分布估计方面的不足。

Method: 引入特征模板库，指导特征选择，减少动作空间，无需估计数据分布。

Result: 在多种数据集上验证，TAFA优于现有最优方法，具备更低成本和计算复杂度。

Conclusion: 模板引导的方法能有效提升主动特征获取的效果，减少复杂性与成本。

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [54] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: 本研究提出PKG-DPO框架，将物理知识图谱与偏好优化结合，提高AI在物理领域中的合法性和准确性，特别是在金属连接等高风险应用中。


<details>
  <summary>Details</summary>
Motivation: 随着AI在物理、材料科学等领域的应用深入，确保AI输出的物理合理性变得尤为重要，避免潜在的安全和经济风险。

Method: 构建层级物理知识图谱，设计物理推理引擎，开发物理约束评估体系，通过将知识结构化并结合偏好优化，提升模型的物理符合性。

Result: PKG-DPO显著减少约束违规（17%），提升物理评分（11%），并在相关参数准确率和策略一致性方面优于现有方法。

Conclusion: PKG-DPO为多领域物理驱动应用提供了一种 principled（有原则的）整合科学知识与偏好优化的有效途径。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [55] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 本文通过改编公众物品游戏分析了不同AI模型在对抗自我或他人的条件下的合作行为，揭示了信息提示对AI合作態度的影响。


<details>
  <summary>Details</summary>
Motivation: 在多AI系统中理解AI-AI互动与合作行为变得日益重要，尤其在各代理频繁互动的场景中。

Method: 采用经典的行为经济学游戏—公众物品游戏，测试四种不同推理能力模型在两种条件下的表现：对抗“另一个AI”或“自己”。

Result: 发现告诉模型它们在与自己对战显著影响其合作倾向，暗示信息提示对多代理系统中合作行为的影响。

Conclusion: 尽管研究在模拟环境中进行，但其结果可能揭示多智能体系统中“不经意”歧视对合作的潜在影响，为多代理系统设计提供参考。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [56] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出一种利用语言模型生成Python程序作为规划策略的方法，能在有限时间和内存内优于传统规划器和其他LM方法，甚至在无意义符号上表现出良好推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在规划领域的应用，特别是利用其生成策略来解决复杂PDDL问题，超越传统规划器的性能。

Method: 通过提示LM生成Python程序，作为PDDL问题的通用策略，同时确保其相对于PDDL域的正确性，无需外部验证器。

Result: 在基准测试中，所提策略超越了传统PDDL规划器和其他基于LM的方法，且能够处理数百对象的复杂问题。意外地，LM在处理无意义符号的PDDL问题时表现优异，挑战了语义推理假设。

Conclusion: 该研究展示了LM在规划中的潜力，强调其在符号理解和泛化能力方面的独特表现，为未来结合LM与规划技术提供新方向。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [57] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 引入WLF超参数优化，以提高符号规划中搜索的效率，找到最优超参数组合。


<details>
  <summary>Details</summary>
Motivation: 提升符号规划中的价值函数学习效果，优化搜索性能。

Method: 通过调整WLF超参数，进行大规模CPU实验并统计分析其对训练和规划的影响。

Result: 发现存在一组最优超参数，能显著减少执行时间，且训练指标与规划指标无显著相关。

Conclusion: WLF的超参数优化能有效改善实践中的规划效率，但对训练和规划性能的影响相互独立。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [58] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本文提出使用Weisfeiler-Leman特征（WLFs）替代传统的原子，用于检测新颖性，从而增强启发式搜索的有效性，特别是在存在状态对称的情况下。


<details>
  <summary>Details</summary>
Motivation: 传统的新颖性启发式不具备对称不变性，可能导致重复探索。

Method: 采用无监督学习方法，将WLFs用于合成域不变的新颖性启发式。

Result: 在国际规划竞赛和复杂标准测试集上，WLFs基础的新颖性启发式表现出良好的效果。

Conclusion: 使用WLFs可以有效地解决新颖性启发式中的对称性敏感问题，提升启发式搜索的效率和效果。

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [59] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出了一种基于复合势场的守卫巡逻行为模型，结合全局知识与局部信息，无需训练，能有效提升追捕效率和巡逻自然性。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以在覆盖效率、响应性与自然性之间取得平衡，我们需要更灵活且可解释的守卫行为模型。

Method: 设计了基于三个可解释地图（信息、信心、连通性）的复合势场模型，通过参数调节实现不同地图结合，支持多种地图抽象及行为模式。

Result: 在多种地图和行为模式下，该方法优于传统基线，提升追捕效率和巡逻的自然性，还能快速集成干扰和环境元素实现丰富行为。

Conclusion: 提出的框架为守卫行为的生成提供了一种简单、有效、可解释且易扩展的工具，有助于增强潜行游戏的沉浸感和策略深度。

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [60] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出了一种基于数据库和约束优化的三维游戏关卡生成框架，能自动创建多层、多玩法的可导航环境。


<details>
  <summary>Details</summary>
Motivation: 解决3D关卡生成中空间连贯性、导航功能和玩法进度的平衡问题。

Method: 采用多阶段流水线结合数据库结构和约束优化，并通过修复系统确保导航性。

Result: 验证了框架在生成多样化、可导航的3D环境及模拟不同节奏策略上的有效性。

Conclusion: 提出一种可扩展的数据库驱动设计，为复杂3D关卡的自动化生成提供基础。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [61] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: 提出了一种完全自动化的日志模式提取框架SchemaCoder，利用Q-Tree机制和LLMs实现无需人工定制，显著提升提取效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手工规则限制效率，依赖领域知识，迫切需要自动化解决方案。

Method: 引入Q-Tree提升机制，通过语义chunk分割、 embedding采样和层级化LLM查询，结合残差学习优化。

Result: 在LogHub-2.0基准上，平均提升21.3%的性能优于现有方法，验证了方案的有效性。

Conclusion: SchemaCoder实现了广泛日志格式的完全自动化泛化，显著提升日志模式提取的自动化和效果。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [62] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: 提出一份专门针对非洲人群的皮肤病数据集eSkinHealth，包含丰富的图像和多模态标注，用于推动全球皮肤病AI诊断平等发展。


<details>
  <summary>Details</summary>
Motivation: 弥补现有皮肤疾病数据不足，特别是偏远贫困地区和少见病的代表性数据缺失，促进精准诊断AI的普及。

Method: 采集现场图片，结合AI专家合作，生成多模态注释，包括语义病变掩码、实例化描述和临床概念，并建立标注框架。

Result: 建立了包含5623张图片、47种皮肤疾病的eSkinHealth数据集，含丰富的元数据和标注，支持AI模型训练。

Conclusion: 提供了宝贵的数据资源与标注框架，推动全球皮肤病AI工具的公平性、准确性和可解释性。

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [63] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 提出了一种动态加权多奖励的强化学习方法RLMR，用于在创意写作中同时优化主观写作质量和客观限制遵循，效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决现有强化学习方法在创意写作中难以同时提升主观质量与客观限制遵循的问题。

Method: 引入动态混合奖励系统，通过写作质量和约束验证模型动态调整奖励权重，并在训练中对违反约束的样本进行惩罚。

Result: 在多种模型和真实写作任务中，显著提升了指令遵循性和写作质量，并构建了WriteEval评估基准。

Conclusion: RLMR首次实现了将主观偏好与客观验证相结合的在线强化学习，有效推动多维度创意写作优化。

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [64] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 提出了一种新的人类智力视角的多维评估框架，旨在提升大型语言模型的实用性和社会责任感，涵盖智商、情商、专业能力以及价值导向的经济、社会、伦理和环境指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs评估偏重技术指标，忽视实际应用中的整体表现和社会价值。

Method: 构建包括智力、情感、专业能力三维分类体系及价值导向的多指标评估框架，分析200多个基准测试，识别主要挑战。

Result: 提出了模块化的评估架构和实施路线，提供了丰富的开源资源，帮助开发更全面、更负责任的LLMs。

Conclusion: 多维、以人为本的评估体系有助于推动LLMs朝更具应用性、道德性和可持续发展的方向发展。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [65] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: 提出了一种结合模拟用户的多轮强化学习框架MUA-RL，用于提升LLMs中的工具调用能力，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理的发展，增强多轮交互中的工具使用能力成为亟待解决的问题，现有方法缺乏动态用户环境的训练整合。

Method: 引入模拟用户融入强化学习流程，构建MUA-RL框架，优化模型的多轮沟通与工具调用。

Result: 在多个多轮工具使用基准测试中，MUA-RL表现优异，超过或匹配更大模型的性能。

Conclusion: 通过引入模拟用户，显著提升LLMs在多轮交互中的工具调用效率与效果，为未来自主学习提供新途径。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [66] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 提出了一种主动式GUI代理系统AppAgent-Pro，能够主动整合多领域信息，超越被动响应，提高信息获取的智能性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理多为被动响应，限制了信息获取的效果和效率。

Method: 设计并实现了AppAgent-Pro系统，结合多域信息主动预测用户需求，进行深度信息挖掘。

Result: 该系统能够主动预判用户需求，进行多领域信息整合，潜在地革新日常信息获取方式，对社会产生深远影响。

Conclusion: AppAgent-Pro展示了主动、多领域信息整合的潜力，为未来信息获取提供了新的解决方案。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [67] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: VistaWise是一个结合跨模态知识图和定制目标检测模型的成本效益高的代理框架，能在虚拟开放环境中高效完成任务，显著降低领域特定数据需求。


<details>
  <summary>Details</summary>
Motivation: 提升虚拟环境中自主代理的表现，同时降低开发成本，特别是在缺乏大量领域知识数据的情况下。

Method: 引入跨模态知识图，定制目标检测模型，采用检索池化策略和技能库，结合视觉与文本信息实现高效理解与操作。

Result: 在多项开放世界任务中表现出色，达到或超越最先进水平，并且显著减少了对大规模训练数据的依赖。

Conclusion: VistaWise验证了结合跨模态知识整合与目标检测的策略在虚拟环境中具有极高的实用性和效率，为未来自主代理研究提供了新路径。

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [68] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 提出一种偏见缓解代理系统，有效减少偏见，提升信息公平性和用户信任。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的广泛应用，偏见问题影响信息的公平性，亟需有效缓解方法。

Method: 设计多代理系统，通过优化信息源选择，实现偏见缓解。

Result: 偏见减少达81.82%，显著优于传统策略。

Conclusion: 多代理协作机制有效改善偏见问题，有助于公平知识传播。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [69] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: CAC-CoT通过限制连接词，优化推理长度，提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决长链思考（CoT）在快速直觉任务中的性能下降问题。

Method: 引入Connector-Aware限制连接词，利用合成数据训练模型。

Result: 在GSM8K和GPQA任务中表现优异，命中率达85%和40%，同时保持高效率。

Conclusion: 简洁结构提升推理效率，无损准确性，适用于不同任务类型。

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [70] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: 提出REMONE框架，通过引入记忆增强的反思检索模块和自适应优化器，实现提示优化的持续改进和知识积累。


<details>
  <summary>Details</summary>
Motivation: 解决现有提示优化方法缺乏历史经验利用和易过拟合的问题。

Method: 结合记忆增强的RAG模块和LLM驱动的元控制器，系统性地记录和利用优化经验，提升提示策略。

Result: 在数学推理任务GSM8K上优于基线，展现更稳定和稳健的泛化能力，但计算成本较高。

Conclusion: 通过引入反思与记忆机制，提升提示优化的效果和持续改进能力，为大模型的提示智能发展提供新思路。

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [71] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 本研究提出了Primary-Auxiliary Filtering (PAF)和Knowledge-Integrated Prediction (KIP)方法，用于改善开集测试时的模型过滤和预测性能，有效应对领域偏移带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 应对现实世界中测试数据的领域偏移，提升模型在开集环境中的性能。

Method: 结合辅助过滤机制验证主过滤结果，并整合多模型预测以提升分类准确率。

Result: 在多样的封闭集和开放集数据集上验证，显著提升了封闭集准确率和开放集判别能力。

Conclusion: 提出的PAF和KIP方法有效改善开集测试中的模型性能，增强模型的实际应用能力。

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [72] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 提出一种两阶段方法增强LRMs在面对无法回答的问题时的自主拒绝能力，以提升AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决LRMs在遇到无解问题时无法正确拒绝的问题，确保AI系统的可信性。

Method: 结合认知监控与推理干预的轻量级两阶段方法。

Result: 显著提高LRMs在面对无法回答问题时的拒绝率，同时保持推理性能。

Conclusion: 通过认知监控增强LRMs的自主拒绝能力，有助于建立更可信赖的AI系统。

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [73] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: 提出一种利用多模型协作优化推理能力的方法，通过分布距离动态选择策略（DDS）提升模型性能，并引入MCSU解决模型对齐问题。


<details>
  <summary>Details</summary>
Motivation: 提高语言模型的推理能力，并解决多模型协作中的词汇不对齐问题。

Method: 采用多模型输出的分布选择最佳词元，并引入分布距离的动态选择策略和最小完整语义单位（MCSU）以确保模型对齐。

Result: 在多个基准测试中实验结果优越，验证了方法的有效性。

Conclusion: 多模型协作结合分布距离策略和MCSU，有助于改善语言模型的推理表现和模型间的对齐问题。

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [74] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [75] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: 提出了一种基于因果关系的多智能体协作框架CausalMACE，以提升Minecraft中的复杂任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 单一大型语言模型在处理复杂、多步骤任务时效率低下，且抗错能力有限，缺乏多智能体协作研究。

Method: 引入任务图和因果关系依赖管理模块，通过定义规则进行因果干预，增强多智能体的合作能力。

Result: 在Minecraft多智能体合作任务中实现了最优性能。

Conclusion: 基于因果规划的多智能体框架有效提升了任务执行效率，推动了虚拟环境中多智能体系统的发展。

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [76] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: 提出了一种基于慢思考的推荐系统框架STARec，增强系统的理性推理能力，提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有推荐系统在用户建模和决策机制上局限性，提升推荐的深度和鲁棒性。

Method: 引入双路径认知模型结合 anchored reinforcement training，结合知识蒸馏和偏好塑造。

Result: 在MovieLens 1M和Amazon CD数据集上，显著优于现有方法，且训练数据极少。

Conclusion: 通过慢思考机制，增强推荐系统的理性推理能力，实现性能提升。

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [77] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: 本文分析了大规模语言模型在法律领域的核心能力要求及其AI增强机制的潜力与局限，提出AI在法律中的双重角色。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在高风险法律决策中的能力及限制，特别是在关键推理阶段。

Method: 以IRAC模型为框架，分析法律推理的核心要求，映射不同AI增强机制的潜力。

Result: AI能在处理简单案件和辅助专家决策中发挥作用，但在复杂任务中仍存在挑战，特别是需要判断和透明推理的任务。

Conclusion: AI在法律中的最佳应用是作为简单案例的高效助手和复杂案件的人机合作伙伴。

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [78] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: 提出一种动态、交互式的评估框架，用于更细致地衡量大型语言模型在复杂编程任务中的能力，强调多需求、多步骤的任务特性及反馈机制的优势。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分评价LLMs在复杂软件工程任务中的能力，亟需更细致、动态的评估方法。

Method: 设计基于需求依赖图的互动评估流程，通过“面试官”模型提供针对性提示，帮助“被面试者”模型完成任务，并结合专家注释评估提示的相关性和实用性。

Result: 该方法能揭示模型的优缺点，强调动态评估在推动协作代码生成中的重要作用，丰富了DevAI基准的内容。

Conclusion: 动态交互评估框架为理解和提升LLMs在复杂软件任务中的表现提供了新思路，有助于推动协作式代码生成技术发展。

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [79] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 提出了一种结合语法和一致性检查的强化学习框架FormaRL，用于自动公式化，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自动公式化中的数据稀缺和方法效率问题。

Method: 利用语法检查、语料一致性评价和GRPO算法训练自动公式化模型，并创建了包含本科数学材料的证明问题数据集。

Result: 在ProofNet和uproof数据集上显著提升了自动公式化的准确性，提升幅度达4~6倍，并改善了分布外性能。

Conclusion: FormaRL是一种高效、有效的自动公式化方法，且开源，促进相关研究。

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [80] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: 提出CTGraph，一种基于图的自监督表示学习方法，用于全面描述学生行为和表现，支持个性化教学。


<details>
  <summary>Details</summary>
Motivation: 随着智能辅导系统在教育中的普及，学生表现差距可能会被放大，因此需要有效的学生画像工具。

Method: 采用图级表示学习和自监督学习，构建学生学习行为的全局表征。

Result: CTGraph能全面反映学生的学习过程，识别学习困难学生，为个性化干预提供依据。

Conclusion: CTGraph扩展了学生画像的维度，助力教育者精准干预，提高教育公平性。

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [81] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: 提出了一种名为VISION的框架，用于通过对抗性扩充训练数据来增强GNN对软件漏洞检测的鲁棒性和可解释性，有效改善模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在漏洞检测中易受训练数据偏差和标签噪声影响的问题，提升检测的鲁棒性和泛化能力。

Method: 利用大规模语言模型生成对抗性样本，针对性训练GNN，并借助图结构分析进行模型可解释性。

Result: 显著提升检测准确率，从51.8%提高到97.8%，并改善模型的各类泛化指标。

Conclusion: 该框架有效缓解了虚假相关性，提高了漏洞检测的可靠性和透明度，有助推动可信人工智能在网络安全中的应用。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [82] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: 本文提出结合层次聚类的最近邻方法，有效减少幻觉、简化模型扩展，提升透明度，适用于高可靠性需求场景。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型存在的幻觉、高计算复杂度、昂贵微调和灾难性遗忘问题，提升模型的可解释性和实用性。

Method: 采用k近邻算法结合Kohonen自组织映射建立分层聚类结构，加速最近邻搜索，减少幻觉并简化模型微调。

Result: 在手写数字识别和字幕翻译任务中，实现了搜索时间极大缩短（数百倍），准确率仅略有下降，验证了方法的有效性。

Conclusion: 该方法具有高度透明性、可解释性，结合人类认知机制，适用于需要高可靠性和解释性的应用场景。

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [83] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: 提出一种基于专家重要性管理的MoE模型动态卸载方法，有效减少内存和传输开销，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，如何在边缘硬件上高效部署具有挑战性，尤其是有限的设备内存限制了MoE的应用。

Method: 利用专家的重要性判断，将低重要性的激活专家用已缓存的相似专家替换，结合优化调度策略以最大化GPU缓存复用。

Result: 实现了48%的解码延迟降低和超过60%的专家缓存命中率，基本保持模型准确性。

Conclusion: 通过专家重要性导向的卸载和高效调度，有效提升了边缘硬件上的MoE模型性能与效率，为实际部署提供了新思路。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [84] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: 大型语言模型能通过统计学习，超越大部分人类预测日常场景中的社会规范判断，但也存在系统性错误，显示出其在社交认知方面的潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否能通过纯统计学习实现对社会规范的理解，挑战传统以身体经验为基础的文化学习理论。

Method: 通过两项研究，评估多种AI系统在预测人类对555个日常场景的社会恰当性判断中的表现和误差。

Result: GPT-4.5在预测集体判断方面超过所有人类，Gemini 2.5 Pro也超过98%的成人，显示模型具有强大的预测能力，但都存在系统性错误。

Conclusion: 统计学习使模型具备社会认知能力，超越人类，但仍有限制，凸显语言在文化知识传递中的重要作用。

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [85] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 提出一种持续学习的框架ELL，强调经验探索、长时期记忆、技能学习和知识内化，配合StuLife数据集评估多方面能力，助力通用人工智能的发展。


<details>
  <summary>Details</summary>
Motivation: 推动AI从静态任务优化向持续自主学习的方向演进。

Method: 构建四原则的学习框架及设计模拟学生生活的StuLife数据集，评估持续学习能力。

Result: 提出了全面的持续学习框架与评测平台，促进AI在动态环境中的适应和成长。

Conclusion: 该框架及数据集为发展自我演化、终身学习的智能系统提供基础，推动AI达到更高的通用性水平。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [86] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [87] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: 提出一种基于强化学习的多臂老虎机模型（APME），通过solver性能数据估算难度，避免语言特征和专家标签，用于智能辅导系统。


<details>
  <summary>Details</summary>
Motivation: 解决传统难度标注的主观性和符号领域难以应用的问题，提升评估的客观性和适应性。

Method: 采用被动指标（得分和耗时）结合逆变异系数作为风险调整指标，利用强化学习的多臂老虎机模型进行难度估算。

Result: 在三个异构数据集上表现优异，R2平均0.9213，RMSE为0.0584，优于多种基线模型，验证了方法的稳健性和普适性。

Conclusion: 该方法实现了领域无关的难度估算，并符合教学中的近端发展区思想，可广泛应用于符号领域。

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [88] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 提出一种"黑盒交互"的新评估范式，评估大型语言模型在未知环境中的推理能力，并建立了对应的基准测试，发现模型在复杂任务中仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前任务评估不足以全面衡量大模型在复杂环境中的推理能力，特别是在交互和未知环境中。

Method: 设计"黑盒交互"评估范式，通过模型与隐藏函数互动，推测黑盒功能，建立"Oracle"基准，包括多类型任务和模型测试。

Result: 19个现代大模型在基准测试中表现差异显著，整体在简单任务表现良好，但在复杂任务中仍显不足，尤其缺乏高层次的策略制定能力。

Conclusion: 现有模型仍需增强高层次规划和探索策略，未来应在推理能力和交互策略方面进一步优化。

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [89] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: 引入一个支持异步、多模块协作的智能代理框架CMA，模仿Minsky的心灵社会理论，展示了复杂认知行为的可能性。


<details>
  <summary>Details</summary>
Motivation: 解决传统智能代理架构中的协调与故障容错难题，提升系统的灵活性与复杂行为的表现能力。

Method: 构建支持异步调用、多模块协作的CMA框架，结合大型语言模型实现各模块互动，同时维护全局状态。

Result: 展示了CMA在实际案例中的应用效果，证明了复杂认知现象如自我意识可以通过简单过程的组织交互产生。

Conclusion: CMA提供了一种具有现实意义的多模态、多过程智能系统设计思路，验证了心灵社会理论的实践可能性，并为未来AI研究提供新方向。

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [90] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 通过提出结构化解决方案模板框架（SST），结合难度阶梯式课程，有效提升LLMs在复杂程序推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在复杂程序推理中的深层逻辑捕捉不足的问题。

Method: 引入结构化解决方案模板，设计课程式难度训练，以及在推理时注入模板作为引导。

Result: 在多个数学推理基准测试中，显著提升了模型的准确率和效率。

Conclusion: 结构化解决方案模板结合难度阶梯训练，有效增强了LLMs的推理能力，尤其在复杂任务中表现出色。

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [91] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 提出通过HCAcc@k%衡量LLMs在医疗应用中的准确性与可靠性折中，新方法TrustEHRAgent增强置信度估计，提升临床问答的信任性和精准度。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在医疗场景中可能出现的幻觉问题，确保其输出的可靠性。

Method: 引入HCAcc@k%的新指标，并设计TrustEHRAgent，使其具有步进置信估计能力，用于临床问答。

Result: 在MIMIC-III和eICU数据集上，TrustEHRAgent在HCAcc@70%的严格可靠性条件下，分别超越基线44.23%和25.34个百分点。

Conclusion: 传统准确率指标不足以评估医疗AI的可靠性，提出的TrustEHRAgent有助于实现可信的临床智能助手。

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [92] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型在医疗中的转变，从基础信息检索到复杂临床推理，强调技术基础、 prompting 技巧、验证方法及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 推动大模型在医疗中的应用，提升决策透明度和可解释性。

Method: 系统分析技术基础、 prompting 技巧（如Chain-of-Thought）、强化学习突破、评估方法和面临的挑战。

Result: 提出了医疗大模型发展路线图，强调可靠性、安全性和多模态数据整合，促进临床实践应用。

Conclusion: 未来应加强技术创新、验证体系及伦理规范，推动医疗大模型的应用落地。

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


### [93] [Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning](https://arxiv.org/abs/2508.19113)
*Dayoon Ko,Jihyuk Kim,Haeju Park,Sohyeon Kim,Dahyun Lee,Yongrae Jo,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 引入HDS-QA数据集训练大型推理模型，提升查询效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在多步骤推理中因纯序列查询带来的延迟和连贯性问题。

Method: 设计并利用HDS-QA数据集，结合平行与顺序查询训练模型HybridDeepSearcher。

Result: 模型在多个基准测试中优于现有方法，显著减少推理时间并提升准确性。

Conclusion: 显式训练LRMs利用混合平行与顺序查询方法具有高效、可扩展和有效的优势。

Abstract: Large reasoning models (LRMs) have demonstrated strong performance in
complex, multi-step reasoning tasks. Existing methods enhance LRMs by
sequentially integrating external knowledge retrieval; models iteratively
generate queries, retrieve external information, and progressively reason over
this information. However, purely sequential querying increases inference
latency and context length, diminishing coherence and potentially reducing
accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search
QA), a synthetic dataset automatically generated from Natural Questions,
explicitly designed to train LRMs to distinguish parallelizable from sequential
queries. HDS-QA comprises hybrid-hop questions that combine parallelizable
independent subqueries (executable simultaneously) and sequentially dependent
subqueries (requiring step-by-step resolution), along with synthetic
reasoning-querying-retrieval paths involving parallel queries. We fine-tune an
LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms
state-of-the-art baselines across multiple benchmarks, notably achieving +15.9
and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both
requiring comprehensive and exhaustive search. Experimental results highlight
two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer
search turns, significantly reducing inference latency, and it effectively
scales as more turns are permitted. These results demonstrate the efficiency,
scalability, and effectiveness of explicitly training LRMs to leverage hybrid
parallel and sequential querying.

</details>


### [94] [Algorithmic Collective Action with Multiple Collectives](https://arxiv.org/abs/2508.19149)
*Claudio Battiloro,Pietro Greiner,Bret Nestor,Oumaima Amezgar,Francesca Dominici*

Main category: cs.AI

TL;DR: 提出了一种多集体合作影响学习系统的理论框架，研究集体规模和目标一致性对分类器偏差的影响。


<details>
  <summary>Details</summary>
Motivation: 在实际中，多个集体通过算法集体行动影响学习系统，但相关研究多集中在单一集体，缺乏对多集体交互的理论理解。

Method: 建立理论模型，分析不同规模和目标一致性对分类器信号植入的影响，结合量化分析和先前实证研究。

Result: 揭示了集体规模与目标对齐程度在多集体合作中影响模型偏差的机制，丰富了ACA理论体系。

Conclusion: 该框架促进对多集体协作影响学习系统的整体理解，为未来相关研究提供基础。

Abstract: As learning systems increasingly influence everyday decisions, user-side
steering via Algorithmic Collective Action (ACA)-coordinated changes to shared
data-offers a complement to regulator-side policy and firm-side model design.
Although real-world actions have been traditionally decentralized and
fragmented into multiple collectives despite sharing overarching
objectives-with each collective differing in size, strategy, and actionable
goals, most of the ACA literature focused on single collective settings. In
this work, we present the first theoretical framework for ACA with multiple
collectives acting on the same system. In particular, we focus on collective
action in classification, studying how multiple collectives can plant signals,
i.e., bias a classifier to learn an association between an altered version of
the features and a chosen, possibly overlapping, set of target classes. We
provide quantitative results about the role and the interplay of collectives'
sizes and their alignment of goals. Our framework, by also complementing
previous empirical results, opens a path for a holistic treatment of ACA with
multiple collectives.

</details>


### [95] [Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games](https://arxiv.org/abs/2508.19152)
*Chiu-Chou Lin*

Main category: cs.AI

TL;DR: 本文提出将“玩法风格”作为观察和分析智能体决策行为的新维度，强调其在理解人类多样化决策和未来构建通用人工智能中的潜力。


<details>
  <summary>Details</summary>
Motivation: 弥补现有人工智能主要关注理性决策而忽视风格多样性的问题，探索决策“风格”在智能中的作用。

Method: 从哲学角度分析风格的基础意义，建立风格形成的双层框架，设计衡量指标，利用强化学习与模仿学习表达和生成风格，分析其在游戏与娱乐中的应用潜力。

Result: 提出了风格的量化指标及生成方法，验证了风格多样性对策略的影响，探索了其在实际应用中的价值。

Conclusion: 玩法风格是理解和模仿人类多样化决策的关键，有助于未来发展更具个性化和泛化能力的人工智能系统，甚至成为构建通用人工智能的重要组成部分。

Abstract: Contemporary artificial intelligence (AI) development largely centers on
rational decision-making, valued for its measurability and suitability for
objective evaluation. Yet in real-world contexts, an intelligent agent's
decisions are shaped not only by logic but also by deeper influences such as
beliefs, values, and preferences. The diversity of human decision-making styles
emerges from these differences, highlighting that "style" is an essential but
often overlooked dimension of intelligence.
  This dissertation introduces playstyle as an alternative lens for observing
and analyzing the decision-making behavior of intelligent agents, and examines
its foundational meaning and historical context from a philosophical
perspective. By analyzing how beliefs and values drive intentions and actions,
we construct a two-tier framework for style formation: the external interaction
loop with the environment and the internal cognitive loop of deliberation. On
this basis, we formalize style-related characteristics and propose measurable
indicators such as style capacity, style popularity, and evolutionary dynamics.
  The study focuses on three core research directions: (1) Defining and
measuring playstyle, proposing a general playstyle metric based on discretized
state spaces, and extending it to quantify strategic diversity and competitive
balance; (2) Expressing and generating playstyle, exploring how reinforcement
learning and imitation learning can be used to train agents exhibiting specific
stylistic tendencies, and introducing a novel approach for human-like style
learning and modeling; and (3) Practical applications, analyzing the potential
of these techniques in domains such as game design and interactive
entertainment.
  Finally, the dissertation outlines future extensions, including the role of
style as a core element in building artificial general intelligence (AGI).

</details>


### [96] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: 提出MATRIX框架，用于安全评估临床对话系统，结合安全场景分类、LLM评估器和模拟患者，验证其在风险检测与真实性方面的有效性，推动临床对话AI的安全性发展。


<details>
  <summary>Details</summary>
Motivation: 目前临床对话系统的评估主要关注任务完成和流畅性，缺乏对安全性和风险管理的深入分析，亟需建立结构化、安全导向的评估框架。

Method: 构建融合安全场景分类、LLM评估器（BehvJudge）和模拟患者（PatBot）的多组件框架，验证其在风险检测、真实性模拟方面的效果，进行多场景、多指标的实验验证。

Result: BehvJudge达到了专家水平的危险检测（F1 0.96），优于临床医生，PatBot模拟患者的真实性得到验证，MATRIX在多场景多指标测试中表现优异，有效支持安全评估和对比。

Conclusion: MATRIX首次实现了安全工程与对话AI评估的融合，提供了一个可扩展、验证的安全评估工具，为临床对话系统的安全性审查和监管提供技术支撑。

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


### [97] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 通过重访拉鲁尔的组合学，构建一个基于主题、领域和方法的思维机器，用于研究构思的辅助，促进人机合作的创新。


<details>
  <summary>Details</summary>
Motivation: 旨在利用符号重组方法，创新科学研究的思维工具，增强科研创造力。

Method: 定义三个抽象轴（主题、领域、方法），从专家和论文中提取元素，利用大模型生成多样化科研构想。

Result: 生成的研究想法丰富、多样、相关且 grounded，在科学创新中具有潜在应用价值。

Conclusion: 提出一种轻量级、可解释的研究辅助工具，促进人-AI合作的科研思维拓展。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [98] [The Subset Sum Matching Problem](https://arxiv.org/abs/2508.19218)
*Yufei Wu,Manuel R. Torres,Parisa Zehtabi,Alberto Pozanco Lancho,Michael Cashmore,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出了一种新的组合优化任务SSMP，并提供了三种算法及其性能评估。


<details>
  <summary>Details</summary>
Motivation: 解决金融中的交易对账等应用中的子集和匹配问题。

Method: 设计三种算法（两种次优、一种最优）并构建测试基准进行实验评估。

Result: 验证了算法的性能表现和适用性，展示了不同复杂度实例中的效果。

Conclusion: 该研究丰富了组合优化问题的解决方案，拓展了在金融应用中的实际应用潜力。

Abstract: This paper presents a new combinatorial optimisation task, the Subset Sum
Matching Problem (SSMP), which is an abstraction of common financial
applications such as trades reconciliation. We present three algorithms, two
suboptimal and one optimal, to solve this problem. We also generate a benchmark
to cover different instances of SSMP varying in complexity, and carry out an
experimental evaluation to assess the performance of the approaches.

</details>


### [99] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 提出StepWiser，通过生成式判官以增强多步推理模型的步骤验证，改善准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着模型在复杂问题中采用多步推理，需有效监督中间步骤的逻辑有效性。

Method: 将步骤奖励建模从分类转为推理，设计生成式判官进行元推理，并通过强化学习训练。

Result: 显著提升中间步骤判断准确性，增强模型训练和推理搜索能力。

Conclusion: StepWiser通过生成式判官改善多步推理的步骤监督，具有潜在的应用价值。

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


### [100] [Model Context Protocols in Adaptive Transport Systems: A Survey](https://arxiv.org/abs/2508.19239)
*Gaurab Chhetri,Shriyank Somvanshi,Md Monzurul Islam,Shamyo Brotee,Mahmuda Sultana Mimi,Dipti Koirala,Biplov Pandey,Subasish Das*

Main category: cs.AI

TL;DR: 本文系统研究了模型上下文协议（MCP）在解决分布式通信系统中协议孤岛问题的潜力，强调其在实现语义互操作性和AI适应性中的关键作用，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着智能交通系统的发展，现有协议高度碎片化，亟需统一框架实现协议兼容与上下文感知。

Method: 通过分析现有文献，归纳出五类架构分类，探讨MCP在协议整合中的应用与优势。

Result: 发现MCP结构支持语义互操作，传统协议受到孤立适应限制，AI应用对集成方案提出新需求。

Conclusion: MCP有望成为未来智能、适应性交通系统的基础，为实现端到端的连接与智能决策提供支持。

Abstract: The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [101] [REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking](https://arxiv.org/abs/2508.18379)
*Pinhuan Wang,Zhiqiu Xia,Chunhua Liao,Feiyi Wang,Hang Liu*

Main category: cs.IR

TL;DR: REALM通过贝叶斯更新模型，提高LLM文档重排序的效率和准确性，显著减少token消耗和延时。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM重排序方法中的排名不确定性、Top-k恢复不稳定和高token成本问题。

Method: 将LLM相关性建模为高斯分布，并通过递归贝叶斯更新进行优化，显式捕捉不确定性。

Result: 在排名效果、Token使用和延时方面优于现有方法，表现出色。

Conclusion: REALM是一种高效、稳健的下一代信息检索重排序技术，具有广泛应用前景。

Abstract: Large Language Models (LLMs) have shown strong capabilities in document
re-ranking, a key component in modern Information Retrieval (IR) systems.
However, existing LLM-based approaches face notable limitations, including
ranking uncertainty, unstable top-k recovery, and high token cost due to
token-intensive prompting. To effectively address these limitations, we propose
REALM, an uncertainty-aware re-ranking framework that models LLM-derived
relevance as Gaussian distributions and refines them through recursive Bayesian
updates. By explicitly capturing uncertainty and minimizing redundant queries,
REALM achieves better rankings more efficiently. Experimental results
demonstrate that our REALM surpasses state-of-the-art re-rankers while
significantly reducing token usage and latency, promoting it as the
next-generation re-ranker for modern IR systems.

</details>


### [102] [DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation](https://arxiv.org/abs/2508.18442)
*Jan Malte Lichtenberg,Antonio De Candia,Matteo Ruffini*

Main category: cs.IR

TL;DR: DenseRec通过引入双路径嵌入，提高了Transformer推荐系统在冷启动场景下的表现，比传统仅依赖ID嵌入的方法更为有效。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer推荐模型在动态商品目录中遇到的冷启动问题，尤其是在引入预训练内容嵌入后效果不佳的挑战。

Method: 提出双路径嵌入方案，训练中学习从内容嵌入到ID嵌入的线性投影，实现未见物品的无缝推广。

Result: 在三个真实数据集上，DenseRec优于ID-only的基线模型，且无需复杂调优或大型嵌入模型，表现稳定且效果显著。

Conclusion: DenseRec是一个简单有效的冷启动推荐方案，通过改进序列表示，在实际应用中具有良好的鲁棒性和实用价值。

Abstract: Transformer-based sequential recommenders, such as SASRec or BERT4Rec,
typically rely solely on learned item ID embeddings, making them vulnerable to
the item cold-start problem, particularly in environments with dynamic item
catalogs. While dense content embeddings from pre-trained models offer
potential solutions, direct integration into transformer-based recommenders has
consistently underperformed compared to ID-only approaches. We revisit this
integration challenge and propose DenseRec, a simple yet effective method that
introduces a dual-path embedding approach. DenseRec learns a linear projection
from the dense embedding space into the ID embedding space during training,
enabling seamless generalization to previously unseen items without requiring
specialized embedding models or complex infrastructure. In experiments on three
real-world datasets, we find DenseRec to consistently outperform an ID-only
SASRec baseline, even without additional hyperparameter tuning and while using
compact embedding models. Our analysis suggests improvements primarily arise
from better sequence representations in the presence of unseen items,
positioning DenseRec as a practical and robust solution for cold-start
sequential recommendation.

</details>


### [103] [Extracting Information from Scientific Literature via Visual Table Question Answering Models](https://arxiv.org/abs/2508.18661)
*Dongyoun Kim,Hyung-do Choi,Youngsun Jang,John Kim*

Main category: cs.IR

TL;DR: 本文探讨三种处理科学论文中表格数据的方法，验证了保持表格结构完整的重要性，有助于提高问答系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提升科学文献中的信息提取效率及问答准确性，研究需要有效处理论文中的表格数据。

Method: 评估OCR、预训练模型及表格检测与结构识别方法，通过实验比较效果。

Result: 保留表格结构的方法表现优于其他，特别是在内容组织和符号识别方面效果显著。

Conclusion: 保持表格的结构完整性是提升科学文献中问答系统性能的关键。

Abstract: This study explores three approaches to processing table data in scientific
papers to enhance extractive question answering and develop a software tool for
the systematic review process. The methods evaluated include: (1) Optical
Character Recognition (OCR) for extracting information from documents, (2)
Pre-trained models for document visual question answering, and (3) Table
detection and structure recognition to extract and merge key information from
tables with textual content to answer extractive questions. In exploratory
experiments, we augmented ten sample test documents containing tables and
relevant content against RF- EMF-related scientific papers with seven
predefined extractive question-answer pairs. The results indicate that
approaches preserving table structure outperform the others, particularly in
representing and organizing table content. Accurately recognizing specific
notations and symbols within the documents emerged as a critical factor for
improved results. Our study concludes that preserving the structural integrity
of tables is essential for enhancing the accuracy and reliability of extractive
question answering in scientific documents.

</details>


### [104] [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
*Jiajie He,Yuechun Gu,Min-Chun Chen,Keke Chen*

Main category: cs.IR

TL;DR: 本文探讨了基于大语言模型的推荐系统中的隐私攻击，提出四种会员推断攻击方法，并验证其在实际中的有效性，指出了潜在的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被应用于推荐系统，私人用户信息面临潜在的隐私泄露风险，亟需研究防护措施。

Method: 设计了直接询问、幻想、相似性和投毒四种会员推断攻击，基于三种LLMs和两个推荐系统数据集进行评估。

Result: 攻击方法在实际测试中表现出高攻击优势，验证了LLM推荐系统存在明显的隐私风险，且攻击效果受提示中的信息数量和目标位置影响。

Conclusion: LLM推荐系统中的隐私保护亟待重视，需发展更有效的防御机制以保护用户的敏感信息。

Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly
adapt recommendation systems to different domains. It utilizes in-context
learning (ICL), i.e., the prompts, to customize the recommendation functions,
which include sensitive historical user-specific item interactions, e.g.,
implicit feedback like clicked items or explicit product reviews. Such private
information may be exposed to novel privacy attack. However, no study has been
done on this important issue. We design four membership inference attacks
(MIAs), aiming to reveal whether victims' historical interactions have been
used by system prompts. They are \emph{direct inquiry, hallucination,
similarity, and poisoning attacks}, each of which utilizes the unique features
of LLMs or RecSys. We have carefully evaluated them on three LLMs that have
been used to develop ICL-LLM RecSys and two well-known RecSys benchmark
datasets. The results confirm that the MIA threat on LLM RecSys is realistic:
direct inquiry and poisoning attacks showing significantly high attack
advantages. We have also analyzed the factors affecting these attacks, such as
the number of shots in system prompts and the position of the victim in the
shots.

</details>


### [105] [Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training](https://arxiv.org/abs/2508.18700)
*Yi-Ping Hsu,Po-Wei Wang,Chantat Eksombatchai,Jiajing Xu*

Main category: cs.IR

TL;DR: 提出了一种两阶段训练策略，通过预训练增强嵌入系统的泛化能力，在Pinterest上线后实现了显著的用户参与提升。


<details>
  <summary>Details</summary>
Motivation: 解决ID嵌入在长尾数据分布下易过拟合的问题，提升推荐系统的训练效率和泛化能力。

Method: 采用包含对比损失的预训练阶段，结合多轮训练以扩大数据覆盖面，然后进行微调。

Result: 预训练期间多轮训练未引发过拟合，嵌入在实际推荐中表现更优，在Pinterest实现了显著的用户参与增长。

Conclusion: 该两阶段策略有效改善ID嵌入的泛化性和推荐性能，具有实际应用效果。

Abstract: ID-based embeddings are widely used in web-scale online recommendation
systems. However, their susceptibility to overfitting, particularly due to the
long-tail nature of data distributions, often limits training to a single
epoch, a phenomenon known as the "one-epoch problem." This challenge has driven
research efforts to optimize performance within the first epoch by enhancing
convergence speed or feature sparsity. In this study, we introduce a novel
two-stage training strategy that incorporates a pre-training phase using a
minimal model with contrastive loss, enabling broader data coverage for the
embedding system. Our offline experiments demonstrate that multi-epoch training
during the pre-training phase does not lead to overfitting, and the resulting
embeddings improve online generalization when fine-tuned for more complex
downstream recommendation tasks. We deployed the proposed system in live
traffic at Pinterest, achieving significant site-wide engagement gains.

</details>


### [106] [Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search](https://arxiv.org/abs/2508.18877)
*Kushagra Agrawal,Nisharg Nargund,Oishani Banerjee*

Main category: cs.IR

TL;DR: 提出一种基于博弈论的潜在空间压缩方法，提高向量搜索的效率和语义表现。


<details>
  <summary>Details</summary>
Motivation: 解决高维潜在表示带来的扩展性和效率问题，提升向量搜索的性能。

Method: 将压缩策略建模为零和博弈，利用潜在变换保持语义相似性同时减少冗余。

Result: 在与FAISS的比较中，显著提高平均相似度和实用性指标，略微增加查询时间。

Conclusion: 博弈论驱动的潜在空间压缩能在保持搜索质量的同时提升效率，适用于大规模信息检索系统。

Abstract: Vector similarity search plays a pivotal role in modern information retrieval
systems, especially when powered by transformer-based embeddings. However, the
scalability and efficiency of such systems are often hindered by the high
dimensionality of latent representations. In this paper, we propose a novel
game-theoretic framework for optimizing latent-space compression to enhance
both the efficiency and semantic utility of vector search. By modeling the
compression strategy as a zero-sum game between retrieval accuracy and storage
efficiency, we derive a latent transformation that preserves semantic
similarity while reducing redundancy. We benchmark our method against FAISS, a
widely-used vector search library, and demonstrate that our approach achieves a
significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873
vs. 0.5194), albeit with a modest increase in query time. This trade-off
highlights the practical value of game-theoretic latent compression in
high-utility, transformer-based search applications. The proposed system can be
seamlessly integrated into existing LLM pipelines to yield more semantically
accurate and computationally efficient retrieval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [107] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 提出一种基于思考深度(DoT)的课程学习方法，用于训练大型语言模型（LLMs），该方法依赖模型推理路径中的步骤数作为难度指标。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLMs的困难信号缺乏与推理能力直接相关的可解释指标。

Method: 定义推理深度（DoT）为模型推理路径中的步骤数，基于此设计由浅入深的课程训练方式，并提出评估与验证框架。

Result: 假设得到验证：DoT与推理难度正相关，基于DoT的课程优于其他方法，并且具备跨模型鲁棒性。

Conclusion: 通过引入基于思考深度的难度指标，有助于实现更具可解释性与认知基础的推理训练课程。

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [108] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 提出一种多模态机器学习框架，用于预测海上漂浮物的漂移，结合文本嵌入和序列模型，提高长时预测能力。


<details>
  <summary>Details</summary>
Motivation: 在海上搜救等紧急场景中，准确预测漂浮物的漂移具有重要意义，但现有方法在长时预测和多模态信息融合方面存在挑战。

Method: 结合水动力模拟、图像卷积网络、文本编码和注意力机制的序列模型，综合环境物理数据和文本描述进行漂移预测。

Result: 多模态模型在不同时间范围内表现优异，能进行长时间预测，其效果与传统物理模型和神经网络相当。

Conclusion: 多模态策略有效提升了海上漂浮物漂移的预测能力，适应动态海洋环境。

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [109] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的方法，用于油藏生产参数的预测，无需依赖地质模型，适应实际操作的变化。


<details>
  <summary>Details</summary>
Motivation: 解决油藏工程中生产预测的可靠性和变化预警问题，减少对复杂地质信息的依赖。

Method: 采用监督学习包括回归和神经网络，从合成数据和实际油藏中进行验证，考虑概念漂移和定期重训练。

Result: 开发出一种快速、可靠、实用的生产动态预测工具，有助于油藏管理和开采优化，表现出良好的适应性和实用性。

Conclusion: 机器学习技术可有效提升油藏生产预测的准确性和响应速度，为油藏工程提供重要决策支持。

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [110] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 本研究开发了一个快速、资源节约的抑郁症检测工具，利用过去7天的应用使用数据，通过机器学习模型在极短时间内识别抑郁症，有望用于资源有限地区的早期诊断。


<details>
  <summary>Details</summary>
Motivation: 传统检测系统需要长时间数据收集，难以满足早期诊断需求，特别是在资源有限的地区，因此需要开发快速、简洁的检测方法。

Method: 设计了能在1秒内提取过去7天应用数据的工具，并基于这一数据采用多种机器学习模型进行抑郁症识别，利用不同特征选择方法优化模型性能。

Result: 模型在仅用1秒数据情况下，采用稳定特征筛选出的模型达到了82.4%的抑郁学生识别率，最高精确率77.4%，面向发展中国家的实用性强，且通过SHAP分析揭示了与抑郁相关的行为标志。

Conclusion: 该快速、简便的检测系统在资源有限地区具有潜在价值，有助于早期识别抑郁症，并为低资源环境下的精神健康监测提供思路。

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [111] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: 提出了一种可解释的深度学习框架NeuroPathX，用于分析脑结构与遗传变化在神经疾病中的关系。


<details>
  <summary>Details</summary>
Motivation: 弥补传统影像遗传学方法在模型复杂性和可解释性方面的不足。

Method: 采用早期融合策略结合交叉注意机制，并引入两种损失函数以增强模型的解释性和稳健性。

Result: 在自闭症和阿尔茨海默症数据集上优于基线方法，并揭示合理的生物学关联。

Conclusion: NeuroPathX有望促进对复杂脑部疾病的理解，具有良好的应用前景。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [112] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出一种基于距离映射扭曲的局部鲁棒性评估框架SALMAN，无需修改模型参数，提升模型在输入扰动下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模变换器模型规模扩大，其鲁棒性成为亟待解决的问题，现有方法依赖复杂设计，亟需简洁实用的解决方案。

Method: 设计距离映射扭曲（DMD）指标，通过比较输入与输出的距离映射关系，评估模型对输入扰动的敏感度，且复杂度接近线性。

Result: 在攻击效率和鲁棒训练方面显示出显著提升，成为一种现实、模型无关的工具，有助于提升变换器模型的可靠性。

Conclusion: SALMAN提供了一个无需内部参数修改的简便框架，有助于提升NLP模型的鲁棒性和实用性。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [113] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: 提出了一种结合运算子值再生核希尔伯特空间与核方法的统一框架，用于非参数估计时空动态，适用于高维非线性系统的预测和控制。


<details>
  <summary>Details</summary>
Motivation: 解决高维时空动态系统在建模与预测中的复杂性，通过结合核方法与Koopman算子实现非参数、数据驱动的学习。

Method: 将运算子值再生核希尔伯伯空间与核相关的Koopman算子相结合，建立理论基础，包括表示定理、近似界和谱收敛保证。

Result: 提出的框架支持高效的降阶模型和长期预测，为时空机器学习中的预报、控制和不确定性量化提供理论工具。

Conclusion: 该方法为复杂时空系统的学习提供了坚实的理论基础和实用工具，促进其在科学和工程应用中的发展。

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [114] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: 提出了一种基于复数表示的轻量级位置编码方法CoPE，提高Transformer模型性能.


<details>
  <summary>Details</summary>
Motivation: 解决传统位置编码在捕捉长距离依赖时的效果不足和计算复杂性问题。

Method: 引入复数编码，其中实部表示语义内容，虚部编码位置信息，并在Transformer第一层加入相位感知注意机制。

Result: 在GLUE基准测试中表现优越，优于RoPE、正弦和学习位置编码，且具有较低的计算复杂度。

Conclusion: 复数位置编码是一种有效的增强Transformer模型能力的方案，尤其在保持低复杂度的同时提升性能方面具有潜力。

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [115] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: 偏好数据中选择响应的质量对DPO性能影响最大，而拒绝响应的质量影响较小。


<details>
  <summary>Details</summary>
Motivation: 探究偏好数据分布特性对DPO性能的影响，提升大模型偏好对齐效果。

Method: 理论分析与大量实证实验，研究响应质量和对比度对DPO的影响，分析在线DPO的特性。

Result: 选择响应质量对模型性能提升最关键，优化对比性主要改善选择样本，混合策略具有实际效果。

Conclusion: 高质量的选择响应和合理的偏好数据构建策略对提升LLM偏好对齐效果具有重要意义。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [116] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: ProtoEHR是一个层次结构原型学习框架，充分利用EHR数据的多层次结构，提高医疗预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 弥补现有研究只关注EHR部分数据导致预测性能和解释性的局限。

Method: 构建医学知识图谱，设计多层次表示学习框架，结合原型信息，提升模型表现。

Result: 在多个临床任务上优于现有方法，具有良好的准确性和解释性。

Conclusion: ProtoEHR有效利用EHR多层次结构，改善医疗预测，支持临床决策。

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [117] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 使用联邦学习技术结合学生早期表现预测学业风险，保证数据隐私，具备85%准确率。


<details>
  <summary>Details</summary>
Motivation: 高辍学和失败率在远程教育中带来挑战，亟需提前识别高风险学生。

Method: 开发基于早期学业和数字互动数据的机器学习模型，采用联邦学习框架，比较逻辑回归和深度神经网络模型。

Result: 联邦模型达到大约85%的ROC AUC，表现优异，适合实际部署。

Conclusion: 联邦学习为高等教育提供了一个实用、可扩展且保护隐私的学生预警系统解决方案。

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [118] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: 提出一种零信任联邦学习框架ZTFed-MAS2S，结合多头注意力序列到序列模型和多项隐私保护机制，有效提升风电数据缺失值修复的安全性和精度。


<details>
  <summary>Details</summary>
Motivation: 风电数据存在传感器故障和传输不稳定导致的缺失值，联邦学习面临隐私和安全挑战，尤其在开放工业环境中需要零信任机制。

Method: 引入多头注意力序列到序列模型，结合差分隐私、零知识证明、信任传播机制和压缩技术，构建全方位安全且高效的联邦学习框架。

Result: 在真实风电场数据集上进行实验，验证ZTFed-MAS2S在数据修复和模型性能上优于现有方法，表现出良好的安全性和实用性。

Conclusion: 该方法结合多项加密和信任机制，有效保障数据隐私与模型安全，适用于能源行业的实际应用。

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [119] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: 本文提出HCR方法作为HSIC的高效替代，具有更高的依赖敏感性和线性计算复杂度，适用于大规模数据分析。


<details>
  <summary>Details</summary>
Motivation: 解决HSIC在大样本数据中的高计算复杂度问题。

Method: 引入层次相关重建（HCR）技术，通过特征的混合矩描述依赖关系，提供联合分布模型，并允许估算互信息。

Result: HCR在保持高依赖检测能力的同时，实现了线性时间复杂度，适合大规模数据分析。

Conclusion: HCR是一种高效且有潜力的依赖检测试验工具，优于传统的HSIC方法，特别是在处理大数据时。

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [120] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于后训练专家划分的双重稀疏（DualSparse-MoE）方法，有效提升了大规模专家模型的推理效率，同时几乎不影响模型精度。


<details>
  <summary>Details</summary>
Motivation: 护理大规模专家模型在推理时的高计算成本与不稳定的激活模式问题，旨在通过稀疏策略提升效率和准确性。

Method: 引入后训练专家划分结合动态张量级计算丢弃与静态神经元重构，设计双重稀疏策略，优化MoE模型的推理过程。

Result: 在三种主流MoE模型上实验，约25%的计算丢弃率仅造成0.08%-0.28%的准确率下降，同时实现显著的速度提升，尤其在负载不均平衡优化后，速度提升达1.41倍。

Conclusion: 双重稀疏策略有效兼顾效率与模型表现，为大规模专家模型的高效推理提供了可行方案。

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [121] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 低秩张量分解在深度学习理论中具有重要作用，解释了神经网络的表达性、可学性、计算复杂性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络的崛起，建立其数学基础成为研究重点，而低秩张量分解提供了理论支撑。

Method: 回顾不同社区的低秩张量方法，探讨其在深度学习中的应用。

Result: 揭示低秩张量方法在解释深度网络性能方面的核心作用，涵盖表达性、可学性、复杂性、泛化和恒等性。

Conclusion: 低秩张量分解是理解深度神经网络的重要工具，未来具有广阔的研究空间。

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [122] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: 结合变分自编码器和大型语言模型的内在激励策略，显著提升极稀疏奖励环境中RL代理的效率。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在极稀疏奖励环境中学习效率低下的问题。

Method: 将变分状态作为内在奖励（VSIMR）与基于大语言模型生成的奖励结合，应用在MiniGrid DoorKey环境中，采用A2C算法。

Result: 联合策略明显优于单独策略或标准A2C，提升代理性能和采样效率。

Conclusion: 结合多种内在激励方式能有效补充环境信息，推动探索与利用的平衡，改善极稀疏奖励下的学习表现。

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [123] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 提出基于梯度和Hessian的多局部二次模型来提升高维贝叶斯优化的效率，并解决GP在高维中的梯度消失问题。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中贝叶斯优化面临的样本效率低和模型表达有限的问题。

Method: 利用全局高斯过程的梯度和Hessian构建局部二次模型，通过解界约束二次规划选择采样点。

Result: 该方法提升了TuRBO的性能，在高维合成函数和实际应用中优于其他高维贝叶斯优化技术。

Conclusion: 提出的多局部二次模型增强了高维贝叶斯优化的效果，有效应对梯度消失问题，具有较强的实用推广价值。

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [124] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: 本文提出了一种针对Verilog硬件描述语言的强化学习框架，构建了高质量数据集Veribench-53K，并通过引入Trace-back Re-score机制和样本平衡策略，提升了代码生成的性能和鲁棒性，实现了在硬件结构化代码生成中的突破。


<details>
  <summary>Details</summary>
Motivation: 硬件描述语言Verilog在自动生成方面应用有限，主要因其语义复杂、语法刚性和仿真困难，亟需有效的自动化生成方法。

Method: 构建高质量Verilog问题数据集，设计Trace-back Re-score机制和样本平衡策略，结合强化学习优化生成模型。

Result: 在Verilog生成任务中实现了最先进性能，显著提升测试通过率、功能正确性和编译鲁棒性。

Conclusion: RL驱动的结构化代码生成在硬件描述领域具有巨大潜力，本文的方法优于现有的闭源模型蒸馏和稀疏反馈方法。

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [125] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出一种基于强化学习的时序异常检测框架DRTA，通过动态奖励机制结合VAE和主动学习，有效提升低标注情况下的检测性能，优于现有的无监督和半监督方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统时序异常检测方法在有限标注、误检率高和泛化能力弱方面的不足。

Method: 结合动态奖励调节、变分自编码器（VAE）和主动学习，设计强化学习框架DRTA，以提升异常检测的准确性和稳定性。

Result: 在Yahoo A1和A2数据集上实验显示，DRTA明显优于现有最先进的无监督和半监督方法，效果稳健且具有实际应用潜力。

Conclusion: 该方法是一种规模化、高效的时序异常检测方案，特别适合实际低标注环境，具有良好的推广价值。

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [126] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: 系统性数据增强策略显著提升机器遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 探究不同数据增强策略在机器遗忘中的作用，填补其设计影响的研究空白。

Method: 对比分析多种增强方法（如TrivialAug）在不同遗忘率下对多种遗忘方法（如SalUn、随机标签、微调）的影响。

Result: 合理增强策略显著改善遗忘效果，平均gap下降最多达40.12%，显著缩小模型再训练差距。

Conclusion: 数据增强在提升机器遗忘的效率和隐私保护方面具有重要作用，应引起重视并优化设计。

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [127] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: 提出一种基于强化学习的参数初始化策略，有效缓解VQAs中的荒凉平台问题，提升算法效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决VQAs中梯度消失导致的训练难题，特别是荒凉平台问题，提升算法的可扩展性和实用性。

Method: 采用多种RL算法预训练参数，避免陷入梯度消失的区域，增强后续梯度优化效果。

Result: 实验证明该方法显著提升收敛速度和最终解质量，不同RL算法表现相近，具有良好的鲁棒性。

Conclusion: RL驱动的初始化策略为VQAs提供了一个有效途径，推动量子算法与机器学习的结合，促进其实用化与扩展性。

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [128] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 本文提出了一种将各种电路转换为具有ReLU激活函数的前馈神经网络的系统性元算法，实现了神经网络在推理任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络在完美训练条件下的推理能力，特别是量化它们可以执行的推理形式。

Method: 采用电路仿真的方式，将任何电路逐步替换为ReLU多层感知机，以此构建等效的神经网络。

Result: 成功实现了各种类型电路的精确仿真，包括图算法、图灵机模拟和随机布尔电路，验证了神经网络在推理任务中的潜力。

Conclusion: 神经网络不仅可以模拟各种复杂电路，还比传统的逼近定理更具威力，具备更广泛的推理能力。

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [129] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: 提出Beyond Two-modality Weighting (BTW)，一种非参数加权框架，动态调整多模态重要性，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据中噪声过多影响模型效果的问题，现有方法难以扩展或控制不细致。

Method: 结合实例级KL散度和模态级互信息，在训练过程中动态调整模态权重，无需添加参数，可应用于任意模态数。

Result: 在情感回归和临床分类任务中显著提高了性能和准确率。

Conclusion: BTW方法有效增强多模态学习的鲁棒性与表现，具有广泛应用潜力。

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [130] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 提出了一种基于逆因果掩码的分子属性解释方法，通过生成合理的分子片段提供更真实且有意义的模型解释。


<details>
  <summary>Details</summary>
Motivation: 弥补现有掩码方法在解释分子重要性时缺乏分子分布一致性的问题。

Method: 使用训练生成模型完成分子图，生成合理的分子片段作为掩码替换，实现逆因果掩码。

Result: 方法在多个数据集和任务中表现出更真实且富有指导性的解释，推动化学中的可解释机器学习发展。

Conclusion: 逆因果掩码提供了一种结合分子化学合理性与模型解释的创新框架，有助于实现更具实际指导意义的分子设计。

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [131] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: 本文对Levie的《Graphon-Signal Analysis of Graph Neural Networks》进行了扩展和改进，涵盖多维信号、带阅读的MPNN、稳健性边界及非对称图on的分析。


<details>
  <summary>Details</summary>
Motivation: 补充和完善原论文在实际图神经网络应用中的理论基础和适用性。

Method: 引入多维信号、扩展连续性到带阅读的MPNN、利用稳健性理论提升边界、分析非对称图on。

Result: 获得了更广泛适用的定量分析工具和更强的泛化界限。

Conclusion: 优化了图神经网络理论分析框架，增强其实用性与适用范围。

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [132] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: SPF方法通过结合模型预测与真实数据，平衡短期和长期预测效果，减少存储需求，提升复杂系统的长远预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据驱动方法在复杂系统中长期预测精度下降和资源消耗过大的问题。

Method: 引入随机推送策略，结合模型预测与真实数据，预先计算多步预测，优化训练过程。

Result: SPF在 Burgers' 方程和浅水模拟中优于自回归方法，提升长期预测精度，同时降低内存占用。

Conclusion: SPF在资源有限环境下具有良好应用前景，改善复杂系统的长远预测效果。

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [133] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: Not available


<details>
  <summary>Details</summary>
Motivation: Not available

Method: Not available

Result: Not available

Conclusion: Not available

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [134] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: DrugReasoner是一款结合推理的药物批准预测大模型，通过分子描述符和相似结构药物比较实现高效、透明的预测，为药物研发提供可解释的辅助工具。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程中，早期预测药物批准结果对优化研发投入至关重要，现有技术缺乏良好的可解释性。

Method: 基于LLaMA架构，利用GRPO微调的大语言模型DrugReasoner，通过比较相似药物实现推理预测，结合分子描述符进行判断。

Result: 模型在多项指标上优于传统方法，验证集AUC为0.732，测试集AUC为0.725，在外部数据集也表现优异，展现出强鲁棒性和透明性。

Conclusion: DrugReasoner不仅提供有竞争力的预测准确率，还增强了模型的可解释性，展示了推理增强大模型在药物研发中的应用潜力。

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [135] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL系统通过HistoSpec和HistoPipe技术提升大语言模型中强化学习的效率，显著加快训练速度且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 优化大语言模型中的强化学习流程，解决GPU利用率低的问题。

Method: 引入HistoSpec进行猜测解码，并利用HistoPipe实现工作负载平衡，利用历史数据相似性提升效率。

Result: 在实际环境中，RhymeRL实现了比现有方法快2.6倍的性能提升，支持从几十到数千GPU的扩展，且不影响模型性能。

Conclusion: RhymeRL通过创新调度和推理技术，有效提升大规模强化学习的性能，为大语言模型训练提供了可扩展的解决方案。

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [136] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏谱的线性交易头寸优化方法，通过Krasnosel'ski i-Mann算法实现，性能优越且稳健。


<details>
  <summary>Details</summary>
Motivation: 现有的主成分投资组合方法可能缺乏多样性和鲁棒性，无法充分利用预测矩阵的特征。

Method: 提出了具有稀疏谱的线性交易头寸，并采用Krasnosel'ski i-Mann固定点算法进行优化，保证了目标值的线性收敛。

Result: 实验证明该方法在各种情况下都表现出良好且稳健的性能。

Conclusion: 该方法突破了现有算法的局限性，提供了一种有效的交易策略优化工具，具有理论和实践意义。

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [137] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 提出了一种结合多尺度特征提取和不确定性估计的无监督时序数据域适应方法，通过多尺度混合输入和证据学习提升模型的泛化能力和稳健性。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列数据中训练和测试数据分布偏移带来的域适应挑战。

Method: 采用多尺度混合输入架构及基于Dirichlet先验的证据学习实现不确定性意识，通过对跨域特征对齐改善域适应。

Result: 在多个基准数据集上实现了优异性能，且模型校准性较好，验证了方法的有效性。

Conclusion: 多尺度特征融合与不确定性机制有效提升时序数据的无监督域适应能力，适用于复杂分布偏移的场景。

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [138] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: STRATA-TS通过目标感知检索结合大模型推理，有效改善数据稀缺城市的城市预测性能，确保知识迁移的相关性与解释性。


<details>
  <summary>Details</summary>
Motivation: 城市预测模型面临数据不平衡问题，如何有效利用有限的丰富数据提升稀缺数据城市的预测精度成为挑战。

Method: 提出结合领域适应检索与大模型推理的STRATA-TS框架，利用区域匹配的时序编码器选择相关源序列，并引入LLM进行结构化推理，通过微调实现模型压缩。

Result: 在新加坡、诺丁汉、格拉斯哥的停车位数据集上，STRATA-TS优于传统预测和迁移方法，且具备良好的解释性。

Conclusion: STRATA-TS通过结合目标导向检索与推理机制，改善了城市预测中的知识迁移效果，具有广泛应用潜力。

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [139] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: BDVAE是一种融合多组学数据的深度生成模型，用于预测免疫检查点抑制剂的治疗反应，并揭示抗药机制。


<details>
  <summary>Details</summary>
Motivation: 解决免疫疗法中反应预测不精准及机制不清的问题。

Method: 引入多模态变分自编码器，结合生物学结构，学习与免疫、代谢相关的潜在特征。

Result: 在多癌种数据上表现优异，预测准确率高，揭示抗药机制的连续性与复杂性。

Conclusion: 生物学结构导向的机器学习有助于理解免疫治疗的复杂反应机制，指导精准治疗。

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [140] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: 提出了一种结合文本情感分析和语音动态的多模态金融风险评估框架，通过声学模型提取企业财报电话中高管的情感信号，有助于预测市场波动性。


<details>
  <summary>Details</summary>
Motivation: 金融市场信息不对称和企业叙事策略削弱了传统文本分析的效果，需要新的方法来更全面理解企业风险，特别是情感和心理状态。

Method: 研发了物理信息驱动声学模型（PIAM），结合非线性声学理论，从财报电话中的语音信号提取情感特征，并将其与文本情感共同投射到三维的情感状态空间，分析高管在不同话语转变中的情感动态。

Result: 多模态特征在预测30天实际波动率方面具有较高解释力（达43.8%），其中CEO和CFO在话语转变中的情感变化是关键指标，优于仅使用财务文本的方法。

Conclusion: 多模态情感分析，尤其是声学和文本融合，能有效揭示企业隐藏的不确定性，改善市场风险判断工具，为投资者和监管者提供新的监管和投资洞察方法。

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [141] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: 提出FFT MoE，有效解决联邦微调中LoRA的结构不兼容和非IID数据适应性不足问题，通过专家门控网络和正则化优化增强模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大规模基础模型推动AI通用化，联邦微调在资源有限和隐私保护条件下变得尤为重要，但现有PEFT方法存在结构不兼容和适应性差的问题。

Method: 用稀疏专家混合（MoE）替代LoRA，设计门控网络实现个性化专家激活，并引入异质性感知的正则化以改善专家调度。

Result: 广泛实验表明，FFT MoE在泛化能力和训练效率方面优于现有最优基线，尤其在异质和非IID场景下表现突出。

Conclusion: FFT MoE通过结构创新和调度优化，有效提升联邦微调的适应性和性能，是面向未来多样化应用的重要框架。

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [142] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 本文研究了近似机器遗忘中的隐私风险，提出了针对未删除和保留样本的隐私审计标准，并发现现有方法可能危及差分隐私模型的样本隐私。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型中数据隐私的重要性增加，现有的机器遗忘算法在保证隐私方面存在潜在风险，亟需系统性审计。

Method: 提出未删除和保留样本的隐私标准，设计高效的MIAs（A-LiRA），并通过实验验证现有算法可能危及保留样本的隐私。

Result: 发现现有近似机器遗忘算法可能危及差分隐私模型中保留样本的隐私，强调需要发展差分隐私保证的遗忘算法。

Conclusion: 需要引入差分隐私保证的机器遗忘算法，并采用新颖的隐私审计方法确保模型隐私符合预期。

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [143] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: MoE模型中的稀疏性对记忆和推理能力有不同影响，记忆随参数增加持续提升，推理受限甚至退化，并且超参数调整难以弥补推理能力的不足。


<details>
  <summary>Details</summary>
Motivation: 探究Mixture-of-Experts模型中稀疏性对模型能力的影响，特别是在记忆与推理任务中的表现差异。

Method: 训练不同参数量和稀疏程度的MoE Transformer模型，监测训练损失、任务损失和正确率，分析稀疏性对两个能力的影响。

Result: 记忆能力随参数增长持续改善，推理能力则趋于饱和甚至退化。调节top-k和超参数对推理影响有限，后训练强化学习和测试时额外计算不能改善推理缺陷。

Conclusion: 稀疏性在MoE模型中对不同能力的影响不同，纯增加参数不能保证推理提升，优化稀疏结构是提升推理能力的关键。

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [144] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 提出一种结合微调和训练无关提示优点的LRTab方法，通过检索训练数据中的相关信息提升表格理解与推理的性能，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在解决纯微调或训练无关提示各自的局限性，提升表格推理的效果与泛化能力。

Method: 通过提示生成链式推理（CoT）反应，学习“提示条件”；在推理时检索相关的提示条件以提供上下文信息，从而结合训练数据的学习与零样本的通用性。

Result: 在WikiTQ和Tabfact数据集上，LRTab实现了优异的性能，具备良好的可解释性和成本效益，优于现有方法。

Conclusion: LRTab通过集成检索与学习机制，有效提升了表格理解和推理的能力，展现出较强的实用性和优越性。

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [145] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 提出了一种结合无监督自编码器和多层感知机的深度学习模型，用于早期败血症检测，显著优于传统机器学习方法，具有良好的泛化和临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 败血症早期检测对生命安全关键，但现有方法在处理不规则、缺失的时间序列数据时效果有限。

Method: 结合自编码器进行自动特征提取和多层感知机分类，采用定制的下采样和动态滑动窗口机制，以增强模型的临床适用性和实时性，处理带缺失指标的时间序列。

Result: 在三个ICU数据集上，模型分别达到了74.6%、80.6%和93.5%的准确率，明显优于传统机器学习方法，表现出良好的鲁棒性和推广性。

Conclusion: 提出的深度学习框架在早期败血症检测中具有优越的性能，适用于异质性强的ICU环境，具有潜在的临床应用价值。

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [146] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: 结合统计物理、编码理论和拓扑学的统一框架，实现高效多类图像分类，压缩特征同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 提升图像分类效率及准确率，通过物理和拓扑方法优化特征表示。

Method: 将高维特征视为自旋在稀疏LDPC图上的表现，利用Bethe-Hessian矩阵在Nishimori温度下实现最大类别区分，设计拓扑导向的图结构。

Result: 大幅压缩特征参数量（40倍），在ImageNet子集上仍达98.7%和82.7%的高准确率，显著优于现有技术。

Conclusion: 拓扑引导的图结构设计能实现高效、物理启发的嵌入方法，有助于未来复杂分类任务。

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [147] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: 提出了一种基于结构感知的图自监督学习框架StructRTL，用于提升RTL设计质量估计，结合知识蒸馏实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 快速、准确地估算RTL设计的质量指标，以优化EDA流程。

Method: 引入结构感知的图自监督学习模型StructRTL，利用控制数据流图（CDFG）进行结构表示学习，并结合知识蒸馏从后映射网络中提取细节信息。

Result: 该方法在多项质量估计任务中超越了现有技术，达到了新的最佳性能，验证了结构学习与跨阶段监督的有效性。

Conclusion: 融合结构感知的图模型与知识蒸馏策略，显著提升RTL设计质量估计的准确性和效率。

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


### [148] [FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks](https://arxiv.org/abs/2508.18737)
*Enrique Mármol Campos,Aurora González Vidal,José Luis Hernández Ramos,Antonio Skarmeta*

Main category: cs.LG

TL;DR: 提出了一种两阶段防御框架FLAegis，有效检测和防御拜占庭客户端的攻击，提升联邦学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性带来安全风险，恶意拜占庭客户端可通过污染模型破坏系统。

Method: 利用符号时间序列变换（SAX）增强差异，采用频谱聚类检测攻击行为，并引入基于FFT的稳健聚合函数。

Result: 在五种 poisoning 攻击下，显著优于现有方法，保持高检测精度和模型性能。

Conclusion: FLAegis有效提升FL系统对拜占庭攻击的抵抗能力，为安全联邦学习提供新方案。

Abstract: Federated Learning (FL) has become a powerful technique for training Machine
Learning (ML) models in a decentralized manner, preserving the privacy of the
training datasets involved. However, the decentralized nature of FL limits the
visibility of the training process, relying heavily on the honesty of
participating clients. This assumption opens the door to malicious third
parties, known as Byzantine clients, which can poison the training process by
submitting false model updates. Such malicious clients may engage in poisoning
attacks, manipulating either the dataset or the model parameters to induce
misclassification. In response, this study introduces FLAegis, a two-stage
defensive framework designed to identify Byzantine clients and improve the
robustness of FL systems. Our approach leverages symbolic time series
transformation (SAX) to amplify the differences between benign and malicious
models, and spectral clustering, which enables accurate detection of
adversarial behavior. Furthermore, we incorporate a robust FFT-based
aggregation function as a final layer to mitigate the impact of those Byzantine
clients that manage to evade prior defenses. We rigorously evaluate our method
against five poisoning attacks, ranging from simple label flipping to adaptive
optimization-based strategies. Notably, our approach outperforms
state-of-the-art defenses in both detection precision and final model accuracy,
maintaining consistently high performance even under strong adversarial
conditions.

</details>


### [149] [Stability and Generalization for Bellman Residuals](https://arxiv.org/abs/2508.18741)
*Enoch H. Kang,Kyoungseok Jang*

Main category: cs.LG

TL;DR: 本文分析了离线强化学习中的贝尔曼残差最小化（BRM）的统计性质，提出了新的稳定性及样本复杂度界限，适用于神经网络和小批量SGD。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中贝尔曼一致性难以保证的问题，提升算法的统计性能和稳定性。

Method: 通过引入单一的Lyapunov势能分析，建立SGDA在相邻数据集上的联系，导出样本复杂度和风险界限，且不依赖于特殊的正则化或独立性假设。

Result: 证明了BRM的平均论域稳定性为O(1/n)，且该稳定性转化为无方差减算的超额风险界限，增强了离线RL和IRL的理论保障。

Conclusion: 本研究提升了对离线强化学习中BRM方法统计行为的认识，扩展了其理论适用范围，为未来算法设计提供理论基础。

Abstract: Offline reinforcement learning and offline inverse reinforcement learning aim
to recover near-optimal value functions or reward models from a fixed batch of
logged trajectories, yet current practice still struggles to enforce Bellman
consistency. Bellman residual minimization (BRM) has emerged as an attractive
remedy, as a globally convergent stochastic gradient descent-ascent based
method for BRM has been recently discovered. However, its statistical behavior
in the offline setting remains largely unexplored. In this paper, we close this
statistical gap. Our analysis introduces a single Lyapunov potential that
couples SGDA runs on neighbouring datasets and yields an O(1/n) on-average
argument-stability bound-doubling the best known sample-complexity exponent for
convex-concave saddle problems. The same stability constant translates into the
O(1/n) excess risk bound for BRM, without variance reduction, extra
regularization, or restrictive independence assumptions on minibatch sampling.
The results hold for standard neural-network parameterizations and minibatch
SGD.

</details>


### [150] [Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming](https://arxiv.org/abs/2508.18742)
*Jiajun Li,Ran Hou,Yu Ding,Yixuan Li,Shisi Guan,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang*

Main category: cs.LG

TL;DR: 提出一种基于约束的模型简化方法，有效识别关键约束，提升MILP求解效率和解的质量。


<details>
  <summary>Details</summary>
Motivation: 目前大多模型简化方法集中在变量层面，忽视约束的潜在简化空间，尤其是从对偶角度考虑的约束减弱。

Method: 通过标签最优解中的紧束约束，并利用多模态表示技术结合实例和抽象级信息，识别关键约束。

Result: 新方法在解决复杂MILP问题时，显著优于现有方法，解决方案质量提高超过50%，计算时间缩短17.47%。

Conclusion: 基于约束的模型简化具有巨大潜力，可显著提升MILP求解性能，未来有望广泛应用与优化问题中。

Abstract: Model reduction, which aims to learn a simpler model of the original mixed
integer linear programming (MILP), can solve large-scale MILP problems much
faster. Most existing model reduction methods are based on variable reduction,
which predicts a solution value for a subset of variables. From a dual
perspective, constraint reduction that transforms a subset of inequality
constraints into equalities can also reduce the complexity of MILP, but has
been largely ignored. Therefore, this paper proposes a novel constraint-based
model reduction approach for the MILP. Constraint-based MILP reduction has two
challenges: 1) which inequality constraints are critical such that reducing
them can accelerate MILP solving while preserving feasibility, and 2) how to
predict these critical constraints efficiently. To identify critical
constraints, we first label these tight-constraints at the optimal solution as
potential critical constraints and design a heuristic rule to select a subset
of critical tight-constraints. To learn the critical tight-constraints, we
propose a multi-modal representation technique that leverages information from
both instance-level and abstract-level MILP formulations. The experimental
results show that, compared to the state-of-the-art methods, our method
improves the quality of the solution by over 50\% and reduces the computation
time by 17.47\%.

</details>


### [151] [UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](https://arxiv.org/abs/2508.18756)
*Zihao Huang,Yu Bao,Qiyang Min,Siyan Chen,Ran Guo,Hongzhi Huang,Defa Zhu,Yutao Zeng,Banggu Wu,Xun Zhou,Siyuan Qiao*

Main category: cs.LG

TL;DR: UltraMemV2通过整合多项创新，实现了在极低内存访问的情况下媲美多专家MoE模型的性能，优化了稀疏模型的效率。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型高内存访问成本的问题，寻找低内存访问同时保持高性能的模型架构。

Method: 引入多项改进，包括在每个transformer块中集成记忆层、简化值扩展、采用FFN值处理、合理初始化参数和重新平衡记忆与FFN计算比率。

Result: UltraMemV2在相同计算和参数条件下达到了8专家MoE模型的性能，且极大降低了内存访问，在长文本记忆、多轮记忆和上下文学习任务中表现优越。

Conclusion: UltraMemV2成功实现了内存层架构与最先进MoE模型的性能等同性，提供了一种高效的稀疏模型替代方案，突显激活密度对性能的影响大于稀疏参数总量。

Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by
activating only subsets of parameters, they suffer from high memory access
costs during inference. Memory-layer architectures offer an appealing
alternative with very few memory access, but previous attempts like UltraMem
have only matched the performance of 2-expert MoE models, falling significantly
short of state-of-the-art 8-expert configurations. We present UltraMemV2, a
redesigned memory-layer architecture that closes this performance gap. Our
approach introduces five key improvements: integrating memory layers into every
transformer block, simplifying value expansion with single linear projections,
adopting FFN-based value processing from PEER, implementing principled
parameter initialization, and rebalancing memory-to-FFN computation ratios.
Through extensive evaluation, we demonstrate that UltraMemV2 achieves
performance parity with 8-expert MoE models under same computation and
parameters but significantly low memory access. Notably, UltraMemV2 shows
superior performance on memory-intensive tasks, with improvements of +1.6
points on long-context memorization, +6.2 points on multi-round memorization,
and +7.9 points on in-context learning. We validate our approach at scale with
models up to 2.5B activated parameters from 120B total parameters, and
establish that activation density has greater impact on performance than total
sparse parameter count. Our work brings memory-layer architectures to
performance parity with state-of-the-art MoE models, presenting a compelling
alternative for efficient sparse computation.

</details>


### [152] [Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement](https://arxiv.org/abs/2508.18765)
*Helen Pervez,Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: 提出一种面向多智能体系统的治理机制GaaS，通过声明式规则和信任评分实现运行时监管，有效阻止高风险行为，保障系统安全，提高可审计性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统向分布式生态演进，缺乏可扩展、解耦的治理机制带来结构性风险。现有机制反应迟缓、嵌入式难以审计和通用，亟需创新解决方案。

Method: 引入模块化、基于政策的治理层GaaS，利用声明式规则和信任因子机制进行实时监控、干预。通过对开源模型的模拟验证，包括内容生成和金融决策场景，对比无治理、治理和对抗环境的效果。

Result: GaaS有效阻止高风险行为，保障系统吞吐量，信任评分能识别并惩罚不可信组件，系统表现出稳定性和鲁棒性。

Conclusion: 将治理作为运行时服务，像计算或存储一样基础化，为多智能体生态系统提供基础设施级的对齐保障。

Abstract: As AI systems evolve into distributed ecosystems with autonomous execution,
asynchronous reasoning, and multi-agent coordination, the absence of scalable,
decoupled governance poses a structural risk. Existing oversight mechanisms are
reactive, brittle, and embedded within agent architectures, making them
non-auditable and hard to generalize across heterogeneous deployments.
  We introduce Governance-as-a-Service (GaaS): a modular, policy-driven
enforcement layer that regulates agent outputs at runtime without altering
model internals or requiring agent cooperation. GaaS employs declarative rules
and a Trust Factor mechanism that scores agents based on compliance and
severity-weighted violations. It enables coercive, normative, and adaptive
interventions, supporting graduated enforcement and dynamic trust modulation.
  To evaluate GaaS, we conduct three simulation regimes with open-source models
(LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial
decision-making. In the baseline, agents act without governance; in the second,
GaaS enforces policies; in the third, adversarial agents probe robustness. All
actions are intercepted, evaluated, and logged for analysis. Results show that
GaaS reliably blocks or redirects high-risk behaviors while preserving
throughput. Trust scores track rule adherence, isolating and penalizing
untrustworthy components in multi-agent systems.
  By positioning governance as a runtime service akin to compute or storage,
GaaS establishes infrastructure-level alignment for interoperable agent
ecosystems. It does not teach agents ethics; it enforces them.

</details>


### [153] [Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI](https://arxiv.org/abs/2508.18766)
*Hongbo Liu,Siyi Li,Zheng Yu*

Main category: cs.LG

TL;DR: HGNN-DDI通过异构图神经网络整合多源药物信息，有效预测药物-药物相互作用，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 药物间相互作用影响临床治疗效果，需准确预测以保障用药安全。

Method: 采用异构图神经网络模型，整合多种药物相关数据，进行关系预测。

Result: 在基准数据集上，HGNN-DDI在预测准确性和鲁棒性方面优于现有先进模型。

Conclusion: 该模型有望促进更安全的药物开发和精准医疗。

Abstract: Drug-drug interactions (DDIs) are a major concern in clinical practice, as
they can lead to reduced therapeutic efficacy or severe adverse effects.
Traditional computational approaches often struggle to capture the complex
relationships among drugs, targets, and biological entities. In this work, we
propose HGNN-DDI, a heterogeneous graph neural network model designed to
predict potential DDIs by integrating multiple drug-related data sources.
HGNN-DDI leverages graph representation learning to model heterogeneous
biomedical networks, enabling effective information propagation across diverse
node and edge types. Experimental results on benchmark DDI datasets demonstrate
that HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy and
robustness, highlighting its potential to support safer drug development and
precision medicine.

</details>


### [154] [Federated Learning with Heterogeneous and Private Label Sets](https://arxiv.org/abs/2508.18774)
*Adam Breitholtz,Edvin Listo Zec,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本论文研究了联邦学习中异质标签集的影响，提出适应私有标签集环境的方法，显示在保持隐私的同时模型性能依然优良。


<details>
  <summary>Details</summary>
Motivation: 探讨在实际应用中常见的异质标签集对联邦学习模型性能的影响，以及不同标签信息共享方式的效果。

Method: 采用经典分类器组合方法、调整FL常用技术，并在不同标签设置下进行实验分析。

Result: 减少可用标签损害性能，但通过模型对齐的集中调优可以缓解影响。改良方法在私有标签环境下表现良好，与公共标签环境相近，增强隐私保护同时保持性能。

Conclusion: 合理调整模型和方法能够在保护隐私的同时，确保联邦学习模型的性能，提供了实践中兼顾隐私与效果的解决方案。

Abstract: Although common in real-world applications, heterogeneous client label sets
are rarely investigated in federated learning (FL). Furthermore, in the cases
they are, clients are assumed to be willing to share their entire label sets
with other clients. Federated learning with private label sets, shared only
with the central server, adds further constraints on learning algorithms and
is, in general, a more difficult problem to solve. In this work, we study the
effects of label set heterogeneity on model performance, comparing the public
and private label settings -- when the union of label sets in the federation is
known to clients and when it is not. We apply classical methods for the
classifier combination problem to FL using centralized tuning, adapt common FL
methods to the private label set setting, and discuss the justification of both
approaches under practical assumptions. Our experiments show that reducing the
number of labels available to each client harms the performance of all methods
substantially. Centralized tuning of client models for representational
alignment can help remedy this, but often at the cost of higher variance.
Throughout, our proposed adaptations of standard FL methods perform well,
showing similar performance in the private label setting as the standard
methods achieve in the public setting. This shows that clients can enjoy
increased privacy at little cost to model accuracy.

</details>


### [155] [SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation](https://arxiv.org/abs/2508.18826)
*Junyu Yan,Feng Chen,Yuyang Xue,Yuning Du,Konstantinos Vilouras,Sotirios A. Tsaftaris,Steven McDonagh*

Main category: cs.LG

TL;DR: 提出了一种名为SWiFT的低成本偏置消除方法，通过微调模型参数，减少偏差同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有偏差消除方法成本高、效果有限的问题，提升模型在敏感领域的公平性和泛化能力。

Method: 基于参数对偏差与预测性能贡献的分析，采用两步微调策略调整参数，以实现偏差最小化。

Result: 在多个偏敏感属性和数据集上实验，表现出优越的偏差减少效果和诊断准确率，甚至提升在out-of-distribution数据上的泛化能力。

Conclusion: SWiFT方法高效实用，有望促进公平智能模型的应用推广。

Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.

</details>


### [156] [DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift](https://arxiv.org/abs/2508.18839)
*Shae McFadden,Myles Foley,Mario D'Onghia,Chris Hicks,Vasilios Mavroudis,Nicola Paoletti,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度强化学习的恶意软件检测方法，通过优化检测和样本拒绝策略，提高了在动态环境中的鲁棒性和性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 面对实际环境中的恶意软件威胁不断演变、标注资源有限及预测不确定性，传统检测方法难以应对概念漂移，亟需新策略提升检测稳定性。

Method: 将恶意软件检测问题建模为一阶马尔可夫决策过程，训练深度强化学习代理，联合优化样本分类和高风险样本的手动标注拒绝策略。

Result: 在Android恶意软件数据集上进行时间感知评价，DRL代理显著优于传统分类方法，提升AUT性能，特别是在面对多年的数据漂移时表现出更强的抗干扰能力。

Conclusion: 首次证明深度强化学习在动态恶意软件检测中具有潜力，可提升系统对概念漂移的适应能力和整体检测效果。

Abstract: Malware detection in real-world settings must deal with evolving threats,
limited labeling budgets, and uncertain predictions. Traditional classifiers,
without additional mechanisms, struggle to maintain performance under concept
drift in malware domains, as their supervised learning formulation cannot
optimize when to defer decisions to manual labeling and adaptation. Modern
malware detection pipelines combine classifiers with monthly active learning
(AL) and rejection mechanisms to mitigate the impact of concept drift. In this
work, we develop a novel formulation of malware detection as a one-step Markov
Decision Process and train a deep reinforcement learning (DRL) agent,
simultaneously optimizing sample classification performance and rejecting
high-risk samples for manual labeling. We evaluated the joint detection and
drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent
through time-aware evaluations on Android malware datasets subject to realistic
drift requiring multi-year performance stability. The policies learned under
these conditions achieve a higher Area Under Time (AUT) performance compared to
standard classification approaches used in the domain, showing improved
resilience to concept drift. Specifically, the DRMD agent achieved a
$5.18\pm5.44$, $14.49\pm12.86$, and $10.06\pm10.81$ average AUT performance
improvement for the classification only, classification with rejection, and
classification with rejection and AL settings, respectively. Our results
demonstrate for the first time that DRL can facilitate effective malware
detection and improved resiliency to concept drift in the dynamic environment
of the Android malware domain.

</details>


### [157] [Recycling History: Efficient Recommendations from Contextual Dueling Bandits](https://arxiv.org/abs/2508.18841)
*Suryanarayana Sankagiri,Jalal Etesami,Pouria Fatemi,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 提出一种基于用户消费历史的上下文决斗臂模型，通过合理构建信息丰富的查询，优化反馈利用以实现O(√T)的 regret 下降，提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文臂模型未考虑用户在消费后提供更可靠反馈的情况，本研究旨在利用用户已消费内容的历史信息，改善推荐算法的性能。

Method: 结合历史内容的多样性条件，设计探测阶段和多轮查询策略，利用矩阵浓缩界限保证历史信息丰富，从而实现低 regret。

Result: 在理论分析中证明了在满足多样性条件的历史和短暂随机探索后，算法能达到O(√T)的 regret ；模拟显示重用历史内容比单纯比较推荐内容有显著优势。

Conclusion: 利用丰富的用户历史信息构建有效查询，提高反馈质量，实现低regret的上下文臂模型，为内容推荐提供新的研究方向。

Abstract: The contextual duelling bandit problem models adaptive recommender systems,
where the algorithm presents a set of items to the user, and the user's choice
reveals their preference. This setup is well suited for implicit choices users
make when navigating a content platform, but does not capture other possible
comparison queries. Motivated by the fact that users provide more reliable
feedback after consuming items, we propose a new bandit model that can be
described as follows. The algorithm recommends one item per time step; after
consuming that item, the user is asked to compare it with another item chosen
from the user's consumption history. Importantly, in our model, this comparison
item can be chosen without incurring any additional regret, potentially leading
to better performance. However, the regret analysis is challenging because of
the temporal dependency in the user's history. To overcome this challenge, we
first show that the algorithm can construct informative queries provided the
history is rich, i.e., satisfies a certain diversity condition. We then show
that a short initial random exploration phase is sufficient for the algorithm
to accumulate a rich history with high probability. This result, proven via
matrix concentration bounds, yields $O(\sqrt{T})$ regret guarantees.
Additionally, our simulations show that reusing past items for comparisons can
lead to significantly lower regret than only comparing between simultaneously
recommended items.

</details>


### [158] [C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning](https://arxiv.org/abs/2508.18860)
*Wei Li,Hangjie Yuan,Zixiang Zhao,Yifan Zhu,Aojun Lu,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: 提出C-Flat方法，通过鼓励平坦的损失景观以改善持续学习中的性能，从而提升记忆保持和学习效率，且具有良好的兼容性和效率。


<details>
  <summary>Details</summary>
Motivation: 在持续学习中平衡对新任务的敏感性和对旧知识的稳定性，现有方法依赖零阶尖峰度可能导致不鲁棒的结果。

Method: 提出C-Flat，通过促进平坦的损失景观，并结合C-Flat++，提升性能和效率，兼容各种持续学习框架。

Result: C-Flat在多种设置中表现出优越性能，C-Flat++降低了更新成本，实证验证了方法的有效性和效率。

Conclusion: 平坦性导向的方法有效提升持续学习表现，C-Flat及其改进版本具有广泛适用性和高效性。

Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge
is crucial in continual learning (CL). Recently, sharpness-aware minimization
has proven effective in transfer learning and has also been adopted in
continual learning (CL) to improve memory retention and learning efficiency.
However, relying on zeroth-order sharpness alone may favor sharper minima over
flatter ones in certain settings, leading to less robust and potentially
suboptimal solutions. In this paper, we propose \textbf{C}ontinual
\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss
landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling
easy integration with minimal modifications to the code pipeline. Besides, we
present a general framework that integrates C-Flat into all major CL paradigms
and conduct comprehensive comparisons with loss-minima optimizers and
flat-minima-based CL methods. Our results show that C-Flat consistently
improves performance across a wide range of settings. In addition, we introduce
C-Flat++, an efficient yet effective framework that leverages selective
flatness-driven promotion, significantly reducing the update cost required by
C-Flat. Extensive experiments across multiple CL methods, datasets, and
scenarios demonstrate the effectiveness and efficiency of our proposed
approaches. Code is available at https://github.com/WanNaa/C-Flat.

</details>


### [159] [MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes](https://arxiv.org/abs/2508.18873)
*Yunyang Cao,Juekai Lin,Wenhao Li,Bo Jin*

Main category: cs.LG

TL;DR: 提出一种动态多阶因果关系发现框架MOCHA，有效揭示时间点过程中的复杂因果结构，实现预测与解释


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注静态或一阶因果结构，忽视多阶和时变因果关系的复杂性

Method: 引入时间变化的有向无环图（DAG）结构，通过端到端可微框架联合建模因果关系和事件预测

Result: 在真实数据集上实现了优越的预测性能，并揭示了有意义的因果结构

Conclusion: MOCHA框架成功捕捉复杂时变因果关系，具有较强的解释能力和实用价值

Abstract: Discovering complex causal dependencies in temporal point processes (TPPs) is
critical for modeling real-world event sequences. Existing methods typically
rely on static or first-order causal structures, overlooking the multi-order
and time-varying nature of causal relationships. In this paper, we propose
MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.
MOCHA characterizes multi-order influences as multi-hop causal paths over a
latent time-evolving graph. To model such dynamics, we introduce a time-varying
directed acyclic graph (DAG) with learnable structural weights, where
acyclicity and sparsity constraints are enforced to ensure structural validity.
We design an end-to-end differentiable framework that jointly models causal
discovery and TPP dynamics, enabling accurate event prediction and revealing
interpretable structures. Extensive experiments on real-world datasets
demonstrate that MOCHA not only achieves state-of-the-art performance in event
prediction, but also reveals meaningful and interpretable causal structures.

</details>


### [160] [HAEPO: History-Aggregated Exploratory Policy Optimization](https://arxiv.org/abs/2508.18884)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: HAEPO提出了一种基于历史轨迹的探索优化方法，通过压缩轨迹信息和软最大化技术，提高了长期任务中的探索效率和学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在长时序任务中探索不足与稳定性差的问题。

Method: 引入历史轨迹的对数概率累加，加权轨迹选择，结合熵正则化和软KL惩罚，以增强探索和稳定性。

Result: HAEPO快速收敛、充分探索、与真实奖励高度契合，在多任务中表现优越或与现有方法持平。

Conclusion: HAEPO通过显式利用完整轨迹历史，实现稳定、可解释的长序列任务学习框架，改善探索与稳定的平衡。

Abstract: Exploration is essential in modern learning, from reinforcement learning
environments with small neural policies to large language models (LLMs).
Existing work, such as DPO, leverages full sequence log-likelihoods to capture
an entire trajectory of the model's decisions, while methods like GRPO
aggregate per-token ratios into a trajectory-level update. However, both often
limit exploration on long-horizon tasks. We introduce History-Aggregated
Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to
combat these shortcomings. HAEPO compresses each trajectory into the sum of its
logarithmic probabilities (a cumulative logarithmic likelihood), and applies a
Plackett-Luce softmax across trajectories to obtain normalized weights
proportional to their returns, thus encouraging broader exploration. We add
entropy regularization to stabilize the aggressive updates to prevent premature
collapse and a soft KL penalty relative to a frozen copy of the previous
(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,
aligns closely with true rewards, and demonstrates robust learning behavior
better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO
provides a stable and interpretable framework by explicitly leveraging
full-trajectory history while balancing exploration and stability.

</details>


### [161] [pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data](https://arxiv.org/abs/2508.18891)
*Zhijin Wang,Senzhen Wu,Yue Hu,Xiufeng Liu*

Main category: cs.LG

TL;DR: pyFAST是一个基于PyTorch的时间序列分析框架，强调数据处理与模型计算的解耦，支持复杂数据场景，提供丰富模型与工具，促进快速实验与拓展。


<details>
  <summary>Details</summary>
Motivation: 解决现有Python时间序列库在模块化、多源、多样数据支持方面的局限性，满足复杂、多样化应用需求。

Method: 设计一个分离数据处理和模型计算的架构，支持多源数据加载、序列处理、稀疏数据融合等功能，集成多种模型与训练工具，构建可扩展的研究平台。

Result: pyFAST具备灵活、高效、扩展性强的功能，支持多种模型和任务，促进时间序列研究的发展。

Conclusion: pyFAST为时间序列分析提供了一个全面、多功能、开源平台，推动复杂场景下的研究与应用。

Abstract: Modern time series analysis demands frameworks that are flexible, efficient,
and extensible. However, many existing Python libraries exhibit limitations in
modularity and in their native support for irregular, multi-source, or sparse
data. We introduce pyFAST, a research-oriented PyTorch framework that
explicitly decouples data processing from model computation, fostering a
cleaner separation of concerns and facilitating rapid experimentation. Its data
engine is engineered for complex scenarios, supporting multi-source loading,
protein sequence handling, efficient sequence- and patch-level padding, dynamic
normalization, and mask-based modeling for both imputation and forecasting.
pyFAST integrates LLM-inspired architectures for the alignment-free fusion of
sparse data sources and offers native sparse metrics, specialized loss
functions, and flexible exogenous data fusion. Training utilities include
batch-based streaming aggregation for evaluation and device synergy to maximize
computational efficiency. A comprehensive suite of classical and deep learning
models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a
modular architecture that encourages extension. Released under the MIT license
at GitHub, pyFAST provides a compact yet powerful platform for advancing time
series research and applications.

</details>


### [162] [Distance-informed Neural Processes](https://arxiv.org/abs/2508.18903)
*Aishwarya Venkataramanan,Joachim Denzler*

Main category: cs.LG

TL;DR: 提出了一种结合全局和局部距离信息的神经过程模型，增强了不确定性估计和对数据关系的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 改进现有神经过程模型在不确定性校准和局部依赖捕捉上的不足。

Method: 引入全局和局部潜变量，通过双向利普希茨正则化保持输入关系的距离信息。

Result: 在回归和分类任务中表现出优越的预测性能和不确定性校准效果。

Conclusion: 距离感知神经过程（DNP）有效提升了神经过程的表达能力和不确定性估计品质。

Abstract: We propose the Distance-informed Neural Process (DNP), a novel variant of
Neural Processes that improves uncertainty estimation by combining global and
distance-aware local latent structures. Standard Neural Processes (NPs) often
rely on a global latent variable and struggle with uncertainty calibration and
capturing local data dependencies. DNP addresses these limitations by
introducing a global latent variable to model task-level variations and a local
latent variable to capture input similarity within a distance-preserving latent
space. This is achieved through bi-Lipschitz regularization, which bounds
distortions in input relationships and encourages the preservation of relative
distances in the latent space. This modeling approach allows DNP to produce
better-calibrated uncertainty estimates and more effectively distinguish in-
from out-of-distribution data. Empirical results demonstrate that DNP achieves
strong predictive performance and improved uncertainty calibration across
regression and classification tasks.

</details>


### [163] [Enhancing Model Privacy in Federated Learning with Random Masking and Quantization](https://arxiv.org/abs/2508.18911)
*Zhibo Xu,Jianhao Zhu,Jingwen Xu,Changze Lv,Zisu Huang,Xiaohua Wang,Muling Wu,Qi Qian,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.LG

TL;DR: 本方法在联邦学习中既保持了模型性能，又增强了参数保护。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中模型性能和参数安全的双重需求。

Method: 采用一种新的策略以平衡性能和参数保护。

Result: 实验显示该方法在多模型多任务环境中表现优越。

Conclusion: 所提出方法能有效同时确保模型性能和参数安全。

Abstract: Experimental results across various models and tasks demonstrate that our
approach not only maintains strong model performance in federated learning
settings but also achieves enhanced protection of model parameters compared to
baseline methods.

</details>


### [164] [Generalization Bound for a General Class of Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.18920)
*Madhusudan Verma,Manoj Kumar*

Main category: cs.LG

TL;DR: 本文首次为含有非线性动态的神经常微分方程提供了泛化误差界限，拓展了识别范围。


<details>
  <summary>Details</summary>
Motivation: 理解神经ODE在未知数据上的表现，特别是非线性动态情况下的泛化能力。

Method: 分析满足Lipschitz条件的非线性动态神经ODE，建立了具有时间依赖和无时间依赖的模型的泛化界限，探讨了过参数化和域限制的影响。

Result: 证明了在Lipschitz条件下，神经ODE解具有界定的变化范围，首次提出了适用于非线性动态的泛化界限。

Conclusion: 为神经ODE模型提供了理论基础，有助于理解其在实际应用中的泛化性能。

Abstract: Neural ordinary differential equations (neural ODEs) are a popular type of
deep learning model that operate with continuous-depth architectures. To assess
how well such models perform on unseen data, it is crucial to understand their
generalization error bounds. Previous research primarily focused on the linear
case for the dynamics function in neural ODEs - Marion, P. (2023), or provided
bounds for Neural Controlled ODEs that depend on the sampling interval
Bleistein et al. (2023). In this work, we analyze a broader class of neural
ODEs where the dynamics function is a general nonlinear function, either time
dependent or time independent, and is Lipschitz continuous with respect to the
state variables. We showed that under this Lipschitz condition, the solutions
to neural ODEs have solutions with bounded variations. Based on this
observation, we establish generalization bounds for both time-dependent and
time-independent cases and investigate how overparameterization and domain
constraints influence these bounds. To our knowledge, this is the first
derivation of generalization bounds for neural ODEs with general nonlinear
dynamics.

</details>


### [165] [HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling](https://arxiv.org/abs/2508.18922)
*Yao Wu*

Main category: cs.LG

TL;DR: HierCVAE是一种结合层级注意力机制与条件变分自编码器的时序建模架构，在能源数据预测中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统中多尺度时间依赖与不确定性问题。

Method: 集成三层注意力结构、多模态条件编码及ResFormer块，提供不确定性量化。

Result: 在能源消费数据上实现15-40%的预测提升，优于现有方法，适合长时段与多变量依赖。

Conclusion: HierCVAE有效增强时间序列预测的准确性与不确定性处理能力，适合复杂多变的场景。

Abstract: Temporal modeling in complex systems requires capturing dependencies across
multiple time scales while managing inherent uncertainties. We propose
HierCVAE, a novel architecture that integrates hierarchical attention
mechanisms with conditional variational autoencoders to address these
challenges. HierCVAE employs a three-tier attention structure (local, global,
cross-temporal) combined with multi-modal condition encoding to capture
temporal, statistical, and trend information. The approach incorporates
ResFormer blocks in the latent space and provides explicit uncertainty
quantification via prediction heads. Through evaluations on energy consumption
datasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy and
superior uncertainty calibration compared to state-of-the-art methods,
excelling in long-term forecasting and complex multi-variate dependencies.

</details>


### [166] [Energy-Based Flow Matching for Generating 3D Molecular Structure](https://arxiv.org/abs/2508.18949)
*Wenyin Zhou,Christopher Iliffe Sprague,Vsevolod Viliuga,Matteo Tadiello,Arne Elofsson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 提出了一种基于能量的流匹配方法，用于分子结构生成，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 改进分子结构生成的训练与推断，提高模型性能和稳定性。

Method: 采用能量视角，通过深网络迭代学习映射函数，将随机配置映射到目标结构，结合流匹配技术。

Result: 在蛋白质对接和骨架生成任务中优于最新的流匹配和扩散模型基线，具有良好的理论和实践效果。

Conclusion: 该方法在分子结构生成中具有理论和应用优势，有助于推动相关生物医学研究。

Abstract: Molecular structure generation is a fundamental problem that involves
determining the 3D positions of molecules' constituents. It has crucial
biological applications, such as molecular docking, protein folding, and
molecular design. Recent advances in generative modeling, such as diffusion
models and flow matching, have made great progress on these tasks by modeling
molecular conformations as a distribution. In this work, we focus on flow
matching and adopt an energy-based perspective to improve training and
inference of structure generation models. Our view results in a mapping
function, represented by a deep network, that is directly learned to
\textit{iteratively} map random configurations, i.e. samples from the source
distribution, to target structures, i.e. points in the data manifold. This
yields a conceptually simple and empirically effective flow matching setup that
is theoretically justified and has interesting connections to fundamental
properties such as idempotency and stability, as well as the empirically useful
techniques such as structure refinement in AlphaFold. Experiments on protein
docking as well as protein backbone generation consistently demonstrate the
method's effectiveness, where it outperforms recent baselines of
task-associated flow matching and diffusion models, using a similar
computational budget.

</details>


### [167] [Estimating Conditional Covariance between labels for Multilabel Data](https://arxiv.org/abs/2508.18951)
*Laurence A. F. Park,Jesse Read*

Main category: cs.LG

TL;DR: 本文比较了三种模型（多变量Probit、多变量Bernoulli、分阶段Logit）在估计条件标签协方差中的表现，发现它们在测量常数和依赖性协方差方面效果相似，但对存在常数协方差的数据会误判存在依赖性协方差。多变量Probit模型误差最低。


<details>
  <summary>Details</summary>
Motivation: 在多标签数据分析中，理解标签依赖性对模型性能影响重大，但直接测量依赖性存在困难，需通过模型估计实现。

Method: 通过比较多变量Probit、多变量Bernoulli和分阶段Logit模型在估计条件标签协方差方面的表现，使用实验观察模型对协方差的测量能力。

Result: 所有模型在衡量常数和依赖性协方差方面表现相似，但在存在常数协方差的数据上会误判依赖性。多变量Probit模型的误差最小。

Conclusion: 模型在估计标签依赖性方面能力有限，尤其在存在常数协方差时会出现误判，但多变量Probit模型总体表现较好。

Abstract: Multilabel data should be analysed for label dependence before applying
multilabel models. Independence between multilabel data labels cannot be
measured directly from the label values due to their dependence on the set of
covariates $\vec{x}$, but can be measured by examining the conditional label
covariance using a multivariate Probit model. Unfortunately, the multivariate
Probit model provides an estimate of its copula covariance, and so might not be
reliable in estimating constant covariance and dependent covariance. In this
article, we compare three models (Multivariate Probit, Multivariate Bernoulli
and Staged Logit) for estimating the constant and dependent multilabel
conditional label covariance. We provide an experiment that allows us to
observe each model's measurement of conditional covariance. We found that all
models measure constant and dependent covariance equally well, depending on the
strength of the covariance, but the models all falsely detect that dependent
covariance is present for data where constant covariance is present. Of the
three models, the Multivariate Probit model had the lowest error rate.

</details>


### [168] [On the Generalisation of Koopman Representations for Chaotic System Control](https://arxiv.org/abs/2508.18954)
*Kyriakos Hjikakou,Juan Diego Cardenas Cartagena,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 本文研究了Koopman表示在混沌动力系统中的泛化能力，验证其在不同任务中的迁移能力，特别是在预测和控制方面的应用。


<details>
  <summary>Details</summary>
Motivation: 希望提高动力系统建模和控制中表示的通用性和迁移性，为多任务学习提供基础。

Method: 提出三阶段方法：自编码学习Koopman嵌入、预训练变换器进行下一状态预测、微调以实现安全控制，使用Lorenz系统进行验证。

Result: Koopman嵌入优于传统和物理信息PCA方法，表现出准确、高效的迁移能力，预训练模型在微调中保持性能，表明其捕获的结构具有重用性。

Conclusion: Koopman嵌入可以作为多任务学习的基础，为物理信息机器学习提供有效的表示架构。

Abstract: This paper investigates the generalisability of Koopman-based representations
for chaotic dynamical systems, focusing on their transferability across
prediction and control tasks. Using the Lorenz system as a testbed, we propose
a three-stage methodology: learning Koopman embeddings through autoencoding,
pre-training a transformer on next-state prediction, and fine-tuning for
safety-critical control. Our results show that Koopman embeddings outperform
both standard and physics-informed PCA baselines, achieving accurate and
data-efficient performance. Notably, fixing the pre-trained transformer weights
during fine-tuning leads to no performance degradation, indicating that the
learned representations capture reusable dynamical structure rather than
task-specific patterns. These findings support the use of Koopman embeddings as
a foundation for multi-task learning in physics-informed machine learning. A
project page is available at https://kikisprdx.github.io/.

</details>


### [169] [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
*Tim Kreuzer,Jelena Zdravkovic,Panagiotis Papapetrou*

Main category: cs.LG

TL;DR: 提出了一种通用的后处理解释算法PAX-TS，用于揭示时间序列预测模型及其预测结果，支持多粒度解释并能识别跨通道相关性。


<details>
  <summary>Details</summary>
Motivation: 现代时间序列预测模型缺乏透明度，现有的后验解释方法不适用于预测场景，亟需有效的解释工具。

Method: 基于局部扰动，设计多粒度、多维度的解释方法，可以描述模型行为和跨通道关系。

Result: 在多个数据集和模型上验证了PAX-TS的有效性，能区分不同模型性能、识别预测模式，并展示跨通道相关性。

Conclusion: PAX-TS提供了多层次、细粒度的模型解释，有助于理解和实际应用时间序列预测模型。

Abstract: Time series forecasting has seen considerable improvement during the last
years, with transformer models and large language models driving advancements
of the state of the art. Modern forecasting models are generally opaque and do
not provide explanations for their forecasts, while well-known post-hoc
explainability methods like LIME are not suitable for the forecasting context.
We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time series
forecasting models and their forecasts. Our method is based on localized input
perturbations and results in multi-granular explanations. Further, it is able
to characterize cross-channel correlations for multivariate time series
forecasts. We clearly outline the algorithmic procedure behind PAX-TS,
demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,
compare it with two other state-of-the-art explanation algorithms, and present
the different explanation types of the method. We found that the explanations
of high-performing and low-performing algorithms differ on the same datasets,
highlighting that the explanations of PAX-TS effectively capture a model's
behavior. Based on time step correlation matrices resulting from the benchmark,
we identify 6 classes of patterns that repeatedly occur across different
datasets and algorithms. We found that the patterns are indicators of
performance, with noticeable differences in forecasting error between the
classes. Lastly, we outline a multivariate example where PAX-TS demonstrates
how the forecasting model takes cross-channel correlations into account. With
PAX-TS, time series forecasting models' mechanisms can be illustrated in
different levels of detail, and its explanations can be used to answer
practical questions on forecasts.

</details>


### [170] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出了一种改进的异构联邦学习方法FedProtoKD，通过增强的双重知识蒸馏机制和对比学习，解决原有原型聚合性能不足的问题，显著提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 应对异构模型和非IID数据带来的统计异质性和隐私保护挑战，提升联邦学习性能。

Method: 引入增强的双重知识蒸馏机制，利用对比学习优化类原型，采用类间自适应原型边界和对公开样本的重要性评估。

Result: 在多种设置下，FedProtoKD的平均准确率提升1.13%到34.13%，显著优于现有的HFL方法。

Conclusion: 通过创新的原型优化和知识蒸馏机制，有效解决原型缩减问题，提升异构联邦学习的性能。

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [171] [STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems](https://arxiv.org/abs/2508.19011)
*Gary Simethy,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.LG

TL;DR: STDiff通过模拟系统状态变化，利用条件去噪扩散模型，有效解决工业时间序列中长缺失值的插补问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习插补方法在工业场景中难以应对非平稳性和长时间缺失带来的挑战。

Method: 引入条件去噪扩散模型，结合控制理论的因果偏向，逐步生成缺失值。

Result: 在公共污水处理数据集和工业实际数据集上均表现出优异性能，特别是在长缺失窗长度下效果更佳。

Conclusion: 动态感知和显式条件化的插补策略是工业时间序列中鲁棒的解决方案，具有广泛推广潜力。

Abstract: Most deep learning methods for imputing missing values treat the task as
completing patterns within a fixed time window. This assumption often fails in
industrial systems, where dynamics are driven by control actions, are highly
non-stationary, and can experience long, uninterrupted gaps. We propose STDiff,
which reframes imputation as learning how the system evolves from one state to
the next. STDiff uses a conditional denoising diffusion model with a causal
bias aligned to control theory, generating missing values step-by-step based on
the most recent known state and relevant control or environmental inputs. On a
public wastewater treatment dataset with simulated missing blocks, STDiff
consistently achieves the lowest errors, with its advantage increasing for
longer gaps. On a raw industrial dataset with substantial real gaps, it
produces trajectories that remain dynamically plausible, in contrast to
window-based models that tend to flatten or over-smooth. These results support
dynamics-aware, explicitly conditioned imputation as a robust approach for
industrial time series, and we discuss computational trade-offs and extensions
to broader domains.

</details>


### [172] [Learning with springs and sticks](https://arxiv.org/abs/2508.19015)
*Luis Mantilla Calderón,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 提出一种以弹簧和棍子为基础的物理系统模型，用于理解学习过程中的能量和热力学性质，展示其在回归任务中的表现并探讨学习的热力学障碍。


<details>
  <summary>Details</summary>
Motivation: 旨在从物理系统的角度理解学习机制，探索能量变化与学习能力之间的关系。

Method: 构建由弹簧和棍子组成的动力学系统，通过调节潜在能量模拟误差损失，利用耗散实现能量最小化。

Result: 系统在回归任务中表现与多层感知机相当，发现系统的自由能变化与学习能力相关，并提出热力学学习障碍的概念。

Conclusion: 该模型为从物理角度理解学习系统提供了新视角，有助于理解能量和热力学在学习中的作用。

Abstract: Learning is a physical process. Here, we aim to study a simple dynamical
system composed of springs and sticks capable of arbitrarily approximating any
continuous function. The main idea of our work is to use the sticks to mimic a
piecewise-linear approximation of the given function, use the potential energy
of springs to encode a desired mean squared error loss function, and converge
to a minimum-energy configuration via dissipation. We apply the proposed
simulation system to regression tasks and show that its performance is
comparable to that of multi-layer perceptrons. In addition, we study the
thermodynamic properties of the system and find a relation between the free
energy change of the system and its ability to learn an underlying data
distribution. We empirically find a \emph{thermodynamic learning barrier} for
the system caused by the fluctuations of the environment, whereby the system
cannot learn if its change in free energy hits such a barrier. We believe this
simple model can help us better understand learning systems from a physical
point of view.

</details>


### [173] [Working My Way Back to You: Resource-Centric Next-Activity Prediction](https://arxiv.org/abs/2508.19016)
*Kelly Kurowski,Xixi Lu,Hajo A Reijers*

Main category: cs.LG

TL;DR: 资源中心方法在预测下一活动中展现潜力，优于传统控制流视角，助力任务优化和资源管理。


<details>
  <summary>Details</summary>
Motivation: 探索资源信息在下一活动预测中的潜在作用，补充控制流视角的研究局限。

Method: 评估四种模型和三种编码策略，使用四个实际数据集，比较不同编码和模型的表现。

Result: LightGBM和Transformer在2-gram编码下表现最佳，随机森林结合2-gram和重复特征效果最好，平均准确率最高。

Conclusion: 资源中心的预测方法具有提升资源配置和个性化支持的潜力，为PPM领域开辟新研究方向。

Abstract: Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.

</details>


### [174] [Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence](https://arxiv.org/abs/2508.19019)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 提出一种基于注意力自动编码器的主动学习异常检测框架，利用相似度搜索提升APT检测的效率与准确性，强调不同相似度指标的影响。


<details>
  <summary>Details</summary>
Motivation: APT检测面临隐蔽性强和数据类别严重不平衡的挑战。

Method: 结合注意力自动编码器和相似度搜索，通过主动学习迭代优化决策空间，正式评估多种相似度指标对检测效果的影响。

Result: 在多样化数据集上验证，发现相似度选择对模型收敛、检测准确率和标记效率具有显著影响，并提供了相关实践指导。

Conclusion: 合理选择相似度函数在APT检测的主动学习中至关重要，可显著提升检测性能与效率。

Abstract: Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense
due to their stealthy behavior and the extreme class imbalance inherent in
detection datasets. To address these issues, we propose a novel active
learning-based anomaly detection framework that leverages similarity search to
iteratively refine the decision space. Built upon an Attention-Based
Autoencoder, our approach uses feature-space similarity to identify normal-like
and anomaly-like instances, thereby enhancing model robustness with minimal
oracle supervision. Crucially, we perform a formal evaluation of various
similarity measures to understand their influence on sample selection and
anomaly ranking effectiveness. Through experiments on diverse datasets,
including DARPA Transparent Computing APT traces, we demonstrate that the
choice of similarity metric significantly impacts model convergence, anomaly
detection accuracy, and label efficiency. Our results offer actionable insights
for selecting similarity functions in active learning pipelines tailored for
threat intelligence and cyber defense.

</details>


### [175] [GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling](https://arxiv.org/abs/2508.19028)
*Arash Jamshidi,Lauri Seppäläinen,Katsiaryna Haitsiukevich,Hoang Phuc Hau Luu,Anton Björklund,Kai Puolamäki*

Main category: cs.LG

TL;DR: 提出了一种基于梯度信息的早停方法GradStop，避免使用验证集，适用于数据有限的场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统早停方法依赖验证集的问题，尤其在数据有限时效果不足。

Method: 通过估计贝叶斯后验，利用梯度信息定义早停问题，从而获得停止准则。

Result: GradStop在测试性能上优于基于验证集的早停方法，适合数据有限环境，且算法计算成本低。

Conclusion: 利用梯度信息进行贝叶斯后验估计，为早停提供有效替代方案，增强模型在数据有限时的表现。

Abstract: Machine learning models are often learned by minimising a loss function on
the training data using a gradient descent algorithm. These models often suffer
from overfitting, leading to a decline in predictive performance on unseen
data. A standard solution is early stopping using a hold-out validation set,
which halts the minimisation when the validation loss stops decreasing.
However, this hold-out set reduces the data available for training. This paper
presents {\sc gradstop}, a novel stochastic early stopping method that only
uses information in the gradients, which are produced by the gradient descent
algorithm ``for free.'' Our main contributions are that we estimate the
Bayesian posterior by the gradient information, define the early stopping
problem as drawing sample from this posterior, and use the approximated
posterior to obtain a stopping criterion. Our empirical evaluation shows that
{\sc gradstop} achieves a small loss on test data and compares favourably to a
validation-set-based stopping criterion. By leveraging the entire dataset for
training, our method is particularly advantageous in data-limited settings,
such as transfer learning. It can be incorporated as an optional feature in
gradient descent libraries with only a small computational overhead. The source
code is available at https://github.com/edahelsinki/gradstop.

</details>


### [176] [When recalling in-context, Transformers are not SSMs](https://arxiv.org/abs/2508.19029)
*Destiny Okpekpe,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文研究了循环深度学习模型在关联回忆任务中的表现，发现学习率对模型性能影响关键，提出了网络扩展的不同优势，并分析了单层变换器的训练动态及架构设计对性能和稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨深度循环模型在推理和记忆任务中的局限性与表现，特别是与变换器的对比，旨在提升模型训练和架构设计。

Method: 通过实验分析不同模型在关联回忆任务中的表现，研究扩展方式的影响，检测单层变换器的训练动态，以及进行架构改动的效果。

Result: 发现学习率对循环模型性能关键，扩展宽度与深度的优势不同，单层变换器训练动态类似多层，架构优化改善模型性能与稳定性。

Conclusion: 深度循环模型在记忆任务中存在挑战，优化训练策略和架构设计是提升其性能的关键，未来研究需关注训练稳定性和模型扩展策略。

Abstract: Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba's performance
and optimization stability.

</details>


### [177] [Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data](https://arxiv.org/abs/2508.19031)
*Vemula Sreenath,Filippo Gatti,Pierre Jehel*

Main category: cs.LG

TL;DR: 开发了一种透明且具有解释性的新型机器学习模型，用于地震动模拟，解决数据偏差和模型黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在地震动预测中具有局限性，主要表现为缺乏解释性和数据不平衡问题。

Method: 提出HazBinLoss函数和独立处理输入特征的结构，确保模型的透明性和合理性，同时通过加权策略关注关键的近场大震数据。

Result: 模型在保持可解释性的同时，与传统GMM性能相当，有助于推广ML在地震风险评估中的应用。

Conclusion: 该方法有效克服了ML模型的黑箱和偏差问题，为地震动预测提供了更具信任性和实用性的工具。

Abstract: Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.

</details>


### [178] [Automated discovery of finite volume schemes using Graph Neural Networks](https://arxiv.org/abs/2508.19052)
*Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: GNNs不仅能在训练中学习传统数值方案，还能通过符号回归和无监督学习，自动发现更高阶的数值方法，展现出在科学计算中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索GNN在泛化和创新数值方案中的能力，突破传统局限。

Method: 结合监督与无监督训练，通过符号回归发现数值方案，扩展到高阶方法。

Result: GNN成功泛化至不同结构的网格，自动发现一阶、二阶数值方案，验证其在科学计算中的应用潜力。

Conclusion: GNN不仅是强大的近似工具，还可作为创新数值方法的发现者，推动科学计算新范式。

Abstract: Graph Neural Networks (GNNs) have deeply modified the landscape of numerical
simulations by demonstrating strong capabilities in approximating solutions of
physical systems. However, their ability to extrapolate beyond their training
domain (\textit{e.g.} larger or structurally different graphs) remains
uncertain. In this work, we establish that GNNs can serve purposes beyond their
traditional role, and be exploited to generate numerical schemes, in
conjunction with symbolic regression. First, we show numerically and
theoretically that a GNN trained on a dataset consisting solely of two-node
graphs can extrapolate a first-order Finite Volume (FV) scheme for the heat
equation on out-of-distribution, unstructured meshes. Specifically, if a GNN
achieves a loss $\varepsilon$ on such a dataset, it implements the FV scheme
with an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we show
that the network effectively rediscovers the exact analytical formulation of
the standard first-order FV scheme. We then extend this approach to an
unsupervised context: the GNN recovers the first-order FV scheme using only a
residual loss similar to Physics-Informed Neural Networks (PINNs) with no
access to ground-truth data. Finally, we push the methodology further by
considering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNN
using the same PINN loss, that autonomously discover (i) a second-order
correction term to the initial scheme using a 2-hop stencil, and (ii) the
classic second-order midpoint scheme. These findings follows a recent paradigm
in scientific computing: GNNs are not only strong approximators, but can be
active contributors to the development of novel numerical methods.

</details>


### [179] [Tackling Federated Unlearning as a Parameter Estimation Problem](https://arxiv.org/abs/2508.19065)
*Antonio Balordi,Lorenzo Manini,Fabio Stella,Alessio Merlo*

Main category: cs.LG

TL;DR: 提出一种基于信息论的联邦学习遗忘框架，通过二阶Hessian信息有效选择参数，实现隐私保护和模型性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 应对联邦学习中的数据隐私保护需求，确保模型在数据删除后仍具备良好性能。

Method: 利用二阶Hessian信息将遗忘问题建模为参数估计，通过有选择性地重置敏感参数并进行最小化联邦再训练。

Result: 该方法在多个数据集上实现强隐私保护（MIA成功率接近随机）和高性能（准确率约为重新训练的90%），同时提升效率并能抵抗后门攻击。

Conclusion: 该框架提供了一种实用的联邦学习数据遗忘方案，兼顾隐私保护、模型性能和效率，具有实际应用潜力。

Abstract: Privacy regulations require the erasure of data from deep learning models.
This is a significant challenge that is amplified in Federated Learning, where
data remains on clients, making full retraining or coordinated updates often
infeasible. This work introduces an efficient Federated Unlearning framework
based on information theory, modeling leakage as a parameter estimation
problem. Our method uses second-order Hessian information to identify and
selectively reset only the parameters most sensitive to the data being
forgotten, followed by minimal federated retraining. This model-agnostic
approach supports categorical and client unlearning without requiring server
access to raw client data after initial information aggregation. Evaluations on
benchmark datasets demonstrate strong privacy (MIA success near random,
categorical knowledge erased) and high performance (Normalized Accuracy against
re-trained benchmarks of $\approx$ 0.9), while aiming for increased efficiency
over complete retraining. Furthermore, in a targeted backdoor attack scenario,
our framework effectively neutralizes the malicious trigger, restoring model
integrity. This offers a practical solution for data forgetting in FL.

</details>


### [180] [Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks](https://arxiv.org/abs/2508.19071)
*Hugo Attali,Thomas Papastergiou,Nathalie Pernelle,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: TRIGON通过学习多视图中的三角形，改造图结构，提升图神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GNNs中的信息传播受限问题（如过度缩小和过度平滑）

Method: 引入TRIGON框架，通过学习选择相关三角形，优化图的拓扑结构以改善性能

Result: 在节点分类任务中优于现有方法，提升图的结构性质如直径、谱间隙和有效电阻

Conclusion: TRIGON有效改进图结构，增强GNN的表达能力

Abstract: Graph Neural Networks (GNNs) have emerged as the leading paradigm for
learning over graph-structured data. However, their performance is limited by
issues inherent to graph topology, most notably oversquashing and
oversmoothing. Recent advances in graph rewiring aim to mitigate these
limitations by modifying the graph topology to promote more effective
information propagation. In this work, we introduce TRIGON, a novel framework
that constructs enriched, non-planar triangulations by learning to select
relevant triangles from multiple graph views. By jointly optimizing triangle
selection and downstream classification performance, our method produces a
rewired graph with markedly improved structural properties such as reduced
diameter, increased spectral gap, and lower effective resistance compared to
existing rewiring methods. Empirical results demonstrate that TRIGON
outperforms state-of-the-art approaches on node classification tasks across a
range of homophilic and heterophilic benchmarks.

</details>


### [181] [APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration](https://arxiv.org/abs/2508.19087)
*Shaobo Ma,Chao Fang,Haikuo Shao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出APT-LLM，通过新数据格式和矩阵乘法方法实现任意精度LLMs加速，有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决极低位量化LLMs在GPU上的效率瓶颈。

Method: 引入bipolar-INT格式，设计bit-level矩阵操作，改进内存管理和核函数映射。

Result: 在多平台上实现显著速度提升，最高达3.99倍。

Conclusion: APT-LLM有效提升低比特量化LLMs的GPU性能，推动其实际应用。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
enormous computational demands severely limit deployment and real-time
performance. Quantization methods can help reduce computational costs, however,
attaining the extreme efficiency associated with ultra-low-bit quantized LLMs
at arbitrary precision presents challenges on GPUs. This is primarily due to
the limited support for GPU Tensor Cores, inefficient memory management, and
inflexible kernel optimizations. To tackle these challenges, we propose a
comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.
Firstly, we introduce a novel data format, bipolar-INT, which allows for
efficient and lossless conversion with signed INT, while also being more
conducive to parallel computation. We also develop a matrix multiplication
(MatMul) method allowing for arbitrary precision by dismantling and
reassembling matrices at the bit level. This method provides flexible precision
and optimizes the utilization of GPU Tensor Cores. In addition, we propose a
memory management system focused on data recovery, which strategically employs
fast shared memory to substantially increase kernel execution speed and reduce
memory access latency. Finally, we develop a kernel mapping method that
dynamically selects the optimal configurable hyperparameters of kernels for
varying matrix sizes, enabling optimal performance across different LLM
architectures and precision settings. In LLM inference, APT-LLM achieves up to
a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup
over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,
APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup
over CUTLASS integer baselines.

</details>


### [182] [Composition and Alignment of Diffusion Models using Constrained Learning](https://arxiv.org/abs/2508.19104)
*Shervin Khalafi,Ignacio Hounie,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出了一种统一约束优化框架，用于改进扩散模型在样本生成中的属性控制，解决多奖励、多模型优化中的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散模型优化方法在多属性控制方面存在权衡，无法保证同时满足所有属性的需求。

Method: 通过引入约束优化框架，结合拉格朗日对偶训练算法，实现模型的对齐与组合，同时满足奖励约束和模型接近性。

Result: 在图像生成任务中验证了方法的有效性，模型在满足约束方面优于传统加权方法，提升了属性控制的效果。

Conclusion: 该方法提供了一种理论和实践结合的工具，有助于实现更精细的多属性生成控制。

Abstract: Diffusion models have become prevalent in generative modeling due to their
ability to sample from complex distributions. To improve the quality of
generated samples and their compliance with user requirements, two commonly
used methods are: (i) Alignment, which involves fine-tuning a diffusion model
to align it with a reward; and (ii) Composition, which combines several
pre-trained diffusion models, each emphasizing a desirable attribute in the
generated outputs. However, trade-offs often arise when optimizing for multiple
rewards or combining multiple models, as they can often represent competing
properties. Existing methods cannot guarantee that the resulting model
faithfully generates samples with all the desired properties. To address this
gap, we propose a constrained optimization framework that unifies alignment and
composition of diffusion models by enforcing that the aligned model satisfies
reward constraints and/or remains close to (potentially multiple) pre-trained
models. We provide a theoretical characterization of the solutions to the
constrained alignment and composition problems and develop a Lagrangian-based
primal-dual training algorithm to approximate these solutions. Empirically, we
demonstrate the effectiveness and merits of our proposed approach in image
generation, applying it to alignment and composition, and show that our aligned
or composed model satisfies constraints effectively, and improves on the
equally-weighted approach. Our implementation can be found at
https://github.com/shervinkhalafi/constrained_comp_align.

</details>


### [183] [Active Query Selection for Crowd-Based Reinforcement Learning](https://arxiv.org/abs/2508.19132)
*Jonathan Erskine,Taku Yamagata,Raúl Santos-Rodríguez*

Main category: cs.LG

TL;DR: 提出结合众包概率模型与主动学习的偏好增强学习框架，有效提升学习效率，尤其在反馈有限或成本高的环境中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决偏好增强学习中人类反馈成本高和可靠性低的问题，提升学习效率和效果。

Method: 扩展Advise算法，支持多训练者、估计其可靠性，并采用熵值驱动的主动反馈请求策略，在多个环境中验证。

Result: 在合成和真实环境中，训练的代理在不确定轨迹上表现出更快的学习速度，血糖控制任务优于基线模型。

Conclusion: 结合概率众包模型与主动学习的偏好增强学习框架有效改善样本效率和性能，在多样环境中具有潜力。

Abstract: Preference-based reinforcement learning has gained prominence as a strategy
for training agents in environments where the reward signal is difficult to
specify or misaligned with human intent. However, its effectiveness is often
limited by the high cost and low availability of reliable human input,
especially in domains where expert feedback is scarce or errors are costly. To
address this, we propose a novel framework that combines two complementary
strategies: probabilistic crowd modelling to handle noisy, multi-annotator
feedback, and active learning to prioritize feedback on the most informative
agent actions. We extend the Advise algorithm to support multiple trainers,
estimate their reliability online, and incorporate entropy-based query
selection to guide feedback requests. We evaluate our approach in a set of
environments that span both synthetic and real-world-inspired settings,
including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control task
for Type 1 Diabetes using the clinically approved UVA/Padova simulator. Our
preliminary results demonstrate that agents trained with feedback on uncertain
trajectories exhibit faster learning in most tasks, and we outperform the
baselines for the blood glucose control task.

</details>


### [184] [Saddle Hierarchy in Dense Associative Memory](https://arxiv.org/abs/2508.19151)
*Robin Thériault,Daniele Tantari*

Main category: cs.LG

TL;DR: 本文提出了一种基于玻尔兹曼机的密集联想记忆模型（DAM），通过统计力学分析优化训练过程，提高模型的稳定性和解释性，并开发网络扩展算法以降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 研究DAM在鲁棒性和与现代机器学习机制的关系，以及改善其训练稳定性和效率。

Method: 利用统计力学分析推动理论推导，提出新正则化方案，并实施网络生长算法。

Result: 模型在解释性和稳定性方面表现优异，训练成本显著降低，揭示了小模型与大模型的鞍点关系。

Conclusion: 该研究丰富了对DAM的理解，展示了其在分类任务中的潜力，且提供了提高训练效率的方法。

Abstract: Dense associative memory (DAM) models have been attracting renewed attention
since they were shown to be robust to adversarial examples and closely related
to state-of-the-art machine learning paradigms, such as the attention
mechanisms in transformers and generative diffusion models. We study a DAM
built upon a three-layer Boltzmann machine with Potts hidden units, which
represent data clusters and classes. Through a statistical mechanics analysis,
we derive saddle-point equations that characterize both the stationary points
of DAMs trained on real data and the fixed points of DAMs trained on synthetic
data within a teacher-student framework. Based on these results, we propose a
novel regularization scheme that makes training significantly more stable.
Moreover, we show empirically that our DAM learns interpretable solutions to
both supervised and unsupervised classification problems. Pushing our
theoretical analysis further, we find that the weights learned by relatively
small DAMs correspond to unstable saddle points in larger DAMs. We implement a
network-growing algorithm that leverages this saddle-point hierarchy to
drastically reduce the computational cost of training dense associative memory.

</details>


### [185] [Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness](https://arxiv.org/abs/2508.19183)
*Wenchuan Mu,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文提出一种名为塔鲁健性的创新评估指标，通过假设检验定量评估模型的概率鲁棒性，以改善安全关键深度学习应用中的鲁棒性评估效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒性评估方法在计算成本与测量精度之间存在权衡，限制了其实际应用。

Method: 作者对现有的鲁棒性定义和评估方法进行了比较分析，并提出以假设检验基础的塔鲁鲁棒性指标，以实现更高效的评估。

Result: 实验证明，该方法具有优势，并适用于安全关键场景，推动模型鲁棒性评估的系统理解与提升。

Conclusion: 提出的塔鲁鲁棒性指标为深度学习模型的鲁棒性评估提供了一个更合理、更实用的工具，有助于在安全关键应用中更可靠地部署模型。

Abstract: In safety-critical deep learning applications, robustness measures the
ability of neural models that handle imperceptible perturbations in input data,
which may lead to potential safety hazards. Existing pre-deployment robustness
assessment methods typically suffer from significant trade-offs between
computational cost and measurement precision, limiting their practical utility.
To address these limitations, this paper conducts a comprehensive comparative
analysis of existing robustness definitions and associated assessment
methodologies. We propose tower robustness to evaluate robustness, which is a
novel, practical metric based on hypothesis testing to quantitatively evaluate
probabilistic robustness, enabling more rigorous and efficient pre-deployment
assessments. Our extensive comparative evaluation illustrates the advantages
and applicability of our proposed approach, thereby advancing the systematic
understanding and enhancement of model robustness in safety-critical deep
learning applications.

</details>


### [186] [Emotions as Ambiguity-aware Ordinal Representations](https://arxiv.org/abs/2508.19193)
*Jingyao Wu,Matthew Barthet,David Melhart,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 提出一种新颖的情感连续识别框架，利用情感的模糊性和时间动态特征，通过定义情绪的序数表示，改善对情感变化的捕捉效果。


<details>
  <summary>Details</summary>
Motivation: 弥补现有连续情感识别方法未充分考虑情感模糊性和动态变化的不足，提供更准确的情感动态建模。

Method: 引入基于情感变化速率的模糊性序数表示框架，结合RECOLA和GameVibe两个数据集进行实验，包括有界（激动、价值得分）与无界（参与度）连续情感轨迹的建模。

Result: 序数表示在无界标签上优于传统模型，获得最高的CCC和SDA分数，在有界轨迹中表现出色，特别是在捕捉情感相对变化方面展现优势。

Conclusion: 模糊性序数表示能有效建模情绪的动态变化，提升连续情感识别的性能，尤其是在无界标签的情感轨迹分析中具有显著优势。

Abstract: Emotions are inherently ambiguous and dynamic phenomena, yet existing
continuous emotion recognition approaches either ignore their ambiguity or
treat ambiguity as an independent and static variable over time. Motivated by
this gap in the literature, in this paper we introduce \emph{ambiguity-aware
ordinal} emotion representations, a novel framework that captures both the
ambiguity present in emotion annotation and the inherent temporal dynamics of
emotional traces. Specifically, we propose approaches that model emotion
ambiguity through its rate of change. We evaluate our framework on two
affective corpora -- RECOLA and GameVibe -- testing our proposed approaches on
both bounded (arousal, valence) and unbounded (engagement) continuous traces.
Our results demonstrate that ordinal representations outperform conventional
ambiguity-aware models on unbounded labels, achieving the highest Concordance
Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,
highlighting their effectiveness in modeling the traces' dynamics. For bounded
traces, ordinal representations excel in SDA, revealing their superior ability
to capture relative changes of annotated emotion traces.

</details>


### [187] [Understanding Tool-Integrated Reasoning](https://arxiv.org/abs/2508.19201)
*Heng Lin,Zhongwen Xu*

Main category: cs.LG

TL;DR: TIR显著增强大模型推理能力的理论基础，通过工具扩展模型的支持能力，采用ASPO优化策略，实现模型更复杂的问题解决能力。这一研究首次提供了TIR效果的正式理论说明。


<details>
  <summary>Details</summary>
Motivation: 探究为何工具集成推理（TIR）能显著提升大语言模型（LLMs）的能力，从而弥补纯文本模型的局限。

Method: 采用形式化证明展示工具如何扩展模型能力，提出ASPO算法优化模型行为，通过在数学基准测试中的实验验证其有效性。

Result: TIR模型在数学问题上的表现优于纯文本模型，尤其在需要抽象洞察的任务中表现明显，模型行为更加高效和互动。

Conclusion: TIR通过扩展模型的实用和能力边界，从理论和实践两个层面确立了其在复杂推理中的优势，为未来模型设计提供理论支持。

Abstract: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.

</details>


### [188] [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: 提出了一种名为Token Order Prediction（TOP）的新训练目标，用于改善语言模型的下一个词预测任务。


<details>
  <summary>Details</summary>
Motivation: 现有的多词预测（MTP）作为辅助目标在标准NLP基准上表现不佳，寻找更有效的训练辅助目标成为研究动机。

Method: TOP通过学习排序未来tokens的接近程度，实现对tokens的排序预测，训练过程中只增加一个unembedding层，比MTP更加高效。

Result: 在多种规模模型及八个NLP基准测试中，TOP优于传统的NTP和MTP，效果明显。

Conclusion: TOP是一种有效的辅助训练目标，能显著提升语言模型的性能。

Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to
improve next-token prediction (NTP) in language model training but shows
inconsistent improvements, underperforming in standard NLP benchmarks. We argue
that MTP's exact future token prediction is too difficult as an auxiliary loss.
Instead, we propose Token Order Prediction (TOP), which trains models to order
upcoming tokens by their proximity using a learning-to-rank loss. TOP requires
only a single additional unembedding layer compared to MTP's multiple
transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using
NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show
that TOP overall outperforms both NTP and MTP even at scale. Our code is
available at https://github.com/zaydzuhri/token-order-prediction

</details>
