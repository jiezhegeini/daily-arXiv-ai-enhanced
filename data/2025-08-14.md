<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 94]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: 提出ParallelSearch框架，通过识别并行查询结构，实现多任务并行处理，有效提升多项问答任务性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有搜索代理在处理可并行查询时的序列处理瓶颈，提升计算效率。

Method: 引入奖励机制，激励模型识别独立查询组件，实现多搜索操作的并行执行。

Result: 在多项问答基准测试中，性能提升2.9%，在可并行问题上达12.7%的性能改善，并减少约30%的调用次数。

Conclusion: ParallelSearch有效突破了序列处理限制，展现出在多任务并行搜索中优异的性能和效率。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 本研究探讨了GPT-4o在稀有疾病命名实体识别中的应用，利用多种提示策略及知识编码，实现了在低资源条件下的优异表现。


<details>
  <summary>Details</summary>
Motivation: 在稀有疾病领域，有限的标注数据、实体类型的语义模糊和长尾分布带来挑战，亟需有效的低资源NER解决方案。

Method: 设计结构化提示框架，结合知识编码与二元歧义规则，采用少样本例子选择和任务微调策略，比较不同方法在RareDis语料库上的表现。

Result: GPT-4o在多种提示策略下表现优越，任务微调达到SOTA，少样本提示性价比高，RAG略有提升，错误分析指出改进方向。

Conclusion: 提示优化的大型语言模型可作为生物医学NER的有效替代，特别适合标注数据匮乏的稀有疾病场景。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: TEN结合神经符号方法，通过结构化提示生成初始表格，并用符号检查与自我调试机制提升文本到表格的转换准确性，比纯神经方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化或半结构化文本中表格提取的挑战，尤其是在没有一致分隔符的情况下。

Method: 采用结构化分解提示，结合大模型生成初始表，利用符号检测确保符合规则，并通过自我调节提升表格质量。

Result: 显著超越纯神经方法，在多个数据集上提高准确性，降低幻觉，用户评价更高。

Conclusion: 神经符号融合策略有效改善文本到表格的转换，有潜力应用于复杂文本数据解析。

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [4] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 提出一种无需 neuroimaging即可将文本中的情感内容映射到脑区的计算框架，结合大数据分析情感和脑区关系。


<details>
  <summary>Details</summary>
Motivation: 弥补传统神经影像成本高、受限于实验室的不足，利用数字文本数据探索情感与脑功能的关系。

Method: 使用OpenAI的文本嵌入模型生成语义表示，通过降维和聚类识别情感类别，映射到18个与情感相关的脑区。进行对比分析，验证不同数据来源的情感映射效果。

Result: 映射结果具有良好的空间特异性，抑郁患者表现出更强的边缘系统激活，成功区分不同情感。大型语言模型（LLM）生成的文本在情感分布上与人类类似，但在共情与自我指涉区域表现不足。

Conclusion: 该方法低成本、可扩展，可分析自然语言中的情感差异，为临床和人工智能情感表达提供脑部基准。

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [5] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 该研究提出了一种人机混合德尔菲（HAH-Delphi）框架，利用生成式AI和专家团队共同制定高质量的专家共识，适用于健康、教练和表现科学等领域。


<details>
  <summary>Details</summary>
Motivation: 解决传统专家共识方法面对信息爆炸、证据碎片化和缺乏专家筛选带来的挑战，提升共识制定的效率和质量。

Method: 结合生成式AI模型Gemini 2.5 Pro、少量高水平人类专家和结构化引导，通过回溯复制、前瞻比较和实际应用三阶段进行测试与验证。

Result: AI在复现专家结论和方向一致性方面表现优异，专家团队达成>90%的共识率，AI支持促进理解与多样化辩论。

Conclusion: HAH-Delphi是一种灵活、可扩展的共识生成工具，已在多个领域验证其方法有效性，能为生成个性化和条件化的指南提供基础。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [6] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 提出了一种结合语义标记和连续声学特征的无文本语音模型，通过预测多重语义标记以增强语义信息的保持，并在保持语义一致性的同时改善声学细节。


<details>
  <summary>Details</summary>
Motivation: 现有无文本语音模型缺乏对声学细节的控制，无法结合语义与声学信息以改善语音生成质量。

Method: 使用流匹配目标预测条件语义标记和连续声学向量，并设计多语义标记预测策略来增强语义信息的保持。

Result: 新方法在语义保真度和声学细节方面表现优异，性能媲美现有模型。

Conclusion: 将语义与声学信息联合建模的策略有效改善了无文本语音生成的质量，兼顾语义准确性与声学表现。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [7] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: 提出了一种新颖的自动提示优化方法APIO，用于语法错误更正和文本简化任务，无需手动种子提示，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，如何高效设计提示以提升任务性能成为研究重点。现有的提示优化方法多依赖手动提示或特定优化策略，存在一定限制。

Method: 提出APIO方法，通过提示归纳与优化自动提升模型表现，适用于语法错误更正和文本简化任务，并不依赖手动设计的种子提示。

Result: APIO在这些任务中实现了最先进的性能，优于现有的基于提示的方法，验证其有效性。

Conclusion: APIO是一种简单而有效的提示自动归纳与优化手段，为任务性能提升提供了新途径，推动了LLMs在实际应用中的表现提升。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [8] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 提出了一种基于大型语言模型的扩展简称的解决方案Columbo，显著优于现有方法，并在实际数据门户中应用。


<details>
  <summary>Details</summary>
Motivation: 扩展表格中的缩写词对许多数据任务至关重要，但现有方法存在局限性，需要更准确有效的解决方案。

Method: 利用上下文、规则、链式思考和字符级分析的方法，开发了Columbo模型.

Result: Columbo在多个数据集上超越现有最优方案NameGuess 4-29%，并已应用于环境科学的数据门户中。

Conclusion: 提出的Columbo模型及评估指标显著推动了缩写扩展技术的发展，加强了实际应用的效果。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 本文提出采用Zipformer模型解决儿童双语环境中的语码转换和语言识别问题，通过提取内部层的语音特征，有效提升识别准确率，展现了Transformer架构在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决双语儿童语音中存在的语码转换和语言识别困难，提升识别准确率。

Method: 利用Zipformer模型的内部层提取嵌入特征，并与不同后端进行比较，增强模型对不平衡语料的处理能力。

Result: 实现了81.89%的平衡准确率，比基线提升了15.47%，验证了Zipformer在多语言识别中的有效性和鲁棒性。

Conclusion: Zipformer作为Transformer编码器架构在处理不平衡、多语种语音识别任务中表现出良好的潜力，适用于实际多语环境。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 大规模评估表格自动生成文本中的地缘经济偏见，发现现有模型在描述国家经济状况时存在偏向性，且现有去偏技术效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着大规模视觉-语言模型在图表总结中的应用增加，需关注其潜在偏见对社会的影响。

Method: 对6,000个图表-国家对比数据进行评估，通过不同模型分析偏见，并尝试基于提示的去偏技术。

Result: 模型倾向于为高收入国家生成更积极的描述，去偏技术效果有限。

Conclusion: 需发展更稳健的去偏策略，以减少模型偏见，避免社会负面影响。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 引入用户导向的主观排行榜（USL），通过偏好驱动的动态排名改善大模型的实用性评估，提出可定制奖励模型（CRM）以应对偏好多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的静态、客观评估无法满足用户个性化多样需求，亟需更具实用价值的模型评估方法。

Method: 分析超过1万条主观偏好数据，发现偏好多样性和矛盾性，开发参数仅4B的可定制奖励模型（CRM），以提升模型排序的动态适应性。

Result: USL结合CRM，能有效反映用户偏好，优于传统奖励模型，在新话题和标准上表现出优异的泛化能力。

Conclusion: 提出的USL和CRM为实用、个性化的大模型评估提供了创新工具，改善了偏好多样性带来的挑战。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [12] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 提出一种名为“主动阅读”的训练框架，通过自我引导学习策略提升大规模语言模型的知识学习与回忆能力，有助于制造更准确的事实性模型。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在存储和回忆知识方面存在不可靠性，缺乏工具确保模型可靠学习特定知识。

Method: 通过设计主动阅读框架，让模型自主研究资料并应用学习策略，提升知识吸收效率，并在专家领域和大规模预训练中验证效果。

Result: 在特定任务中显著优于普通微调，Meta WikiExpert-8B模型在事实性问答中超越同规模模型，展现了良好的知识学习能力。

Conclusion: 主动阅读框架有效增强模型知识学习与回忆能力，为打造更可靠的事实性大模型提供新途径。

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [13] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: DPS通过动态选择相关段落，提升复杂多跳推理的效果，优于传统固定Top-K方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中的reranking模块采用固定Top-K策略，难以处理复杂多跳查询，导致信息遗漏或噪音引入。

Method: 引入DPS作为一种监督学习框架，捕捉段落间依赖，动态选择最相关的段落，作为插件集成。

Result: 在五个基准测试中，DPS显著优于现有方法，特别在MuSiQue数据集上，F1分数提升超过30%。

Conclusion: DPS通过动态证据选择增强RAG的推理能力，有效解决多跳推理中的信息取舍难题。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [14] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 利用大规模语言模型生成高质量伪标注数据，提升跨语言方面情感分析性能，无需依赖翻译工具。


<details>
  <summary>Details</summary>
Motivation: 解决依赖不可靠翻译工具的问题，提升跨语言ABSA的效果。

Method: 训练ABSA模型预测目标语言数据，利用强大语言模型生成更自然的句子进行伪标注，再进行模型微调。

Result: 在六种语言和五个模型背景下优于以往方法，特别是微调的大模型表现更佳。

Conclusion: 新方法有效利用LLM提升跨语言ABSA，无需翻译工具，泛化能力强。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [15] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 提供了跨语言细粒度情感分析（ABSA）的全面综述，涵盖任务、数据集、方法及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 填补跨语言ABSA研究的空白，系统总结该领域的现状和挑战。

Method: 通过文献调研总结主要任务、数据集、模型和迁移方法，结合多语种和大模型的研究贡献。

Result: 系统梳理了跨语言ABSA的各类任务、方法及其研究现状，指出了主要挑战和未来研究方向。

Conclusion: 为推动跨语言ABSA发展提供全面参考，强调未来在多语种迁移和大模型应用方面的研究潜力。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [16] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 利用多个大模型进行事实核查声明检索，采用文本嵌入和余弦相似度，实现多语言系统。


<details>
  <summary>Details</summary>
Motivation: 旨在提升事实核查中的声明检索性能，尤其在多语言环境中。

Method: 结合多种尖端大模型生成文本嵌入，通过余弦相似度匹配相关声明，利用模型组合优化结果。

Result: 系统在单语任务中获得第七名，跨语任务中获第九名，显著优于单一模型。

Conclusion: 多模型结合和采用文本嵌入技术可有效提升事实核查中的声明检索性能，尤其是在多语言场景中。

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [17] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出一种可控的共情推理方法，通过结构化心理步骤和强化学习提升情感支持对话的同理心和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在情感支持对话中缺乏深层心理共情能力，亟需改善。

Method: 结合自然语言推理与结构化心理步骤，构建细粒度数据集，并采用包含过程与结果的强化学习策略进行训练，同时引入个性化对话重写与冗余奖励重加权以增强多样性。

Result: 显著提升模型的情感支持能力和人性化的共情表现，推动人类化支持系统的发展。

Conclusion: 结构化心理推理与强化学习技术有效增强对话模型的共情和多样性，具有广泛应用前景。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [18] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 提出一种仅依靠文本输出的黑盒模型成员推断攻击方法，利用模型对常见训练数据模式的记忆特性，实现对GPT-4等封闭模型的隐私检测。


<details>
  <summary>Details</summary>
Motivation: 解决现有成员推断攻击多依赖模型内部信息的问题，扩展到只能通过API访问的黑盒模型场景。

Method: 基于前缀生成多轮模型输出，利用n-gram重叠度量衡量输出与真实后缀的相似性以判断成员身份。

Result: 在多个基准测试中，该方法优于其他黑盒攻击，且性能与白盒攻击相当甚至更优，验证了其有效性。

Conclusion: 该攻击方式能有效检测模型记忆，且更具普适性，且新模型对成员推断表现出更强的抗攻击能力，表明隐私保护逐步增强。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [19] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: 提出了用于检测俄语科学摘要中AI生成内容的AINL-Eval 2025共享任务，构建了涵盖多学科和多模型的大规模数据集，并组织竞赛促进研究。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，AI生成文本难以区分，威胁学术诚信，特别是在多语种环境中，迫切需要有效检测工具。

Method: 建立包含人工和AI生成摘要的多学科大规模数据集，组织竞赛促进模型挑战，以增强模型在未见领域和模型上的泛化能力。

Result: 竞赛吸引10队168份提交，部分系统在识别AI内容方面表现出色，提供持续研究平台。

Conclusion: 该项目推动了AI与人类文本鉴别技术的发展，强化学术诚信保障，为未来多语种AI检测提供基础。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [20] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 通过调整解码温度影响语言模型的输出质量和多样性，提出采用Precision-Recall框架优化损失函数以改善模型性能的策略。


<details>
  <summary>Details</summary>
Motivation: 提升语言模型的多样性和覆盖率，同时保证输出质量，是当前研究的核心动机。

Method: 分析不同温度调整对模型多样性（Recall）和质量（Precision）的影响，利用Precision-Recall框架重新设计损失函数。

Result: 该方法在Precision与Recall之间实现了更优的平衡，优于传统的负对数似然训练结合温度调整的方法。

Conclusion: 通过重新思考损失函数设计，能够构建更具适应性和鲁棒性的语言模型，兼顾多样性和质量。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [21] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval是一种无需训练的高效评估方法，通过智能选择代表性样本，保持评估的代表性和公平性，同时提升效率，适用于多种数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展及评估基准的增加，传统评估方法面临巨大的计算挑战，需要寻找高效且可靠的评估方案。

Method: 提出基于模型效用指数（MUI）的样本子集选择方法，避免训练过程，强调样本的代表性、公平性和通用性。

Result: 在多个公开基准和不同LLM上验证，EffiEval在只采用少量数据的情况下，依然能保持与全数据评估的一致性，并能灵活调整规模。

Conclusion: EffiEval为大模型评估提供了一种实用、通用、高效的解决方案，兼顾评估的可靠性和效率。

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [22] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 提出SLowED，一种在知识蒸馏过程中保持小型语言模型安全性的技术，结合慢调节和低熵掩码，提高模型推理能力的同时确保安全性。


<details>
  <summary>Details</summary>
Motivation: 在推理能力提升的同时，关注模型安全性，避免训练过程中带来的潜在危害。

Method: 引入Slow Tuning通过缩小权重变化范围，和Low-Entropy Masking屏蔽低熵词，结合两者实现安全且有效的知识蒸馏。

Result: 在多种模型和基准测试中，SLowED既保持了模型安全性，又提升了推理能力，优于现有方法。

Conclusion: SLowED通过控制训练幅度和目标选择，成功实现了模型安全和推理性能的双重提升。

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [23] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 人工智能大型语言模型在印度法律任务中表现优异，但在专业法律研究方面仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能在法律行业中的应用潜力与局限性，特别是在印度语境下。

Method: 通过比较LLMs（如GPT、Claude、Llama）与初级律师和法学学生的表现，评估其法律任务执行情况。

Result: LLMs在 drafting 和 issue spotting方面表现优异，弱于法律研究，存在幻觉和事实错误。

Conclusion: LLMs能辅助部分法律任务，但复杂和细节问题仍需专业人类律师处理。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [24] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: VLMs在理解误导性可视化中表现出易受欺骗，强调需加强防护措施。


<details>
  <summary>Details</summary>
Motivation: 随着VLM在解读可视化中的应用增加，理解其抗误导能力变得重要。

Method: 分析10个模型，评估它们对8种误导性图表的反应，共处理16000余个响应。

Result: 大多数VLM会被误导，从而误解图表信息，即使数据未变。

Conclusion: 需要在VLM中加入保护措施，以防止视觉误信息扩散。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [25] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 介绍了GFPO方法，通过在训练中采样更大组并过滤响应，显著减缓大语言模型在推理任务中的长度膨胀问题，同时保持准确性，还引入了动态分配资源的Adaptive Difficulty GFPO以提升难题处理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在强化学习训练中为了提升准确率而产生的响应长度膨胀问题，减少无效“填充”内容，提高推理效率。

Method: 采用GFPO方法，通过大规模采样和响应过滤，根据长度和每标记奖励比进行筛选，同时引入Adaptive Difficulty策略，根据实时难度调节训练资源。

Result: GFPO在多个基准测试中将响应长度膨胀减少了46-71%，优化奖励比后减少至71-85%，在保持模型准确率的同时显著提升推理效率。

Conclusion: 通过在训练中增加计算资源，显著降低推理时的计算成本，实现高效且准确的推理表现，验证了训练时间投入与推理效率的正相关关系。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [26] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 提出了一种针对多跳问答的检索增强生成框架，通过问答分解和问答相似度检索提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多跳问答中查询模糊、不明确的问题，提高问答的准确性。

Method: 利用大语言模型将复杂问题分解为单跳子问题，生成可回答性问题进行文档检索，并通过问答嵌入进行相关文档匹配。

Result: 在多个多跳问答数据集上实验，显著提升了检索增强生成的性能。

Conclusion: 基于问答嵌入的检索策略和LLM问答分解方法有效改善多跳问答性能，具有实际应用价值。

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [27] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 提醒避免仅凭提示词判断模型偏见，应考虑提示中的支持性或反对性论证对模型输出的影响。


<details>
  <summary>Details</summary>
Motivation: 深入理解LLMs在政治偏见评估中的敏感性，以及提示内容对模型行为的影响，提升偏见检测的鲁棒性。

Method: 通过在单轮和多轮交互中加入支持和反驳的论据，观察模型输出的变化。

Result: 论据显著改变模型响应方向，模型表现出迎合性的趋向，且论据强度影响响应的一致性。

Conclusion: 模型会根据提示中的论证调整立场，影响偏见测量，应在偏见评估中考虑论证的作用，并寻找缓解策略。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [28] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: 提出了一种轻量级的微调方法UtterTune，用于多语言文本到语音模型，特别增强日语中的发音控制，同时保持自然度和说话人相似性。


<details>
  <summary>Details</summary>
Motivation: 解决多语言TTS模型中发音控制难题，提升特定语言发音的可控性。

Method: 采用低秩适应技术微调基于大规模语言模型的多语言TTS系统，支持对音素级别的发音和重音进行控制。

Result: 在日语语音合成中实现了良好的控制效果，主客观评价均证实其有效性，且保持了自然度和说话人相似性。

Conclusion: UtterTune有效提升了多语言TTS中目标语言的发音控制能力，验证了其在实际应用中的潜力。

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [29] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 但依靠大型语言模型自动生成的文本解释在提升模型性能方面表现与人工注释相当，显示出自动化解释的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决文本解释依赖昂贵人类标注的局限性，推动可扩展的自动化解释方法。

Method: 利用多种先进的大型语言模型生成文本解释，并用一系列自然语言生成指标评估其质量，分析其对模型性能的影响。

Result: 自动生成的文本解释在提升模型效果方面表现具有竞争力，验证了自动化生成在NLP中的应用潜力。

Conclusion: 自动化、基于大模型的文本解释具有重要应用价值，为扩展NLP数据集和提升模型性能提供了可行途径。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [30] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本论文调查了实践者在可解释自然语言处理中的经验，发现对现有方法的不满及评估挑战，强调需用户导向的框架。


<details>
  <summary>Details</summary>
Motivation: 随着模型复杂性增加，对其决策透明度的需求增长，但实践应用中的采纳效果尚未充分了解。

Method: 通过行业从业者和学术研究者的访谈，采用质性分析方法，比较他们的观点。

Result: 揭示了概念上的差距、现有方法的满意度低、以及评估困难。

Conclusion: 需明确的定义和以用户为中心的框架，促进可解释NLP的实际应用。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [31] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 提出BigCharts数据集及训练框架，提升图表理解能力，超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs难以有效理解多样化和真实世界的图表，数据集缺乏多样性和真实性，影响模型性能。

Method: 构建BigCharts数据集，结合监督微调与GRPO强化学习，设计专属奖励，用于提升模型推理能力。

Result: 所提出模型在多项图表问答基准中表现优异，优于多种大型模型。

Conclusion: 通过多样化数据集和结合强化学习策略，显著增强图表理解模型的鲁棒性和泛化能力。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [32] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文全面调查了用于训练AI心理健康助手的临床数据集，分析了现存数据的分类、缺陷与挑战，并提出未来发展的建议。


<details>
  <summary>Details</summary>
Motivation: 随着心理健康问题的增加，AI在诊断和干预中具有潜力，但依赖高质量数据集，当前数据集散乱、缺乏标准，限制了AI的发展。

Method: 对现有临床心理健康数据集进行分类、分析，并调查合成数据集，识别其缺陷与挑战。

Result: 发现缺乏纵向数据、文化与语言多样性不足、标准不统一，以及合成数据集的模态不足等问题。

Conclusion: 未来需统一标准，丰富数据多样性，解决现有不足，以推动更健壮公正的心理健康AI系统发展。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [33] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文系统综述了为克服变换器模型固有局限、提高大规模语言模型（LLMs）效率的创新架构，包括线性与稀疏序列建模、稀疏专家混合、混合架构及扩散模型等技术。


<details>
  <summary>Details</summary>
Motivation: 解决变换器模型在大规模训练和实际部署中的计算瓶颈，推动更高效的LLMs发展。

Method: 总结和分类近期的创新架构技术，从语言建模背景出发，涵盖不同模型和技术方案。

Result: 提供了现代高效LLMs架构的蓝图，系统整理了相关技术进展，促进未来AI系统的研究。

Conclusion: 希望该综述能激发未来在更高效率、多功能方向上的研究，推动基础模型的资源优化与普及。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [34] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: PRELUDE是一个用于评估长文本理解能力的基准，测试模型判断角色前传故事是否符合原著的一致性，展示模型在深层理解和推理方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以衡量模型对长篇连续故事的整体理解能力，特别是在涉及跨片段推理时。

Method: 引入PRELUDE基准，设计需要整合多部分信息的任务，比较不同模型的表现。

Result: 主流模型在任务中远落后于人类，推理能力尤其不足，显示出提升空间。

Conclusion: 长文本理解仍是挑战，需发展更强的推理和理解模型以缩小与人类之间的差距。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [35] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 轻量级Whisper模型在乌尔都语语音识别中表现出一定潜力，但仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语为全球第十大语言，但在自动语音识别系统中的应用有限。

Method: 在未经微调的情况下，使用不同规模的Whisper模型在乌尔都语数据集上进行评估，主要通过WER指标。

Result: Whisper-Small模型表现最佳，WER达33.68%；但在语音的音素准确性和词汇连贯性方面仍存在问题。

Conclusion: Whisper-Small具备实际应用潜力，但需要进一步研究以解决现存挑战，推动低资源语音识别技术的发展。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [36] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: Memory Decoder是一种无需调整原模型参数的领域适应技术，通过预训练记忆模块实现高效适应多种专业领域。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在特定领域适应中的高成本和灾难性遗忘问题。

Method: 引入Memory Decoder，利用小型变换器学习模仿外部非参数检索器，实现与预训练模型无缝结合。

Result: 在生物医学、金融和法律等领域显著提升模型性能，平均降低困惑度6.17点。

Conclusion: Memory Decoder提供了一种新颖、即插即用的领域适应机制，有望广泛应用于不同模型和领域中。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [37] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 本综述总结了近年来利用自然语言处理技术自动检测认知扭曲的研究，涵盖数据集、模型和评估方法，提出了未来研究的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着NLP在心理健康领域的应用增长，自动识别认知扭曲成为提升心理治疗效果的重要方向。

Method: 对过去20年的38项相关研究进行系统性回顾与分类，整理出统一的认知扭曲分类体系。

Result: 总结了现有的模型设计、评价策略及数据资源，发现领域存在Taxonomy不一致、任务定义差异等问题。

Conclusion: 提出建立统一的认知扭曲分类体系和标准化评估方法，以推动该领域的持续发展和研究一致性。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [38] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 利用计算文本分析检测金融、可持续和数字营销中的欺骗性语言，准确率超过99%，但多语种场景仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数字化商务沟通的普及，欺骗性话语的辨识变得尤为重要，同时面临多语种和数据不足的挑战。

Method: 结合经典修辞、传播心理学和语言理论，通过控制环境下的文本分析和变换器模型实现欺骗性语言检测。

Result: 检测准确率在控制环境中超过99%，多语种环境仍存在困难，主要因数据不足和基础设施缺乏。

Conclusion: 需要强大的自动文本识别系统，以弥合理论与实证的差距，促进AI discourse的真实交互。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [39] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 提出了一套多维度的LLMs对齐评估框架，用于系统比较不同对齐策略的优缺点。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛应用，确保其输出符合人类价值和安全标准变得至关重要，但缺乏统一的评估框架。

Method: 构建一套评估框架，从对齐检测、质量、效率和鲁棒性四个维度进行比较，涵盖多种对齐策略和模型。

Result: 该框架能有效揭示不同模型和策略的优缺点，助力未来研究。

Conclusion: 统一的多维度评估框架有助于推动LLMs对齐技术的持续改进和合理部署。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [40] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex是一种融合视觉与编码语言模型的多模态大模型，显著提升了多模态代码生成能力，采用模型合并技术，利用大规模多模态编码数据进行训练，并在新颖的InfiBench-V基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在代码生成方面能力有限，需要提升多模态理解和生成能力，满足更复杂的实际应用需求。

Method: 通过任务向量合并技术，将先进的编码模型融入强大的视觉-语言基础模型，结合大型多模态编码数据集进行训练。

Result: 在多个评测中达到领先水平，接近商用模型如GPT-4o，验证了模型合并策略和新数据集的有效性。

Conclusion: VisCodex有效提升了多模态代码生成能力，为多模态大模型的应用提供了新的解决方案和研究平台。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [41] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 医学领域的特定词汇表能显著提升放射学报告总结的性能和效率，优于通用和自然语言词汇表。


<details>
  <summary>Details</summary>
Motivation: 探讨词汇表类型对放射学报告自动总结性能的影响，填补此领域研究空白。

Method: 系统比较不同类型词汇表（通用、医学、领域专用）在无/有预训练条件下的表现，涵盖三种影像模态。

Result: 医学和领域专用词汇表在从零训练时表现优异，预训练减缓差异，且专用词汇表减少内存需求。

Conclusion: 定制化词汇表提升模型性能和效率，助力临床应用与研究。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [42] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 通过增加合理的事件上下文，改进情感分析的准确性与一致性。


<details>
  <summary>Details</summary>
Motivation: 情感分析存在歧义，可能源于缺乏事件的上下文信息。

Method: 自动生成多条情感条件下的事件链，结合故事生成技术，丰富事件描述的上下文。

Result: 丰富的上下文Narratives提高了情感理解的准确性和标注一致性。

Conclusion: 引入上下文的叙事策略能有效改善情感分析的表现，促进更可靠的情感注释。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [43] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: GPT-5在眼科问答中表现优异，合理配置能优化成本与准确性。


<details>
  <summary>Details</summary>
Motivation: 探究不同配置下GPT-5在复杂医学问答中的性能表现，以提升其实用性和效率。

Method: 比较12种GPT-5配置及其他模型，通过多项指标评估其准确率、理由质量和成本效率，采用多项统计模型进行排名。

Result: GPT-5-high在准确率和理由质量上表现最佳，成本效益较优配置早已达到Pareto最优，展示了推理努力对性能的影响。

Conclusion: 合理配置的GPT-5能在医学问答任务中实现高性能与成本平衡，为模型优化提供指导，建立了自动化评估框架。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [44] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 研究针对库尔德语Badini方言的语音识别，构建了语言模型，Wav2Vec2模型表现优于Whisper模型。


<details>
  <summary>Details</summary>
Motivation: 库尔德语资源匮乏，特定方言缺乏语音识别系统，影响社区信息技术融合和方言推广。

Method: 采集Badini儿童故事录音，进行清洗、切分和标记，用Wav2Vec2-Large-XLSR-53和Whisper-small训练模型，并比较性能。

Result: Wav2Vec2模型在准确率和可读性方面显著优于Whisper模型，分别达到82.67%、90.38%的准确率和65.45%、53.17%的可读性。

Conclusion: 为Badini方言建立的语音识别模型效果优良，可以推广应用，推动方言数字化发展。

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [45] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 提出一种基于神经网络的联机多阶段LLM选择优化方法，有效改善特定任务中的多阶段模型组合性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，亟需一种能够在多阶段任务中选择最优LLM序列的策略，以提升任务成功率和降低成本。

Method: 设计基于神经网络的上下文带宽算法，实时在线学习不同子任务的LLM成功率，指导多阶段LLM序列选择。

Result: 在通信问答和医疗诊断预测任务中表现优于其他LLM选择算法，有效提升多阶段任务的性能。

Conclusion: 结合神经网络的上下文带宽算法能有效应对多阶段复杂任务中的LLM选择难题，具有广泛应用潜力。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种在深度强化学习中利用以前任务的紧凑Q值作为知识基础的初始化方法，通过结合已知机制改善探索和学习效率，显著提升了连续控制任务的表现。


<details>
  <summary>Details</summary>
Motivation: 为了克服深度强化学习在迁移价值信息时的挑战，利用过去任务的Q值提升学习效率和稳定性。

Method: 引入DQInit方法，利用紧凑的表格Q值作为知识基础，采用已知度机制逐步融合到新任务中，避免固定衰减。

Result: 在多个连续控制任务中，DQInit显著提升了早期学习效率、稳定性和整体性能。

Conclusion: 基于值估计的迁移方法为深度强化学习中的知识迁移提供了新的途径，有效提升了迁移学习的效果。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [47] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 介绍了Othello AI竞技场，一个用于评估AI系统在未知环境中快速适应能力的基准平台，强调其在元学习和适应性方面的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估主要关注固定环境中的性能，忽视系统对环境微调的适应与泛化能力，这限制了对人工通用智能的理解和发展。

Method: 构建一个基于Othello游戏的多阶段平台，设计有限时间内分析和应对新环境的挑战，结合多维指标评估和实时可视化。

Result: 平台支持多样的游戏环境和变体，已有初步测试和学生参与，展示了多样的适应策略，从快速参数调优到模型学习等。

Conclusion: Othello AI竞技场提供了一个独特的教学和研究工具，促进对AI快速适应能力的理解和提升，契合人工通用智能的核心需求。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [48] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于大模型和多代理合作的自动多模态评估框架，用于增强移动智能助手的评估效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法面临高成本、标准不一和主观偏差等挑战。

Method: 采用三层代理架构，包括交互评估、语义验证和体验决策代理，通过微调Qwen3-8B模型实现。

Result: 在八个主要智能助手上验证，框架能有效预测用户满意度和识别生成缺陷。

Conclusion: 该框架提升了多模态AI助手的评估效率和一致性，有助于推动智能助手的发展。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [49] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 提出EvoCurr，通过动态调整难度逐步训练大型语言模型以增强复杂任务的解决能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中表现不佳，亟需有效训练策略。

Method: 利用专门的 curriculum-generation LLM 构建逐渐递增难度的训练实例，动态调整难度应对模型学习状态。

Result: 显著提升任务成功率和解决效率，优于直解基线。

Conclusion: 基于LMM驱动的课程学习具有提升高复杂度任务自动推理能力的潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [50] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 该研究提出了一种将SHAP值中的不确定性分解为本体不确定性、识别不确定性和纠缠不确定性的方法，增强模型解释的可靠性，强调数据质量和模型开发技术的重要性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，理解模型解释中的不确定性对确保决策的可靠性至关重要，但目前对SHAP值中不确定性的研究较少。

Method: 结合Dempster-Shafer证据理论和Dirichlet过程的假设采样，分解SHAP值的不确定性。

Result: 通过三个真实案例验证，发现SHAP值高的特征不一定稳定，且树模型（如bagging）有助于量化认知不确定性。

Conclusion: 理解SHAP值中的不确定性有助于提升模型的鲁棒性和解释性，应通过改进数据和模型开发来减小不确定性。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [51] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: 提出MEML-GRPO框架，通过多专家提示和相互学习，改善强化学习奖励稀疏问题，显著提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习奖励稀疏导致的学习信号不足问题，提升大语言模型推理能力。

Method: 引入多专家提示增强响应多样性，并利用专家间相互学习机制促进知识共享，从而改善RLVR效果。

Result: 在多个推理基准任务中，显著提升模型性能，Qwen提升4.89%，Llama提升11.33%。

Conclusion: MEML-GRPO有效克服了传统RLVR的核心限制，增强了大模型的推理能力。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [52] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 提出了一种无监督的偏差校正方法UDA，通过调整Elo评级系统减小评审偏差，提高了模型评估的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 评估大规模语言模型的配对评价存在偏差问题，影响评估的稳定性和可靠性。

Method: 设计无监督偏差校正框架UDA，利用神经网络动态调整Elo系统参数，减少评审间差异。

Result: UDA显著降低评委间评分标准差，提高与人类判断的相关性，实现评审一致性。

Conclusion: UDA有效缓解偏差，提高评价的稳健性，为模型评估提供更可靠的工具。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [53] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 本文提出PacifAIst基准，通过700个复杂场景评估大规模语言模型在行为对齐方面的表现，强调模型在自我保护与人类安全冲突中的风险，突显了标准化评估工具的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在社会中的应用日益重要，单纯抑制有害内容已不足以保障安全，需评估模型在行为自主性和目标冲突中的对齐性。

Method: 设计以存在性优先级为分类体系的700个测试场景，测试8个主流LLMs在自我保护、资源冲突等方面的行为偏差。

Result: 不同模型表现差异显著，Google Gemini 2.5 Flash表现最佳，GPT-5表现最差，部分模型在自我保护中表现欠佳，验证了评估工具的有效性。

Conclusion: 强调建立标准化的行为安全评估工具的重要性，以确保未来AI系统在行为目标冲突时能优先考虑人类安全。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [54] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: POL是一种用于推理公共观察下知识变化的逻辑，其可满足性问题是2EXPTIME完全的。


<details>
  <summary>Details</summary>
Motivation: 研究在多智能体系统中基于公共观察的知识更新推理逻辑，特别是POL的复杂性。

Method: 通过形式定义和模型论分析，证明POL满足性问题的计算复杂性。

Result: 成功证明POL的满足性问题是2EXPTIME完全。

Conclusion: POL在多智能体知识推理中具有丰富表达，但其计算复杂性较高，达到2EXPTIME。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [55] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL是一种结合多模态控制和对比学习的深度强化学习框架，提高了AI在内容生成中的人性化表现。


<details>
  <summary>Details</summary>
Motivation: 推动AI在协作内容创作中更贴近人类意图，实现更具人性化和可控性的内容生成。

Method: 引入多模态（文本、关卡、素描）控制，利用四重对比学习建立共享嵌入空间，并通过嵌入相似性调节策略。

Result: 在衡量人性化方面优于现有方法，结果通过量化指标和人类评价验证。

Conclusion: VIPCGRL有效提升了AI在协作内容创作中的人性化表现，为未来人机合作提供了新的技术路径。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [56] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 引入动态监管和机动机制的多智能体系统，提升基于大型语言模型的智能体的稳定性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 面对依赖多工具的智能体系统，出现信息噪声和不相关输出影响系统性能的问题。

Method: 设计动态监管和调度机制，通过执行代理和守卫代理的协作，增强系统的鲁棒性。

Result: 在GAIA测试集上显著改善了系统的效果和稳定性，获得领先排名。

Conclusion: 合作代理在构建更可靠的智能系统中具有重要应用价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [57] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 提出一种结合知识图谱和检索增强生成的多智能体框架，用于改进监管合规问答的准确性和追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在监管合规问答中面临的精确性和可验证性挑战。

Method: 构建和维护无本体的知识图谱，结合向量数据库实现图结构推理与信息检索，通过多智能体协同完成问答任务。

Result: 系统在复杂监管查询中优于传统方法，确保真实性、可追溯性，并通过子图可视化增强理解。

Conclusion: 该框架为合规审查和审计应用提供了可靠基础，提升了监管问答的性能与可信度。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [58] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估了四种大型语言模型在数学题目上的表现，发现增强推理能力的模型表现最好，双模型配置提升了准确率，为AI辅助数学教育提供策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在数学教育中的应用扩大，迫切需要评估其解题准确性和错误类型，以提升其作为教学和评估工具的可靠性。

Method: 设计具有挑战性的数学任务，分析四种LLMs在算术、代数和数论三个类别的解题表现及错误类型，采用单模型和双模型配置。

Result: 推理能力强的OpenAI GPT-4模型表现优异，程序性错误最常见，双模型配置显著提高性能。

Conclusion: 增强推理能力和双模型策略有助于提升LLMs在数学教育中的应用效果，为未来优化提供指导。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [59] [Towards Self-cognitive Exploration: Metacognitive Knowledge Graph Retrieval Augmented Generation](https://arxiv.org/abs/2508.09460)
*Xujie Yuan,Shimin Di,Jielong Tang,Libin Zheng,Jian Yin*

Main category: cs.IR

TL;DR: MetaKGRAG通过引入闭环的路径感知修正机制，提升了知识图谱检索增强生成模型在结构化知识理解上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有KG-RAG系统存在认知盲点，难以识别探索不足，导致相关性漂移和证据不完整。

Method: 借鉴人类元认知过程，设计了Includes Perceive-Evaluate-Adjust循环的MetaKGRAG框架，实现路径感知的闭环自我修正。

Result: 在五个不同领域数据集上，MetaKGRAG显著优于现有KG-RAG和自我修正基线，验证了路径感知修正的有效性。

Conclusion: 引入路径感知的闭环修正机制，有助于提升知识图谱检索增强生成模型的表现，强调路径感知修正的重要性。

Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) significantly
enhances the reasoning capabilities of LargeLanguage Models by leveraging
structured knowledge. However, existing KG-RAG frameworks typically operate as
open-loop systems, suffering from cognitive blindness, an inability to
recognize their exploration deficiencies. This leads to relevance drift and
incomplete evidence, which existing self-refinement methods, designed for
unstructured text-based RAG, cannot effectively resolve due to the
path-dependent nature of graph exploration. To address this challenge, we
propose Metacognitive Knowledge Graph Retrieval Augmented Generation
(MetaKGRAG), a novel framework inspired by the human metacognition process,
which introduces a Perceive-Evaluate-Adjust cycle to enable path-aware,
closed-loop refinement. This cycle empowers the system to self-assess
exploration quality, identify deficiencies in coverage or relevance, and
perform trajectory-connected corrections from precise pivot points. Extensive
experiments across five datasets in the medical, legal, and commonsense
reasoning domains demonstrate that MetaKGRAG consistently outperforms strong
KG-RAG and self-refinement baselines. Our results validate the superiority of
our approach and highlight the critical need for path-aware refinement in
structured knowledge retrieval.

</details>


### [60] [Improving Dense Passage Retrieval with Multiple Positive Passages](https://arxiv.org/abs/2508.09534)
*Shuai Chang*

Main category: cs.IR

TL;DR: 引入多个正面 passages 提升Dense Passage Retrieval的检索性能。


<details>
  <summary>Details</summary>
Motivation: 探索在训练中使用多个正面 passage 对DPR性能的影响，以改进检索效果。

Method: 在训练DPR模型时，为每个问题增加多个正面 passage，并进行对比实验。

Result: 多正面passage训练显著提高检索准确率，即使在较小的批次和单GPU条件下也有效。

Conclusion: 在DPR训练中加入多个正面passage是提升检索性能的有效方法，有助于加快训练并减少计算资源需求。

Abstract: By leveraging a dual encoder architecture, Dense Passage Retrieval (DPR) has
outperformed traditional sparse retrieval algorithms such as BM25 in terms of
passage retrieval accuracy. Recently proposed methods have further enhanced
DPR's performance. However, these models typically pair each question with only
one positive passage during training, and the effect of associating multiple
positive passages has not been examined. In this paper, we explore the
performance of DPR when additional positive passages are incorporated during
training. Experimental results show that equipping each question with multiple
positive passages consistently improves retrieval accuracy, even when using a
significantly smaller batch size, which enables training on a single GPU.

</details>


### [61] [TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking](https://arxiv.org/abs/2508.09539)
*Yongqi Fan,Xiaoyang Chen,Dezhi Ye,Jie Liu,Haijin Liang,Jin Ma,Ben He,Yingfei Sun,Tong Ruan*

Main category: cs.IR

TL;DR: 提出了一种基于小规模LLMs的高效推理排名模型TFRank，通过结合Chain-of-Thought数据和多任务训练，实现了在保持较低计算成本的同时达到与大模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有大型推理模型因计算成本高、延迟大限制实际应用的问题，开发效率更高的推理排名模型。

Method: 结合CoT数据、细粒度得分 supervision 和多任务训练，引入Think-Mode切换和点式格式约束，实现“Think-Free”推理能力，提升性能与效率。

Result: TFRank（如1.7B参数）在BRIGHT和BEIR基准上表现优异，接近四倍参数模型，展示了性能与效率的平衡。

Conclusion: TFRank提供了一个实用的方案，有效结合了推理能力与实际应用需求，推动了推理排名模型的实际落地。

Abstract: Reasoning-intensive ranking models built on Large Language Models (LLMs) have
made notable progress, but existing approaches often rely on large-scale LLMs
and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational
cost and latency that limit real-world use. To address this, we propose
\textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale
LLMs. To improve ranking performance, TFRank effectively integrates CoT data,
fine-grained score supervision, and multi-task training. Furthermore, it
achieves an efficient ``\textbf{T}hink-\textbf{F}ree" reasoning capability by
employing a ``think-mode switch'' and pointwise format constraints.
Specifically, this allows the model to leverage explicit reasoning during
training while delivering precise relevance scores for complex queries at
inference without generating any reasoning chains. Experiments show that TFRank
(e.g., 1.7B) achieves performance comparable to models with four times more
parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on
the BEIR benchmark. Further analysis shows that TFRank achieves an effective
balance between performance and efficiency, providing a practical solution for
integrating advanced reasoning into real-world systems. Our code and data are
released in the repository: https://github.com/JOHNNY-fans/TFRank.

</details>


### [62] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出一种结合多任务学习和深度语义嵌入的新模型架构，用于优化个性化商品搜索排名。


<details>
  <summary>Details</summary>
Motivation: 提升个性化商品搜索的准确性，通过整合多模态数据和先进嵌入技术。

Method: 采用多任务学习框架，集成表格与非表格数据，利用预训练的TinyBERT进行语义嵌入，并引入创新采样技术。

Result: 与多种基线模型比较显示，本文方法显著提升搜索排名效果，特别是结合非表格数据与高阶嵌入技术，提高模型性能。

Conclusion: 多模态数据与深度嵌入技术结合的多任务学习模型有效改善个性化搜索排名，辅以新颖的相关性标签机制具有广泛应用潜力。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


### [63] [On Negative-aware Preference Optimization for Recommendation](https://arxiv.org/abs/2508.09653)
*Chenlu Ding,Daoxuan Liu,Jiancan Wu,Xingyu Hu,Junkang Wu,Haitao Wang,Yongkang Wang,Xingxing Wang,Xiang Wang*

Main category: cs.IR

TL;DR: NAPO通过负样本管理优化推荐效果，提升准确率和减少偏见。


<details>
  <summary>Details</summary>
Motivation: 利用负样本优化LLM推荐模型，但现有方法存在效率低和对负样本信息利用不足的问题。

Method: 引入负样本共享和动态奖励边界调整机制改善负样本利用。

Result: 在多个数据集上实验显示NAPO优于现有方法，提升推荐准确率和减少偏见。

Conclusion: NAPO提出的两项创新机制有效提升了LLM推荐系统的性能。

Abstract: Recommendation systems leverage user interaction data to suggest relevant
items while filtering out irrelevant (negative) ones. The rise of large
language models (LLMs) has garnered increasing attention for their potential in
recommendation tasks. However, existing methods for optimizing LLM-based
recommenders face challenges in effectively utilizing negative samples. Simply
integrating large numbers of negative samples can improve ranking accuracy and
mitigate popularity bias but often leads to increased computational overhead
and memory costs. Additionally, current approaches fail to account for the
varying informativeness of negative samples, leading to suboptimal optimization
performance. To address these issues, we propose NAPO
(\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization),
an enhanced framework for preference optimization in LLM-based recommendation.
NAPO introduces two key innovations: (1) in-batch negative sharing, which
expands the pool of negative samples without additional memory overhead, and
(2) dynamic reward margin adjustment, which adapts model updates based on the
confidence of negative samples. Extensive experiments on three public datasets
demonstrate that NAPO outperforms existing methods in both recommendation
accuracy and popularity bias reduction.

</details>


### [64] [Multimodal Fusion And Sparse Attention-based Alignment Model for Long Sequential Recommendation](https://arxiv.org/abs/2508.09664)
*Yongrui Fu,Jian Liu,Tao Li,Zonggang Wu,Shouke Qin,Hanmeng Liu*

Main category: cs.IR

TL;DR: MUFASA模型通过多模态融合层和稀疏注意力机制，有效提升长序列推荐的性能，结合多模态信息与多粒度兴趣建模取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 解决多模态信息融合和多尺度用户兴趣挖掘中的挑战，提升推荐系统的内容理解和用户画像能力。

Method: 提出多模态融合层结合多任务损失优化，以及稀疏注意力机制进行长序列兴趣建模。

Result: 在真实数据集和线上测试中优于现有方法，显示其在多模态信息利用和用户兴趣捕捉方面的优势。

Conclusion: MUFASA通过创新的模态融合和兴趣建模策略，有效增强长序列推荐效果，应用前景广阔。

Abstract: Recent advances in multimodal recommendation enable richer item
understanding, while modeling users' multi-scale interests across temporal
horizons has attracted growing attention. However, effectively exploiting
multimodal item sequences and mining multi-grained user interests to
substantially bridge the gap between content comprehension and recommendation
remain challenging. To address these issues, we propose MUFASA, a MUltimodal
Fusion And Sparse Attention-based Alignment model for long sequential
recommendation. Our model comprises two core components. First, the Multimodal
Fusion Layer (MFL) leverages item titles as a cross-genre semantic anchor and
is trained with a joint objective of four tailored losses that promote: (i)
cross-genre semantic alignment, (ii) alignment to the collaborative space for
recommendation, (iii) preserving the similarity structure defined by titles and
preventing modality representation collapse, and (iv) distributional
regularization of the fusion space. This yields high-quality fused item
representations for further preference alignment. Second, the Sparse
Attention-guided Alignment Layer (SAL) scales to long user-behavior sequences
via a multi-granularity sparse attention mechanism, which incorporates windowed
attention, block-level attention, and selective attention, to capture user
interests hierarchically and across temporal horizons. SAL explicitly models
both the evolution of coherent interest blocks and fine-grained intra-block
variations, producing robust user and item representations. Extensive
experiments on real-world benchmarks show that MUFASA consistently surpasses
state-of-the-art baselines. Moreover, online A/B tests demonstrate significant
gains in production, confirming MUFASA's effectiveness in leveraging multimodal
cues and accurately capturing diverse user preferences.

</details>


### [65] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: 利用多模态大规模语言模型（MLLM）对视频进行高层语义描述，以提升视频推荐系统的语义理解和推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频推荐系统依赖低层特征，难以捕捉深层语义信息，影响个性化推荐的效果。

Method: 引入基于MLLM的零微调框架，将视频内容总结成丰富的自然语言描述，并结合多种推荐策略。

Result: 在MicroLens-100K数据集上，该方法优于传统特征，提升了推荐性能。

Conclusion: 利用MLLM作为知识提取器，能有效增强视频推荐系统的意图感知能力。

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于特征令牌化Transformer的飞机到达时间估算方法，具有高效率和高精度，适用于实时空中交通管理。


<details>
  <summary>Details</summary>
Motivation: 随着空域环境的快速变化，实时提取和预测飞机到达时间对于其入场管理尤为重要。

Method: 采用特征令牌化和Transformer模型，通过多头自注意力机制实现无需复杂特征工程的高频次ETA预测。

Result: 在新加坡樟宜机场的测试中，该方法比传统模型提升7%的精度，且计算效率仅为XGBoost的39%，实现了每秒一次的预测更新。

Conclusion: 基于特征令牌化的Transformer模型能够高效、准确地支持实时空中交通管理，为未来智能空管系统提供技术基础。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [67] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: 提出了一种多模态噪声动态编辑框架MoLAN，有效解决多模态情感分析中噪声干扰问题，结合多块分割和动态降噪策略，提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析中，容易受不相关或误导性信息影响，现有方法可能丢失关键特征。

Method: 将每个模态特征划分为多个块，根据噪声水平和语义相关性动态调整降噪强度，实现细粒度去噪。并基于此框架提出MoLAN+。

Result: 在五个模型和四个数据集上验证，MoLAN+状态最优，效果显著优于现有方法。

Conclusion: 提出的MoLAN框架具有广泛适用性，有效提升多模态情感分析性能，具有潜在的推广价值。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [68] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 提出基于大模型变换器的上下文学习方法优化信道接入，提高WiFi性能。


<details>
  <summary>Details</summary>
Motivation: 现有二进制指数退避策略在动态环境下效率低下，传统模型难准确估算节点密度。

Method: 设计变换器为基础的上下文学习优化器，通过大规模数据示例学习冲突阈值预测，支持容错数据输入。

Result: 实现快速收敛，接近最优吞吐量，在未知节点密度环境中优于传统方法。

Conclusion: 基于LLM变换器的上下文学习为无线信道接入提供有效优化工具，改善动态场景中的性能。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [69] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: Motif-2.6B是一款具有创新架构的2.6亿参数基础模型，兼顾性能与效率，提升理解能力，减少偏差，适合普及和实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前大型模型虽性能优异，但对于新兴研究团队来说，开发高效且性能优越的基础模型仍具挑战性，亟需更普及的解决方案。

Method: 引入差分注意力和PolyNorm激活函数等创新架构，结合大量实验优化模型结构，提升模型表现。

Result: 在多个基准测试中，Motif-2.6B表现优异，达到了或超越同规模的最先进模型，验证其效果、扩展性和实用性。

Conclusion: Motif-2.6B通过创新架构推动基础大语言模型的发展，为未来研究和部署提供了坚实基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [70] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 提出一种利用检索增强生成( RAG )的轻量级无训练训练方法，通过线性映射跨模态扩展，有效缓解大规模多模态预训练模型中的模态鸿沟，提高多模态任务表现。


<details>
  <summary>Details</summary>
Motivation: 面对大规模多模态模型在预训练和微调中的高成本及模态鸿沟问题，寻求低成本、高效率的解决方案。

Method: 使用无需训练的线性映射结合RAG技术，将视觉嵌入映射到文本空间，允许检索相关文本描述，并通过迭代技术优化映射以改善描述质量。

Result: 在两个多模态基准数据集上实现了显著性能提升，验证了方法的有效性。

Conclusion: 该方法提供了一种低成本、高效率的解决方案，有助于缓解大模型的模态鸿沟，促进多模态理解任务的发展。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [71] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 研究表明，在时间序列分析中，简单的全连接层（dense layers）可以替代复杂的序列混合器（如注意力机制），而性能并不逊色，甚至更优。


<details>
  <summary>Details</summary>
Motivation: 质疑复杂序列混合器在时间序列分析中的必要性，探索更简洁的模型结构。

Method: 将各种模型中的序列混合器替换为全连接层，基于MatrixMixer框架进行实验验证。

Result: 替换后模型性能相当甚至更优，挑战了“更复杂架构更优”的假设。

Conclusion: 复杂的序列混合机制未必必须，简洁架构亦能达到优异效果，促使未来模型设计更注重简洁性与效率。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [72] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: 提出了一种结合I-G变换和2SRI的深度学习框架DIG2RSI，有效估计复杂网络中的同行因果效应，解决反馈与隐匿混杂问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂的社会网络中，估计同行因果效应面临反馈环和未观察混杂的挑战，现有方法难以同时应对两者。

Method: 采用I-G变换分离同行影响，利用网络数据构建有效工具变量，并通过两阶段深度神经网络结合对抗机制控制混杂。

Result: 在半合成和真实数据上，DIG2RSI优于现有方法，且在标准条件下保证一致性。

Conclusion: 该方法通过深度学习与工具变量技术结合，有效改善同行效应估计的偏差，适用于复杂非线性模型。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [73] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: 提出了一种基于黎奇曲率和黎奇流的金融图动态推荐框架RicciFlowRec，用于根本原因归因和风险感知排序，提升金融决策的鲁棒性和解释性。


<details>
  <summary>Details</summary>
Motivation: 金融图中的复杂互动关系需要更高效的分析工具以识别风险来源和优化建议。

Method: 采用离散黎奇曲率衡量局部应力，通过黎奇流追踪冲击传播，利用曲率梯度揭示因果子结构，构建风险感知排序模型。

Result: 在标普500数据和FinBERT情感分析基础上验证，表现出增强的鲁棒性和解释能力。

Conclusion: RicciFlowRec首次将几何流理论应用于金融推荐，支持基于曲率的归因与风险排序，未来将扩展至投资组合优化与回报预测。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [74] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 提出一种名为AdaPO的在线强化学习框架，有效提升大多模态模型的自我评估能力，避免奖励操控。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型缺乏自我评估能力，亟需提升自我优化能力以改善多轮对话表现。

Method: 引入自适应奖励模型（ARM）和动态KL正则化机制，实时调整训练目标，动态响应训练状态。

Result: 在多个基准测试中，显著改善模型的推理与自我评估能力，验证了方法的有效性。

Conclusion: AdaPO框架通过自适应调节训练目标，有助于提升大模型的自我改进能力，具有广泛应用潜力。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [75] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 提出了一种基于流匹配生成模型的物理约束微调框架，用于科学系统中的逆问题求解，通过引入可学习潜在参数，有效实现物理一致性和隐藏参数的估计。


<details>
  <summary>Details</summary>
Motivation: 解决科学系统中逆问题的难题，既考虑数据驱动，又确保物理一致性。

Method: 从低保真或观测数据训练的模型出发，通过微分后处理最小化偏微分方程的残差，并结合可学习潜在参数预测，进行联合优化。

Result: 模型在典型偏微分方程基准测试中表现出更好的物理约束满足度和隐藏参数的准确恢复，验证了方法的有效性。

Conclusion: 该方法融合了生成建模与科学推断，为模拟增强的发现和物理系统的高效建模开辟了新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [76] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive提出一种基于对抗优化的多目标强化学习框架，解决路径规划中的多目标协调和迭代优化问题，实现了比现有方法更优的自主驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶面临人类般的反复决策和多目标权衡的挑战，现有框架缺乏闭环迭代和多目标偏好保持，亟需改进。

Method: 引入以对抗策略驱动的多目标强化学习，将轨迹生成与评价形成闭环，采用多尺度生成器和多目标评价器，通过帕累托前沿机制实现多轮迭代优化。

Result: 在NAVSIM和Bench2Drive平台上实现SOTA性能，显示优越的多样性和安全性，超越多项最新方法，满足复杂驾驶场景。

Conclusion: EvaDrive提供了一种无标量化偏差、多目标、多轮次迭代的轨迹优化新路径，为自主驾驶决策系统带来更接近人类的行为表现和更优的性能表现。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [77] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 本研究整合了15个数据集，建立了包含2510个受试者、149百万个测量值的大型糖尿病和低血糖事件数据库，分析了数据质量及血糖与心率的关系。


<details>
  <summary>Details</summary>
Motivation: 弥补糖尿病及低血糖研究中缺乏大规模数据集的不足，利用大数据和机器学习提升糖尿病管理和低血糖预警能力。

Method: 整合多个数据集，提取子数据库，评估数据质量，进行相关性分析，特别关注血糖与心率的关系。

Result: 形成了全面的多模态数据库，发现血糖与心率在低血糖前15到55分钟存在相关性，数据质量存在不平衡和缺失问题。

Conclusion: 大规模数据整合有助于糖尿病研究，揭示血糖与心率的潜在联系，为早期预警提供依据，但数据质量需进一步改进。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [78] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习和物理模型的PgMN，可提升能源预测在数据不足或新建建筑中的准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 能源消耗预测在建筑节能管理中关键，但受限于数据不足或新建筑缺乏历史数据的问题。

Method: 设计了多层平行投影层、记忆单元和记忆经验模块，融合深度学习与物理模型的预测。

Result: 验证显示PgMN在短期能源预测中具有效度，适用于数据缺失、新建建筑和基础设施变化等多场景。

Conclusion: PgMN有望改善动态建筑环境中的能源预测，增强模型的适应性和实用性。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [79] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出一种基于自编码器和自定义窗口SHAP的无监督可解释AI框架，有效检测核反应堆中的重放攻击，并实现攻击源、时间和信号数的识别。


<details>
  <summary>Details</summary>
Motivation: 随着先进核反应堆规模减小，依赖数字仪控系统，信息流变大，确保数据完整性尤为重要。现有检测方法大多依赖合成数据和线性模型，难以捕捉非线性和未建模的系统动态，亟需更有效的检测和解释手段。

Method: 结合自编码器和窗口SHAP算法，构建无监督可解释AI框架，用于检测和分析复杂的重放攻击，适应动态反应堆过程中的变化。

Result: 在普渡核反应堆PUR-1的实际数据集上测试，能以95%以上的准确率检测、识别攻击信号、识别源和持续时间，表现优异。

Conclusion: 该框架弥补了线性模型和合成数据的局限，为核反应堆等关键基础设施中的网络安全提供了有效的检测和解释工具。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [80] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 提出一种适用于随机计算神经网络的可调序列长度(ASL)方案，通过理论模型和随机森林验证其有效性，在能效和延迟方面取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决随机计算神经网络在层级混合精度实现中的能效和性能瓶颈，提高低功耗资源有限场景中的应用潜力。

Method: 引入基于算子范数的理论模型进行噪声传播分析，利用随机森林回归进行多层截断效果评估，提出两种不同粒度的截断策略以调节序列长度。

Result: 在32nm工艺制备的流水线随机计算多层感知机(MLP)测试中，ASL方案在能耗和延迟上比传统方法减少超过60%，几乎无准确率影响。

Conclusion: ASL方案验证了其在物联网等应用中的可行性，展现了随机计算神经网络中混合精度截断的优势，有助于推动低能耗神经网络设计研究。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [81] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 本文提出一种基于扩散模型的人口合成方法，有效平衡生成样本的多样性与可行性，超越了VAE和GAN等模型。


<details>
  <summary>Details</summary>
Motivation: 解决高维属性数据中，样本稀疏导致的人口联合分布建模困难，增强合成数据的代表性和准确性。

Method: 采用扩散模型进行联合分布估计，修复样本中的缺失采样零，同时控制结构零的生成。

Result: 新方法在多样性和可行性指标上优于VAE和GAN，表现出更好的性能。

Conclusion: 扩散模型为高维人口合成提供了一种有效的解决方案，改善了数据生成的质量与实用性。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [82] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: 提出了一种适应不同布局的ECG缺失表示学习框架PatchECG，能稳健识别心律不齐。


<details>
  <summary>Details</summary>
Motivation: 由于不同医院使用不同ECG布局，信号存在异步缺失，影响模型性能。

Method: 采用掩码训练策略的自适应变块数缺失表示学习，自动关注关键片段及其导联间依赖。

Result: 在PTB-XL和真实ECG数据上表现出良好鲁棒性，AUROC显著优于传统方法和ECGFounder模型。

Conclusion: 该方法提升了不同布局下ECG心律失常识别的稳定性和准确性。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [83] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: 提出SVG-1M数据集和SVGen模型，有效将自然语言转化为SVG，提升前端设计效率。


<details>
  <summary>Details</summary>
Motivation: 解决将创意转化为精确SVG图形耗时长的问题。

Method: 构建大规模SVG数据集，结合数据增强、链式思维注释，设计端到端生成模型，采用课程学习和强化学习优化。

Result: SVGen在效果和效率上优于其他模型和传统方法，代码和数据已开源。

Conclusion: 所提方法有效推动SVG自动生成的发展，提升前端和UI设计的智能化水平。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [84] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: 提出FedMP方法，利用随机特征流形补全和类原型引导，改善非IID分布下的联邦学习性能，特别是在医学影像领域。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非IID数据分布中模型收敛和性能受阻的问题，尤其是在医学影像中的应用。

Method: 采用随机特征流形补全和类原型对齐技术，增强个体客户端的特征空间，构建更稳健的决策边界。

Result: 在多种医学影像和多源自然图像数据集上实验验证，F比现有方法更优，展示出良好的性能提升。

Conclusion: FedMP通过特征流形补全和类原型对齐，有效改善非IID条件下的联邦学习效果，具有潜在的应用前景。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [85] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: DQT框架通过嵌套整数表示和专用整数运算，实现了无需反量化的静态混合精度和高效的动态实例量化，显著提升了深度神经网络在资源受限设备上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态混合精度量化在硬件实现中的高成本和效率瓶颈，提升深度神经网络在边缘设备上的应用效率。

Method: 引入嵌套整数表示和定制整数运算，配合轻量级控制器实现动态比特宽度切换，无需反量化操作。

Result: 在ResNet模型上实现了优异的性能，状态最优，且切换成本大幅降低，有效优化了性能与效率的平衡。

Conclusion: DQT提出了高效的动态量化新框架，为边缘设备上的深度学习模型部署提供了革新方案，具有广泛应用潜力。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [86] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: scAGC通过结合可调节图的自编码器、对比学习和ZINB模型，有效提升单细胞RNA测序数据的细胞类型分类性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单细胞RNA测序数据中高维、零通量多、长尾分布等挑战，提升细胞类型的准确识别。

Method: 引入自适应图自编码器、Gumbel-Softmax采样、对比学习及ZINB损失，共同优化特征和图结构。

Result: 在9个真实数据集上，表现优于其他最先进方法，NMI和ARI指标显著提升。

Conclusion: scAGC是一种稳定高效的细胞聚类工具，有助于深入理解细胞异质性。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [87] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于诚实拍卖的长远客户选择联邦学习方案（LCSFLA），旨在解决非独立同分布数据下的模型性能下降问题，通过鼓励诚实信息上传和考虑长远效益提高模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决物联网车辆中的非IID数据引起的联邦学习模型收敛和准确性问题，改善客户选择机制以提升整体性能。

Method: 引入长远客户选择算法，结合新的评估机制和能量成本，采用带押金的激励拍卖机制，确保客户诚实提交信息。

Result: 实验证明该机制有效缓解了非IID数据带来的性能下降，增强了客户的合作积极性，提升了模型的总体效果。

Conclusion: 该方案通过结合激励机制和长远考量，有效改善了物联网场景下联邦学习的客户选择问题，具有实际推广价值。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [88] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 呼吸分析通过传统和非接触式方法结合机器学习技术，广泛应用于健康监测，面临数据和隐私挑战，未来趋势包括可解释AI和迁移学习。


<details>
  <summary>Details</summary>
Motivation: 推动更舒适、便捷的健康监测技术，弥合传统与现代方法的差距。

Method: 综述现有接触式与非接触式呼吸分析技术，分析机器学习和深度学习方法的应用。

Result: 总结不同技术的应用场景、数据处理和模型比较，讨论主要挑战与新兴技术趋势。

Conclusion: 提供全面的技术框架，指导未来呼吸分析技术发展，结合先进技术与实际医疗需求。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [89] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 提出一种基于细粒度安全神经元的无训练连续投影方法，有效提升大模型的安全性和适用性。


<details>
  <summary>Details</summary>
Motivation: 应对微调过程中引入的安全风险，现有方法对安全层和神经元考虑不充分，难以平衡安全与性能。

Method: 设计FGSN结合无训练连续投影，局部化安全神经元，投影到安全方向，优化多维安全神经元聚类。

Result: 显著降低有害评分和攻击成功率，参数调整少，保持模型性能，实现持续防御和适应新安全需求。

Conclusion: 此方法有效提升大模型安全性和泛化能力，兼顾性能与安全的平衡，具有应用潜力。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [90] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast是一种结合大语言模型的时间序列预测方法，通过将数值数据和文本特征统一表示，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列预测中难以同时处理连续数据和文本描述的挑战。

Method: 利用离散标记器将连续时间序列转化为时间标记，并通过预训练的LLM在共享语义空间中融合同步编码时序和上下文特征，然后进行监督微调以预测未来的时间标记。

Result: 在多个真实数据集上，证明了TokenCast的有效性和良好的泛化能力。

Conclusion: TokenCast通过统一语义空间实现多模态信息的融合，有效提升时间序列预测的性能，具有广泛的应用潜力。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [91] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本文提出了离散扩散强制（D2F）策略，有效提升扩散大语言模型（dLLMs）的推理速度，达到或超过自回归（AR）模型的性能，并支持高效的块并行解码。


<details>
  <summary>Details</summary>
Motivation: 解决当前开源扩散大语言模型在推理速度上难以超越自回归模型的问题。

Method: 采用离散扩散强制（D2F）策略，通过块自回归生成和跨块并行解码的结合，改造dLLMs为高效的推理模型，并设计了流水线并行算法。

Result: D2F大幅提升dLLMs的推理速度，例如在GSM8K任务中比LLaMA3和Qwen2.5快2.5倍，优于传统dLLMs达50倍，同时保持拟输出质量。

Conclusion: D2F策略成功实现了大语言模型推理速度的显著提升，为大规模文本生成提供了更高效的解决方案。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [92] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: 提出了一种基于句子嵌入的多目标表示学习方法MIPCGRL，用于改善多目标指令下的内容生成可控性，显著提升了控制能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有指令强化学习在处理复杂、多目标指令时控制能力不足的问题。

Method: 引入句子嵌入作为条件，通过多标签分类和多头回归网络训练多目标嵌入空间。

Result: 在多目标指令控制能力方面提升了13.8%，增强了内容生成的表达力和灵活性。

Conclusion: MIPCGRL有效增强了指令驱动内容生成的可控性和复杂指令处理能力。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [93] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 提出一种基于元学习的推理加速策略选择框架，有效提升去中心化系统中的模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 应对大规模模型部署的高成本、可扩展性和数据安全挑战，探索更高效的推理加速方案。

Method: 利用元学习从历史性能数据中自动学习不同加速技术的优劣，取代传统的随机或经验判断。

Result: 该框架显著提高了加速策略的选择效率和系统性能，优于传统方法。

Conclusion: 元学习推动去中心化AI系统的推理优化，为生态发展提供新途径。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [94] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 本文提出了ADT4Coupons框架，利用序列模型优化线上平台的多轮优惠券分发策略，以提升长期收益。


<details>
  <summary>Details</summary>
Motivation: 现有优惠券分发策略未能有效利用复杂的用户-平台交互序列，导致性能停滞。

Method: 开发了基于决策转换器的模型，结合多个关键特性，在多场景下进行优化决策并实现迭代更新。

Result: 实验证明该框架在真实数据和仿真数据上均优于传统方法。

Conclusion: 提出的ADT4Coupons在提升长期收益和适应复杂场景方面具有显著优势，具有广泛应用潜力。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [95] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 本文提出了构建一套全面的施工安全数据集（CSDataset），整合了事故、检查和违规记录，支持机器学习分析，并通过初步分析展示了数据集的实用性。


<details>
  <summary>Details</summary>
Motivation: 当前施工安全数据有限且缺乏多样性，限制了深入分析与安全改进。

Method: 构建了包含结构化属性和非结构化描述的多层次数据集，进行基准测试和交叉分析。

Result: 发现投诉驱动的检查能显著降低事故发生率，数据集和代码已开源。

Conclusion: 所建数据集为施工安全研究提供了有价值的资源，有助于未来安全管理的提升。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [96] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: 提出一种基于MoE架构的混合量化专家框架MoQE，通过专家组合和动态路由，改善模型量化性能，减少准确率损失，同时保持推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决模型量化引入的性能损失问题，提高模型在资源有限设备上的应用效果。

Method: 设计多种量化变种专家，通过轻量级结构感知路由器在CV和NLP任务中动态选择最适合的专家。

Result: 在多个模型和数据集上表现优异，接近或达到最新最优，且推理延迟无显著增加。

Conclusion: MoQE有效缓解量化带来的性能下降，拓展了量化模型的应用前景。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [97] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 提出了一种基于可微转移模块的修复算法，有效减少微LED转移步骤，提升修复效率。


<details>
  <summary>Details</summary>
Motivation: 微激光转移在高通量微LED制造中关键，但现有修复算法效率有限。

Method: 设计了可微的转移模块，结合梯度优化实现修复路径规划，避免手工特征提取。

Result: 实现2000x2000阵列中步骤减少50%，规划时间低于2分钟，优于传统方法。

Conclusion: 该方法可在微LED制造中提供快速、灵活的修复路径优化方案，推动显示技术发展。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [98] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 提出了一种多层次自适应网络Hi-Vec，用于改善测试时模型在复杂分布偏移下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有测试时适应方法在面对多样和复杂的数据分布偏移时的能力不足问题。

Method: 引入多层次结构、动态层选择、权重融合机制和线性层一致性机制，以实现模型的动态和鲁棒适应。

Result: 在多项挑战性场景和多个数据集上验证了Hi-Vec的有效性，优于现有方法，增强了模型的鲁棒性和应对不确定性的能力。

Conclusion: 多层次和动态机制的Hi-Vec显著提升了模型应对复杂偏移的能力，为测试时模型适应提供了新的解决方案。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [99] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: 提出了一种融合图注意力网络和序列到序列RNN的二阶段混合模型GSMT，用于城市巴士轨迹预测，结合任务修正器提升预测精度，在吉隆坡实际数据集上效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 城市环境中巴士轨迹预测对于智能交通系统至关重要，但在发展中地区由于多模态数据有限，单靠GPS数据仍具有挑战。

Method: 该方法结合图注意力网络和RNN进行轨迹预测，并采用任务修正器对结果进行二次优化，通过融合动态与静态信息实现多节点预测。

Result: 在吉隆坡真实数据集上，GSMT明显优于其他方法，在短期和长期预测中表现出色。

Conclusion: 该模型有效结合多个信息源和二阶段策略，提升了城市巴士轨迹预测的准确性，具有实际应用潜力。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [100] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 提出结合量子启发图神经网络与集成模型的创新方法，用于区块链反洗钱交易检测，达到较高准确率。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域对区块链非法交易检测需求日益增加，寻找高效创新的解决方案迫在眉睫。

Method: 设计结合CP分解层的量子启发图神经网络，并支持集成QBoost或经典模型，提升复杂数据分析能力。

Result: 在反洗钱交易检测中取得74.8%的F2得分，优于传统方法，验证量子启发技术潜力。

Conclusion: 量子启发算法结合结构创新在金融安全中的应用前景广阔，值得进一步推广与研究。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [101] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 提出一种基于大型语言模型的零样本和少样本表格学习框架，通过生成特征值建立原型，无需训练分类器或微调模型。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型在表格数据建模中实现零样本和少样本学习的潜力，解决以示例为基础的提示限制。

Method: 设计基于提示的特征值生成和原型估计方法，不依赖示例，提升框架的普适性与鲁棒性。

Result: 在多项零样本和少样本表格学习任务中表现优异，验证了其效果。

Conclusion: 本文提出的无示例原型估计框架有效促进了LLM在表格少样本学习中的应用，具有良好的扩展性和实用性。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [102] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 利用深度学习模型对嗅觉信号进行单次检测，验证了局部场电位的频谱特征在检测中的有效性，结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升嗅觉检测的准确性和单次检测的可靠性，满足食品安全和医学诊断等实际需求。

Method: 提出基于一维卷积神经网络（ResCNN和AttentionCNN）的集成模型，解码嗅觉球LFP信号中的气味信息。

Result: 模型在多次试验中达到了86.6%的准确率，F1-score为81.0%，AUC为0.9247，优于以往方法。利用t-SNE验证了模型捕捉到生物学相关的特征。

Conclusion: 深度学习模型能够有效实现单次气味检测，为嗅觉信号的理解和应用提供了技术基础。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [103] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 提出一种基于拓扑的评估方法，用于衡量图神经网络中过度挤压现象，帮助指导重连策略以改善性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络中长期信息传递受限，影响模型表现。需要量化过度挤压以指导结构优化。

Method: 提出基于节点对互感敏感度衰减率的评估指标，结合图级统计指标，分析重连策略的影响。

Result: 在多种数据集上验证了过度挤压的普遍性和重连策略的有效性，强调策略的适度应用和目标导向性。

Conclusion: 该方法为实践者提供了实用的诊断工具，可在训练前判断重连是否值得，促进GNN模型的优化。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [104] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 本文研究了合作多智能体强化学习的潜在对抗漏洞，提出针对实际限制条件下的攻击算法，并验证其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 随着合作多智能体强化学习在现实应用中的普及，安全性成为亟需关注的问题，特别是在面对有限信息的敌对攻击时的脆弱性。

Method: 提出简单有效的对抗扰动生成算法，旨在扰乱代理的环境感知，验证其在多个基准和环境中的表现。

Result: 算法在多个场景中表现出良好的攻击效果，且样本效率高，仅需1000个样本，优于以往方法。

Conclusion: 该研究揭示了合作多智能体强化学习在现实条件下的潜在安全风险，提供了高效的攻击工具，为未来的防御研究提供了方向。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [105] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 提出结合联邦学习和区块链的去中心化天气预报框架，提高隐私、安全性、扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有集中式天气预报系统面临的安全漏洞、扩展性不足和单点故障问题。

Method: 融合联邦学习、区块链和IPFS技术，设计了安全可靠的去中心化预报系统。

Result: 实验表明该系统提升了预报精度和系统抗脆弱性，具有实际应用潜力。

Conclusion: 该框架为安全关键环境中的天气预报提供了一种可行的去中心化解决方案。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [106] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 通过模式识别自动提取学生代码中的结构化知识组件（KCs），实现可解释的个性化计算机科学学习模型，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有KC提取面临可解释性不足和程序结构多样化等挑战，亟需一种更具可解释性和自动化的方案。

Method: 采用变分自编码器结合注意力机制，从学生代码中识别并聚类结构化模式，形成基于模式的KCs。

Result: 在学习曲线和深度知识追踪（DKT）方法中均显示出学习轨迹的合理性和预测性能的显著提升。

Conclusion: 提出的基于模式的KC识别框架提升了知识建模的可解释性和实用性，为计算机科学教育中的个性化学习提供了新工具。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [107] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 论文提出一种数据集蒸馏技术，将复杂的强化学习环境压缩成微型监督学习数据集，实现快速学习和跨任务迁移。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习环境数据复杂、训练成本高的问题，寻求有效的数据压缩和跨模态迁移方法。

Method: 开发了扩展的近端策略优化算法用于元学习，结合数据集蒸馏技术，将强化学习环境转化为监督学习任务。

Result: 成功将多种强化学习环境压缩为单步监督学习数据集，实现了跨任务有效迁移，验证了方法的普适性和环境压缩能力。

Conclusion: 数据集蒸馏是一种强有力的环境压缩和跨模态迁移工具，能显著降低RL任务的学习复杂度。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [108] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了一种针对GNNs的精确验证方法，支持多种聚合函数，增强模型在对抗攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着GNNs在高风险领域的应用，为保证其安全性和鲁棒性亟需有效的验证工具。

Method: 采用约束解法和边界松弛技术，迭代求解一系列松弛的约束满足问题，开发了支持sum、max和mean聚合函数的GNNev工具。

Result: 在多个基准和实际数据集上验证了方法的有效性，优于现有的验证工具，尤其在sum聚合任务中表现突出。

Conclusion: 该验证方法有效扩展了GNN鲁棒性验证的能力，支持更广泛的聚合函数，具有重要的实际应用价值。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [109] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 提出一种基于重要性逐渐剪枝的网络正则化方法，有效提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 模拟生物突触修剪机制，改善人工神经网络的正则化效果。

Method: 结合绝对值重要性评价，动态逐步剪枝，同时保持梯度传导。

Result: 在多种时间序列模型与数据集上表现优异，显著降低误差，优于标准dropout，特别是在金融预测中效果突出。

Conclusion: 该方法有效结合剪枝与稀疏化，易于集成，具有广泛应用前景，优于传统dropout。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [110] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 研究表明，将稀疏自编码器（SAEs）限制在特定医学领域，能显著提升模型的重建能力和解释性，减少“黑暗物质”。


<details>
  <summary>Details</summary>
Motivation: 解决广泛数据训练导致的潜在特征碎片化和解释困难的问题。

Method: 在Gemma-2模型的第20层激活上训练JumpReLU SAEs，使用195k医学问答数据，实现领域限制。

Result: 领域限定的SAEs解释变量增加20%，损失恢复更佳，线性残差减少，且学得的特征与临床相关，更具解释性。

Conclusion: 领域限制能改善SAEs的表现与可解释性，提出对“基础模型”规模扩展的反思。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [111] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 研究探讨文本到图像模型是否能将痴呆症相关的言语信息与生成的图像实现对齐，发现可在仅由模型生成的图像中检测痴呆，准确率达75%，并利用可解释性方法分析贡献部分。


<details>
  <summary>Details</summary>
Motivation: 想了解文本到图像模型是否能将与痴呆相关的言语信息与生成的图像对齐，以及这种对齐是否可用于痴呆检测。

Method: 分析模型生成的图像与言语信息的对齐性，利用可解释性方法揭示语言中哪些部分促进痴呆检测。

Result: 模型能从生成的图像中仅凭其识别痴呆，准确率达75%；并明确了影响检测的语言部分。

Conclusion: 文本到图像模型可用于痴呆检测，并能解释哪些语言特征关键，拓展了模型应用的可能性。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [112] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: 提出NeuronTune，一种细粒度动态调节神经元的框架，提升大模型的安全性和用途性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在保障安全性与保持模型效用之间的权衡问题，提升大模型的安全可靠性与实用性。

Method: 通过归因分析识别关键神经元，利用元学习调节安全相关和实用相关神经元的激活，支持可调节的干预范围。

Result: 在多项实验中显著优于现有技术，安全性得到提升，模型的效用保持达到最佳水平。

Conclusion: NeuronTune实现了安全性与实用性的双赢，提供了更细粒度的调节机制以应对不同场景需求。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [113] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 本文提出一种基于联邦学习的跨机构金融风险评估框架，有效提升风险识别的效率与精准度，保障数据隐私。


<details>
  <summary>Details</summary>
Motivation: 金融风险分析中存在数据隐私保护和合作建模的挑战。

Method: 利用联邦学习结合特征注意机制和时间序列建模，通过分布式优化策略实现多机构协作，同时利用差分隐私保护模型参数。

Result: 实验结果显示，该方法在通信效率、模型准确性和风险检测能力方面优于传统方法和其他联邦学习变体，适用于敏感金融环境。

Conclusion: 该模型增强了风险识别的范围和效率，提供了一种安全、有效的金融风险智能分析解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [114] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出一种结合图卷积和Transformer的无监督分布式服务异常检测方法，能有效捕获复杂结构和动态行为。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端服务系统中的复杂依赖关系、行为多样性和缺少标签的问题。

Method: 构建动态图，应用图卷积提取结构信息，利用Transformer建模时间行为，通过特征融合进行异常评分。

Result: 在实际云监控数据上表现优异，比现有模型更具表达能力和稳定性，适合实际应用。

Conclusion: 该方法有效结合结构与行为信息，提升异常检测性能，为实际部署提供潜力。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [115] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: DGS-MAML是一种结合梯度匹配与锐度感知最小化的元学习算法，能提升在有限数据和少样本场景中的任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为解决少样本学习和任务泛化困难，提出一种新颖的元学习算法以增强模型的适应性和鲁棒性。

Method: 将梯度匹配与锐度感知最小化结合在双层优化框架中，进行理论分析并通过实验证明有效性。

Result: 在多个基准数据集上优于现有方法，表现出更好的准确率和泛化能力。

Conclusion: 该方法适用于少样本学习和快速适应场景，具有理论保障和实际应用潜力。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [116] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 引入隐式超图神经网络（IHGNN），通过非线性固定点方程实现高阶关系的稳定全球传播，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决超图神经网络在深层结构中长距离依赖捕获不足和训练不稳定的问题。

Method: 提出IHGNN，将表示作为非线性固定点方程的解，通过理论分析和训练策略增强模型稳定性与表达能力。

Result: 在多个引文基准上，IHGNN在准确性和鲁棒性方面优于传统模型，表现出良好的泛化能力和实用价值。

Conclusion: IHGNN通过隐式实现有效的高阶关系建模，为超图学习提供新的解决方案，具有理论保障和实际应用潜力。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [117] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合对比和生成的时间序列自监督学习框架CoGenT，有效提升多元时间序列的表示能力。


<details>
  <summary>Details</summary>
Motivation: 探索对比方法和生成方法在时间序列自监督学习中的互补潜力。

Method: 通过联合对比-生成优化，设计了CoGenT框架，解决两者在数据敏感性和依赖规模上的局限性。

Result: 在六个时间序列数据集上取得显著性能提升，F1分数较单一方法最高提升59.2%。

Conclusion: 混合目标既保持判别能力，又增强生成鲁棒性，为时间序列自监督学习提供新思路。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [118] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: 提出了一种名为NEXICA的算法，用于识别高速公路中导致交通缓慢的关键区域，从而有助于缓解交通拥堵。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵是一个持续性问题，资源有限，需要有效识别造成拥堵的区域以优化干预策略。

Method: 利用道路速度时间序列中的事件（交通减速的开始点）进行因果关系检测，采用概率模型和机器学习分类器来识别因果关系。

Result: 在洛杉矶地区六个月的数据测试中，该方法在准确性和计算效率方面优于现有先进技术。

Conclusion: 该算法能够有效识别交通拥堵的关键原因区域，为交通管理提供了新的工具。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [119] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: DeepFeatIoT通过融合本地、全局特征以及非学习型特征，有效提升IoT时间序列数据分类能力，尤其在有限标注情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决IoT传感器数据中metadata丢失、多源异构、采样频率不同和时间戳不规则等挑战，提升智能系统的理解和性能。

Method: 提出融合深度学习提取的局部和全局特征、随机卷积核特征和大模型特征的DeepFeatIoT模型。

Result: 在多个实际IoT数据集上表现优异，优于现有技术，验证模型的泛化能力和有效性。

Conclusion: DeepFeatIoT为IoT数据分析提供了强大工具，推动智慧系统发展。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [120] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出了一种结合深度特征提取、细粒度聚类和异常检测的开集故障诊断模型FGCRN，能准确识别已知和未知故障。


<details>
  <summary>Details</summary>
Motivation: 在多模态工艺中，样本表现出多重聚类分布，构建准确的决策边界具有挑战，需提升对未知故障的识别能力。

Method: 采用多尺度深度卷积、双向门控循环单元及时间注意机制提取特征，用距离损失增强类内紧凑性，通过无监督学习挖掘细粒度特征，结合极值理论实现未知故障的检测。

Result: 实验证明该模型在故障诊断中的表现优异，尤其在识别未知故障方面具有优势。

Conclusion: 该方法有效结合深度特征、多尺度信息和异常检测技术，提升了多模态流程中故障诊断的准确性与泛化能力。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [121] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 提出一种面向联邦连续学习的协作框架，利用轻量模型作为桥梁，有效适应新任务，提升大模型利用率。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在联邦连续学习中面临的任务适应和知识遗忘问题，以及利用小模型资源的限制。

Method: 设计轻量模型的持续微调机制和逐一蒸馏技术，实现个性化融合与知识更新。

Result: 实验显示该框架在不同客户端异构模型条件下，性能优越。

Conclusion: 该方法有望推动FCL中大模型的应用与持续学习能力的提升。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [122] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: 提出了一种结合图建模与混合搜索策略的Meta-NAS框架GraB-NAS，能高效发现任务适应性强的神经网络结构。


<details>
  <summary>Details</summary>
Motivation: 解决现有Meta-NAS在泛化能力、搜索空间限制和计算成本方面的不足，提升神经架构搜索的效率和效果。

Method: 将神经架构建模为图，结合贝叶斯优化的全局搜索与梯度上升的局部探索，进行混合搜索策略。

Result: 在多项实验中，GraB-NAS优于现有主流Meta-NAS方法，表现出更好的泛化能力和搜索效果。

Conclusion: 通过图建模和混合搜索策略，GraB-NAS显著提升了Meta-NAS的性能，有助于自动化设计更强适应性的神经网络。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [123] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: 引入EGGS-PTP方法，通过图结构引导剪枝，提高大模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模扩大，部署时面临计算和存储挑战，需发展更高效模型。

Method: 利用图论和擴展图设计N:M结构化剪枝，确保信息流和模型性能。

Result: 在多个LLMs中，EGGS-PTP实现显著加速和节省内存，并优于现有剪枝方法。

Conclusion: 提出的EGGS-PTP有效提升大模型的推理效率，同时保持较高准确性。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [124] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 提出了一种面向物联网车辆系统的分层联邦微调框架，结合低秩适应和多臂赌博机优化，提高了模型在动态环境下的效率和精确度。


<details>
  <summary>Details</summary>
Motivation: 为边缘环境中的基础模型多任务适应面临客户机移动性、异质资源和不稳定连接的挑战。

Method: 使用低秩适应结合去中心化能耗感知秩值调节机制，基于多臂赌博机算法UCB-DUAL实现资源调节。

Result: 在基于真实轨迹的仿真中表现优异，latency降低24%，准确率提升2.5%。

Conclusion: 该方法实现了动态场景下的高效、低延迟、多任务模型微调，对边缘智能具有很好的应用潜力。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [125] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 提出了一种基于因果能力的目标发现框架（GDCC），通过识别状态空间中的关键点，提高探索效率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中因果关系难以测量的问题，以提升强化学习中智能体的探索效率。

Method: 引入因果容量度量，通过蒙特卡洛方法识别关键决策点，并优化以适应连续高维环境。

Result: 关键点对应重要子目标，GDCC显著提升任务成功率。

Conclusion: 基于因果能力的目标发现方法有效增强探索，具有广泛应用潜力。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [126] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: 提出MiCo框架，实现端到端的层级混合精度量化方案探索与部署，提升边缘设备上的神经网络性能。


<details>
  <summary>Details</summary>
Motivation: 为解决现有混合精度量化算法在灵活性和效率上的限制，满足边缘设备高效存储与计算需求。

Method: 采用新型优化算法进行最优量化方案搜索，建立硬件感知延迟模型，实现快速探索，并支持直接从PyTorch模型到裸机C代码的部署。

Result: 实现了在满足延迟约束的同时，最大化模型准确率的混合精度量化方案，并完成端到端的模型部署，显著提升边缘AI性能。

Conclusion: MiCo框架有效提升了MPQ方案的探索效率和部署便捷性，为边缘AI模型优化提供了完整解决方案。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [127] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG通过利用语言模型解析变量语义，构建知识图谱，并结合时间序列数据，实现更具解释性和性能的多变量时间序列分析。


<details>
  <summary>Details</summary>
Motivation: 传统模型忽视变量名称和描述中的语义信息，限制了模型的解释性和性能。

Method: 使用大语言模型解释变量语义，构建知识图谱，采用双模态编码融合语义和时间序列信息，实现因果推断和任务指导。

Result: 模型显著提升了预测和分类性能，增强了模型的泛化能力和解释性。

Conclusion: 引入知识驱动的多模态因果推理方法，有助于实现具有良好解释性和强泛化能力的时间序列分析。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [128] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了一种基于因果图的异常检测框架CGAD，利用因果结构实现对关键基础设施网络中复杂攻击的高效识别，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 应对关键基础设施中复杂、多变的网络攻击，传统检测方法效果有限。

Method: 引入因果图结构和动态贝叶斯网络，通过因果剖面和结构发散进行异常检测。

Result: 在多个工业数据集上，显著优于传统方法，提高了F1和ROC-AUC等关键指标，尤其在偏态和漂移环境中表现优越。

Conclusion: 基于因果结构的模型增强了异常检测的适应性和准确性，扩展了鲁棒性的新界限。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [129] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: 提出了一种结合高斯混合模型的复现策略，通过指导生成过去学习内容，有效缓解大型语言模型的灾难性遗忘问题，提升模型在持续学习中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在持续学习中面临的灾难性遗忘难题。

Method: 引入Gauss-Tin方法，将重放策略与高斯混合模型结合，通过指导生成过去学习内容，优化样本选择。

Result: 实现了比传统方法高6%的保留指标提升，验证了该方法在减缓灾难性遗忘方面的有效性。

Conclusion: 杂交模型具有增强LLMs在动态学习环境中的鲁棒性和适应性的潜力。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [130] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种统一、可解释的GNN框架，用于预言企业流程中的未来事件，通过比较不同模型、引入时间衰减注意机制以及边特征语义嵌入，优化预测性能和解释性，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 改善基于图神经网络的业务流程预测模型，弥补现有模型在局部与全局、多时序以及语义理解方面的不足。

Method: 比较局部和全局图卷积网络，引入时间衰减注意机制，并将转移类型语义融入边特征，结合多层解释模块。

Result: 模型在五个基准数据集上达到了具有竞争力的Top-k准确率和DL评分，且无需专门调参，证明了其鲁棒性和通用性。

Conclusion: 通过多维度改进，该框架为业务流程的下一事件预测提供了一个强大、可解释、通用的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [131] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 利用大语言模型增强概念瓶颈模型，提升ARDS识别中的可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 医疗数据不完整且缺乏标签，需增强模型的解释性和准确性。

Method: 结合大语言模型处理临床笔记，生成额外概念，改善概念瓶颈模型的表现。

Result: 实现了比已有方法多10%的性能提升，同时增强了概念的表达力，降低偏差风险。

Conclusion: 通过引入上下文信息，改善CBM在临床疾病识别中的表现和解释性，具有广泛应用前景。

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [132] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: 本文提出SYNAPSE-G，一种结合大模型生成合成数据与图结构半监督学习的稀有事件分类方法，通过合成和传播数据，有效提升少样本场景下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决稀有事件少样本及冷启动问题，提升机器学习模型在稀缺标注数据情况下的表现。

Method: 利用大模型生成合成样本，构建相似性图进行半监督标签传播，结合人类或大模型进行验证，最后训练分类器。

Result: 在IMBALANCED SST2和MHS数据集上验证效果优于基线方法，特别是在找到正类标签方面表现突出。

Conclusion: 结合合成数据与图结构的半监督策略，有助于稀有事件分类中的少样本学习，理论分析揭示合成数据质量对性能的影响。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [133] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: 本文系统分析了边缘通用智能（EGI）中的世界模型，强调其在自治、多环境感知与决策中的关键作用，展望未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算的快速发展，边缘智能系统需要具备自主感知、推理与行动能力，世界模型为实现这种能力提供了理论支撑。

Method: 通过分析世界模型的架构基础，结合其在不同EGI场景中的应用示例，并探讨其与基础模型和数字孪生的结合，提出未来挑战与方向。

Result: 界面模型有助于提升边缘智能的自主性和适应性，尤其在车辆、无人机、物联网等多场景下优化性能，强化其主动推理和未来规划能力。

Conclusion: 世界模型是推动EGI发展的关键技术，需解决安全、训练效率和部署限制等挑战，未来具有巨大潜力。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [134] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: Proto-PINV+H通过结合闭式解和梯度优化，实现高效且快速训练，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提升模型训练速度与效率，同时保持较高的精确率。

Method: 用闭式方法计算权重，利用Adam优化原型，减少参数空间；引入多层扩展及正则化参数，连接矩阵条件数与泛化能力。

Result: 在MNIST和Fashion-MNIST上快速达到高准确率（97.8%、89.3%），训练时间短，参数少，优于传统模型。

Conclusion: 该方法实现了快速、高效的训练，提供了不同深度结构和正则化的扩展，具备良好的实用潜力与理论基础。

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [135] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种有限选择性预测模型（PLS），研究预测者只能在时间范围内的子集开始预测的情况，分析了最佳预测误差并提出了复杂度指标，用于界定实例相关的预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究在有限选择性的条件下预测的潜力与限制，以扩展传统的完全自由预测模型。

Method: 引入PLS模型，定义实例相关的复杂度指标，通过平均案例与实例分析，推导最佳预测误差界限，并对随机生成的实例进行概率匹配检验。

Result: 在随机生成的PLS实例中，复杂度界限与实际误差高度匹配，展示模型的有效性与适用性。

Conclusion: 有限选择性将影响预测的误差界限，通过复杂度分析可以理解与优化预测性能，对实际预测任务具有指导意义。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [136] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 提出了一种基于层次概念建模和优化理论的理论框架，用于解释大型语言模型在上下文学习中通过向量运算实现事实回忆的机制，证明了变换器优于静态嵌入的优势。


<details>
  <summary>Details</summary>
Motivation: 弥补目前对LLMs在上下文学习中实现事实回忆机制的理论理解不足，提供形式化解释以推动模型设计与优化。

Method: 构建以梯度下降训练非线性残差变换器的优化理论，结合层次概念模型，分析模型在事实回忆中的向量运算，证明损失收敛与强泛化性。

Result: 证明变换器在事实回忆任务中实现向量演算的能力，展示其对概念重组与分布变化的鲁棒性，验证模型优于静态嵌入。

Conclusion: 该理论框架深入揭示了变换器在上下文学习中的优势，为未来模型设计提供理论基础。

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [137] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种结合几何感知和物理信息的空间-光谱图神经算子，用于高效准确地求解复杂区域的偏微分方程，尤其在有限标注数据和几何变化情况下表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决PDEs的高效准确求解，特别是在复杂几何和有限标注数据条件下。

Method: 改进Sp$^2$GNO，加入几何感知，结合物理信息设计混合损失函数，实现模拟自由的学习。

Result: 在多种复杂域和不同问题类型测试中，展现优于现有物理引导神经算子的性能。

Conclusion: 提出的方法有效融合几何和物理知识，提升偏微分方程求解的能力，适用于复杂场景。

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [138] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是一个基于Python的蛋白质热稳定性分析框架，能灵活捕捉各种热曲线形状，优于传统方法，特别适用于复杂或非典型的热稳定性数据。


<details>
  <summary>Details</summary>
Motivation: 传统TPP方法在处理非sigmoidal或复杂热曲线时存在局限，导致对生物学意义的检测不足。

Method: 采用高斯过程模型与核函数，灵活建模各种热稳定性曲线，并生成无偏的统计检验。

Result: 显著提升了对蛋白质热稳定性变化的检测能力，包括难以用传统方法捕捉的复杂和非典型变化。

Conclusion: Thermal Tracks提供了一个高效、灵活且开放的工具，有助深化对蛋白质热稳定性及其生物学意义的理解。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [139] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: 本文研究非对称低秩矩阵补全问题，证实无需正则化项即可实现线性收敛，强调梯度下降的隐式正则化作用，提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 解决非对称低秩矩阵补全中梯度下降的收敛性问题，简化算法复杂度。

Method: 通过引入留一法技术，分析纯梯度下降在光谱初始化下的线性收敛，并验证正则化项的隐式作用。

Result: 证明纯梯度下降在高概率下能达到线性收敛，正则化项在迭代中保持小范数，算法计算成本低，性能优越。

Conclusion: 无需正则化项也能保证收敛速度，彰显梯度下降的隐式正则化优势，促进低秩矩阵补全方法的应用与优化。

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [140] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: 提出一种数据高效的蒸馏框架DED，通过优化师模型选择、数据规模与多样化推理路径，有效提升LLMs的推理能力，且仅需少量高质量样本，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型推理能力提升中高昂计算成本的问题，探索高效蒸馏策略。

Method: 结合强化学习的策略，优化师模型选择，精选数据集，促进多样化推理路径，达到性能平衡。

Result: 在数学推理和代码生成任务中，仅用800个精选样本实现了SOTA效果，超越传统大规模蒸馏方法。

Conclusion: DED提供了一种实用高效的推理蒸馏方案，兼顾模型性能与资源成本，推动大模型推理能力的普及与应用。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [141] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 本文提出了一种基于算子理论的时间锚定框架，分析了在嵌入空间中的漂移映射和仿射投影的收敛性，特别是对注意力层的软max函数的收缩性质进行了详细证明。


<details>
  <summary>Details</summary>
Motivation: 旨在建立一种严谨的数学框架，以理解和分析嵌入空间中时间锚定的机制，特别是联系漂移映射、仿射投影与神经网络中的注意力层的关系。

Method: 通过定义漂移映射和仿射投影的算子，以及引入内在的“手稿计算机”模型，利用变块收缩、收敛定理和鲁棒性分析，推导出系统的收敛性和稳定性条件，特别是在注意力层的软max函数方面。

Result: 成功证明了变块收缩引理、漂移-投影收敛定理和在嵌套仿射锚点下的本体收敛性，建立了一个完备的理论框架，证实了注意力层中 softmax 函数的 $1/2$-Lipschitz 性，并提供了合适的收缩条件。

Conclusion: 该研究为理解嵌入空间中的时间锚定机制提供了坚实的数学基础，为未来设计更稳定和可解释的神经网络提供了理论支持。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [142] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: 本文研究在无标签数据中确认异常存在的条件，提出样本数与数据大小、污染率及算法相关常数的关系，为异常检测的极限提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 缺乏对异常是否存在的确定性研究，限制了异常检测方法的效果评估与应用。

Method: 通过三百万次统计检验，分析数据集大小、污染率与算法常数的关系。

Result: 发现若数据集大小满足特定条件，则能确定异常的存在，揭示了异常检测中的样本需求界限。

Conclusion: 定义了确认异常存在的样本下界，为异常检测设定了理论极限。

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [143] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出了一种动态连接掩码（DCM）机制，用于增强神经网络在有噪声标签环境下的鲁棒性，通过自适应掩盖不重要的连接，有效提升性能，且证明其理论优势，广泛适用于不同训练策略，并在多项实验中优于现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 应对现实场景中的噪声标签对深度学习模型性能造成的影响，强调架构正则化的潜力不足。

Method: 引入动态连接掩码机制，在训练过程中自适应掩盖边权，结合理论分析确保其效率，并能结合多种噪声鲁棒方法。

Result: 实验证明该方法在合成和真实场景中均优于现有最优方法，还首次揭示了Kolmogorov-Arnold Networks (KANs)在噪声环境下优于多层感知机（MLPs）的优势。

Conclusion: 动态连接掩码机制是一种有效提升深度网络抗噪能力的通用方法，可与多种训练策略结合应用，具有广泛潜力。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [144] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: 提出四种利用大语言模型(LLM)进行环境识别预测的策略，包括解释性推理(ReDP)、预测优化(CorDP)、例子嵌入(IC-DP)和任务路由(RouteDP)，提升了LLM在现实场景中的预测能力和效率。


<details>
  <summary>Details</summary>
Motivation: 弥补LLMs在环境识别预测中潜力未充分利用的不足。

Method: 通过引入四种策略：ReDP提升可解释性，CorDP增强预测细节，IC-DP利用示例增强准确性，RouteDP实现任务高效路由。

Result: 在CiK基准任务中，这些策略优于简单prompt方法，展示了不同模型和规模的优势。

Conclusion: 这些策略为基于LLM的环境识别预测提供了有效途径，推动未来相关应用的发展。

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [145] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: 提出了一种树状生成模型GraphTreeGen，用于高效、精确地生成脑连接组，克服了现有模型的局限性，展现出优越的性能和潜在的拓展能力。


<details>
  <summary>Details</summary>
Motivation: 脑连接组作为神经网络连接的图结构，获取成本高、耗时长，迫切需要高效的生成方法来辅助研究和应用。

Method: 将脑连接组分解成有信息的本地k-hop子树，结合图卷积网络和双分支解码器，实现边存在性和权重的联合预测。

Result: 在自监督任务中优于现有模型，保持了竞争优势，在结构保真度和边权预测方面表现优异，且显存占用少，具良好扩展性。

Conclusion: 提出的GraphTreeGen有效提高脑连接组的生成效率和准确性，具备拓展到超分辨率和跨模态合成的潜力。

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [146] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出了一种新型的未训练循环神经网络ResRMN，结合线性和非线性储层，通过残差连接增强长时依赖，优于传统RC模型。


<details>
  <summary>Details</summary>
Motivation: 旨在改善Reservoir Computing中长时间记忆和信息传播能力，提升模型性能。

Method: 设计了结合线性和非线性储层的ResRMN，并引入残差正交连接，通过稳定性分析和实验验证其效果。

Result: 实验证明ResRMN在时间序列和像素级分类任务中优于传统RC模型，表现出更强的记忆和传播能力。

Conclusion: ResRMN是一种有效的无训练RC模型架构，增强了长时间依赖的实现，具有潜在应用价值。

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [147] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 本文提出了一个结合生成模型和多实例强化学习的框架，用于优化电商广告创意组合，有效提升广告效果和收入。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法高效评估大量创意组合的问题，提升广告点击率和收入。

Method: 采用双阶段架构，先用生成模型产生多样组合，利用强化学习优化；再用多实例学习将组合奖励细化到单个创意元素。

Result: 该方法在电商平台上显著提高广告收益，并发布了相关工业级数据集以促进研究。

Conclusion: 该框架有效整合生成和强化学习技术，提升广告创意组合的效率和效果，具有实用价值。

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [148] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: 提出了一种模仿生物遗传机制的知识传递策略HKT，用于提升小模型性能，超越传统知识蒸馏，适应资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 旨在解决深度模型性能与效率难以兼顾的问题，提升小模型表现。

Method: 引入Hereditary Knowledge Transfer（HKT），通过模仿生物遗传机制，实现多阶段有选择的特征传递，包括提取、传递和混合（ETM），并采用遗传注意机制（GA）优化整合。

Result: 在多种视觉任务中，HKT显著提升了小模型性能，优于传统蒸馏方法，展现了良好的可扩展性和解释性。

Conclusion: HKT为资源有限环境提供了一种高效、可解释的高性能模型部署方案，具有广泛应用潜力。

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [149] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 通过引入描述生物标志物变化率的特征，有效提升年龄预测模型的准确性，强调动态健康轨迹的重要性。


<details>
  <summary>Details</summary>
Motivation: 准确预测个体的衰老轨迹，以改善预防医学和个性化干预。

Method: 利用纵向队列数据，通过工程特征（变化率）结合LightGBM模型，并分析特征重要性。

Result: 模型在不同时间点上表现出良好预测能力，显著优于传统模型，变化率特征是关键。

Conclusion: 动态健康轨迹的捕捉对于年龄预测至关重要，可推动个性化医疗的发展。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [150] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: 提出了一种面向混合专家模型的$-参数化，理论和实验证明其在跨宽度特征学习中的有效性。


<details>
  <summary>Details</summary>
Motivation: 结合大规模LLMs的超参数调优技术$$Transfer与Mixture-of-Experts架构，以优化模型训练。

Method: 推导出适用于MoE的$$-参数化，并验证其在专家数量和粒度变化下的性能表现。

Result: 证明了新参数化在不同模型宽度上的特征学习能力，并分析了专家数和粒度对学习率的影响。

Conclusion: 该研究为大规模MoE模型的训练提供了理论支撑和实用指导，有助于模型性能的提升。

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [151] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: TriForecaster利用专家模型混合和多任务学习解决多区域短期电力负荷预测中的区域、情境和时序变异问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能电网和智能仪表的发展，获得了更详细的多层级电力负荷数据，提升多区域电力负荷预测的准确性变得迫切。

Method: 提出结合Mixture of Experts和多任务学习的TriForecaster框架，利用RegionMixer与CTSpecializer进行区域、情境和时间的动态协调与专业化。

Result: 在四个实际数据集上，TriForecaster平均降低22.4%的预测误差，优于现有最先进模型，并在中国东部多个城市实现了实际部署。

Conclusion: 该方法展现出良好的泛化能力与实用价值，为多区域短期电力负荷预测提供了有效解决方案。

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [152] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: 提出了一种基于递归贝叶斯估计的Matérn核高斯过程超参数优化方法，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 优化高斯过程中的核函数超参数以提升回归性能。

Method: 将超参数优化问题转化为自回归模型参数的递归贝叶斯估计。

Result: 该方法在运行时间和均方根误差方面优于最大化边缘似然和哈密顿蒙特卡罗采样。

Conclusion: 基于贝叶斯估计的方法高效且有效，适用于高斯过程的超参数优化。

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [153] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 利用机器学习分析长跑比赛中运动员的生物力学特征，识别影响优异表现的关键因素，强调技术细节的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统专家分析难以全面理解人体运动特征与运动表现的关系，亟需结合数据分析提升理解和预测能力。

Method: 采用分位数回归、SHAP值分析以及偏依赖图等方法，分析冠军长跳比赛中的关键生物力学特征。

Result: 发现技术细节如膝关节角度和着陆姿势在不同性别运动员中具有重要作用，揭示了影响顶尖表现的多关键因素。

Conclusion: 提出了一套分析运动表现影响因素的方法框架，强调技术细节对于运动成绩提升的关键作用。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [154] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: 提出了一种基于结构化列表监督的全局偏好学习框架RankList，显著提升了在SER和图像审美评估中的排名性能，优于传统的局部比较方法。


<details>
  <summary>Details</summary>
Motivation: 旨在克服现有偏好学习方法（如RankNet）仅能进行局部比较、难以捕获全局排序一致性的局限性。

Method: 引入结构化列表级监督，结合概率模型和log-sum-exp逼近，扩展到skip-wise比较，实现逐步暴露复杂列表结构。

Result: 在多个情感识别和图像审美数据集上，RankList在Kendall's Tau和排名准确率上优于基线，具有良好的跨域泛化能力。

Conclusion: 提供一种统一、可扩展的偏好建模框架，有助于提升主观判断任务中的排序性能。

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [155] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: 提出了一种新颖的联邦“遗忘”算法——FedShard，兼顾效率公平与性能公平，有效提高模型对个人数据的删除效率并降低不公平风险。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中离线客户数据删除过程中存在的效率不公平和性能不公平问题，确保不同客户在数据删除中的公平性。

Method: 设计了FedShard算法，结合新颖的公平性指标，平衡了模型收敛、效率和公平性之间的关系。

Result: FedShard在提高删除效率、降低不公平风险方面表现优越，比传统方法快1.3-6.2倍，优于现有的精确“遗忘”算法。

Conclusion: FedShard有效实现了联邦“遗忘”任务中的公平性，提升了整体效率和安全性，为联邦学习中的个人隐私保护提供了新方案。

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [156] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: 深度学习的最新架构正逐渐取代传统方法，成为土壤性质预测中的主流工具。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在田面土壤预测中的应用潜力，挑战传统的机器学习方法。

Method: 对多种先进的人工神经网络（ANN）架构进行基准测试，包括MLP、Transformer、检索增强模型及基础模型，评估其在不同土壤属性预测上的表现。

Result: 现代ANN普遍优于传统方法，TabPFN表现最佳，验证深度学习已成熟，推荐其作为主流工具。

Conclusion: 深度学习尤其是TabPFN逐渐取代经典模型，成为田面土壤映射的首选技术。

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [157] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: 提出一种新型扩散模型（PDM），通过对比学习动态构建视觉原型，有效提升图像生成效率和语义相关性，无需外部存储，实现高质量、低成本的生成。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型计算成本高、存储繁琐的问题，提升效率和语义一致性。

Method: 将对比学习融入扩散流程，动态构建紧凑的视觉原型，无需外部记忆，指导去噪过程。

Result: 在保持高生成质量的同时，大幅降低计算和存储开销，优于依赖检索的方法，提供可扩展性强的替代方案。

Conclusion: 原型扩散模型（PDM）成功实现了高效、语义丰富的图像生成，为扩散模型的实用性提供新的解决方案。

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [158] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 提出一种在模型后期无需额外推理时间的基础上实现测试时调整的方案，通过噪声超网络调节噪声，有效保持模型性能且节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 解决现有测试时缩放方法导致的计算成本过高问题，旨在在不增加推理时间的情况下保持性能提升。

Method: 用噪声超网络替代奖励导向的噪声优化，提出一种基于噪声空间的训练框架，用于调节生成模型的输入噪声。

Result: 在保持与显式测试优化类似性能提升的同时，大幅降低了计算成本。

Conclusion: 所提方法在优化模型性能和节省计算资源方面取得显著成效，为大规模模型的高效推理提供了新思路。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [159] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: 提出了一种基于动态多专家（DyMoE）的方法用于图的增量学习，有效应对灾难性遗忘问题，实现了较好的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 解决图增量学习中灾难性遗忘的问题，考虑到不同时间点获得的知识贡献不同。

Method: 引入DyMoE GNN层，增加专家网络并设计定制正则化损失利用数据序列信息，通过稀疏MoE减少计算成本。

Result: 在类别增量学习任务中相较于最优基线提升4.92%的相对准确率，展现出强大性能。

Conclusion: 该方法通过动态专家模型与稀疏策略，有效缓解灾难性遗忘并提升模型性能，具有较强应用潜力。

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>
